This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
__init__.py
config.py
core/api.py
core/config.py
core/data_utils.py
data_utils.py
dqn_position_sizing.py
dreamer_position_sizing.py
evaluation/__init__.py
evaluation/compare_strategies.py
evaluation/evaluate_strategy.py
evaluation/metrics.py
evaluation/plot_results.py
evaluation/README.md
evaluation/visualization.py
main.py
market_regimes.py
metalabeling.py
metalabeling/__init__.py
metalabeling/labelers.py
metalabeling/metalabeling.py
metalabeling/models.py
metalabeling/strategy.py
models/__init__.py
models/base_rl.py
models/dqn.py
models/dreamer.py
models/hmm.py
models/ppo.py
models/sac.py
models/transformer.py
position_sizing/base.py
position_sizing/dqn_position_sizing.py
position_sizing/dreamer_position_sizing.py
position_sizing/ppo_position_sizing.py
position_sizing/sac_position_sizing.py
ppo_position_sizing.py
project_plan.py
regime_evaluator.py
regimes/detector.py
regimes/regime_evaluator.py
rl_position_sizing.py
run_experiments.py
sac_position_sizing.py
utils/__init__.py
utils/data_processing.py
utils/visualization.py

================================================================
Files
================================================================

================
File: __init__.py
================
"""
DL MetaLabeling Framework

A framework for deep learning-based meta-labeling in trading strategies.

This package provides tools for:
- Market regime detection
- Meta-labeling of trading signals
- Position sizing using reinforcement learning
- Strategy evaluation and visualization

The framework integrates these components to create robust trading strategies
that adapt to changing market conditions.
"""

import logging
import os

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Create logger
logger = logging.getLogger('dl_metalabeling')

# Import core components
from .core.api import DLMetaLabAPI

# Import regime detection components
from .market_regimes import HMMRegimeDetector, TransformerRegimeDetector
from .regimes.regime_evaluator import RegimeEvaluator

# Import meta-labeling components
from .metalabeling import (
    BaseLabeler,
    TripleBarrierLabeler,
    FixedHorizonLabeler,
    BaseMetaLabeler,
    RandomForestMetaLabeler,
    NeuralNetworkMetaLabeler,
    MetaLabeling,
    RegimeMetaLabeling
)

# Import position sizing components
from .models.base_rl import RLPositionSizer
from .models.dqn import DQNPositionSizer
from .models.ppo import PPOPositionSizer
from .models.sac import SACPositionSizer
from .models.dreamer import DreamerPositionSizer

# Import utility functions
from .utils import (
    # Data processing
    load_data,
    preprocess_data,
    add_returns,
    add_technical_indicators,
    add_volatility_features,
    add_target_signal,
    prepare_training_data,
    generate_cv_time_series_splits,
    calculate_performance_metrics,
    
    # Visualization
    set_plotting_style,
    plot_returns,
    plot_regime_distribution,
    plot_signal_distribution,
    plot_performance_metrics,
    plot_confusion_matrix,
    plot_feature_importance,
    plot_learning_curves,
    plot_regime_performance
)

# Define version
__version__ = '0.2.0'

# Define exported components
__all__ = [
    # Core API
    'DLMetaLabAPI',
    
    # Regime detection
    'HMMRegimeDetector',
    'TransformerRegimeDetector',
    'RegimeEvaluator',
    
    # Meta-labeling
    'BaseLabeler',
    'TripleBarrierLabeler',
    'FixedHorizonLabeler',
    'BaseMetaLabeler',
    'RandomForestMetaLabeler',
    'NeuralNetworkMetaLabeler',
    'MetaLabeling',
    'RegimeMetaLabeling',
    
    # Position sizing
    'RLPositionSizer',
    'DQNPositionSizer',
    'PPOPositionSizer',
    'SACPositionSizer',
    'DreamerPositionSizer',
    
    # Utility functions - Data processing
    'load_data',
    'preprocess_data',
    'add_returns',
    'add_technical_indicators',
    'add_volatility_features',
    'add_target_signal',
    'prepare_training_data',
    'generate_cv_time_series_splits',
    'calculate_performance_metrics',
    
    # Utility functions - Visualization
    'set_plotting_style',
    'plot_returns',
    'plot_regime_distribution',
    'plot_signal_distribution',
    'plot_performance_metrics',
    'plot_confusion_matrix',
    'plot_feature_importance',
    'plot_learning_curves',
    'plot_regime_performance'
]

================
File: config.py
================
"""
Configuration parameters for the Deep Learning Market Regime Detection and Position Sizing project.
"""

class Config:
    # General parameters
    RANDOM_SEED = 42
    DATA_PATH = "data/forex_data.csv"
    TRANSACTION_COST = 0.0001  # 1 pip for forex
    
    # Data preprocessing parameters
    PREPROCESSING_PARAMS = {
        'calculate_returns': True,
        'calculate_volatility': True,
        'window_sizes': [5, 10, 20, 50],
        'features_to_normalize': ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126'],
        'technical_indicators': ['rsi', 'macd', 'bbands', 'ema']
    }
    
    # Market regime detection parameters
    N_REGIMES = 3
    REGIME_FEATURES = ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126']
    
    # HMM parameters
    HMM_COV_TYPE = 'full'
    
    # Transformer parameters
    TRANSFORMER_EMBEDDING_DIM = 64
    TRANSFORMER_NUM_HEADS = 4
    TRANSFORMER_NUM_LAYERS = 2
    TRANSFORMER_BATCH_SIZE = 32
    TRANSFORMER_EPOCHS = 100
    TRANSFORMER_LR = 0.001
    
    # Meta-labeling parameters
    METALABEL_FEATURES = ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126', 'hmm_regime', 'transformer_regime']
    METALABEL_TARGET = 'direction'
    METALABEL_MODEL_TYPE = 'random_forest'
    METALABEL_LABELING_METHOD = 'triple_barrier'
    METALABEL_CV_FOLDS = 5
    METALABEL_HYPERPARAMS = {
        'n_estimators': 100,
        'max_depth': 5,
        'min_samples_split': 10,
        'min_samples_leaf': 5
    }
    
    # Position sizing parameters
    FIXED_POSITION_SIZE = 1.0
    
    # Base RL parameters
    RL_STATE_DIM = 8  # Number of state features
    RL_ACTION_DIM = 5  # Number of discrete position sizes
    RL_LEARNING_RATE = 0.001
    RL_GAMMA = 0.99
    RL_EPSILON = 0.1
    RL_EPOCHS = 100
    RL_BATCH_SIZE = 64
    
    # DQN parameters
    DQN_STATE_DIM = 10  # Extended state with more features
    DQN_ACTION_DIM = 9  # More granular position sizes
    DQN_HIDDEN_DIMS = [128, 64]
    DQN_LEARNING_RATE = 0.0005
    DQN_GAMMA = 0.99
    DQN_EPSILON_START = 1.0
    DQN_EPSILON_END = 0.01
    DQN_EPSILON_DECAY = 0.995
    DQN_BUFFER_SIZE = 10000
    DQN_EPOCHS = 200
    DQN_BATCH_SIZE = 64
    DQN_TARGET_UPDATE = 10
    
    # PPO parameters
    PPO_STATE_DIM = 10
    PPO_ACTION_DIM = 1  # Continuous action space
    PPO_HIDDEN_DIMS = [128, 64]
    PPO_ACTOR_LR = 0.0003
    PPO_CRITIC_LR = 0.001
    PPO_GAMMA = 0.99
    PPO_GAE_LAMBDA = 0.95
    PPO_CLIP_PARAM = 0.2
    PPO_VALUE_COEF = 0.5
    PPO_ENTROPY_COEF = 0.01
    PPO_EPOCHS = 300
    PPO_BATCH_SIZE = 64
    PPO_NUM_UPDATES = 10
    
    # SAC parameters
    SAC_STATE_DIM = 10
    SAC_ACTION_DIM = 1  # Continuous action space
    SAC_HIDDEN_DIMS = [256, 256]
    SAC_ACTOR_LR = 0.0003
    SAC_CRITIC_LR = 0.0003
    SAC_ALPHA_LR = 0.0003
    SAC_GAMMA = 0.99
    SAC_TAU = 0.005
    SAC_BUFFER_SIZE = 100000
    SAC_EPOCHS = 500
    SAC_BATCH_SIZE = 256
    SAC_INITIAL_RANDOM = 10000
    
    # Dreamer parameters
    DREAMER_STATE_DIM = 10
    DREAMER_ACTION_DIM = 1  # Continuous action space
    DREAMER_DETER_DIM = 200
    DREAMER_STOCH_DIM = 30
    DREAMER_HIDDEN_DIM = 200
    DREAMER_ACTOR_LR = 0.0001
    DREAMER_CRITIC_LR = 0.0001
    DREAMER_WORLD_LR = 0.0001
    DREAMER_GAMMA = 0.99
    DREAMER_LAMBDA_GAE = 0.95
    DREAMER_HORIZON = 15
    DREAMER_EPOCHS = 1000
    DREAMER_BATCH_SIZE = 50
    DREAMER_COLLECT_STEPS = 1000
    DREAMER_TRAIN_STEPS = 100

================
File: core/api.py
================
"""
DL MetaLabeling Package API

This is the main interface for using the DL MetaLabeling package.
It provides a unified API for:
1. Market regime detection (HMM and Transformer)
2. Meta-labeling with ML/DL models
3. DL position sizing with various RL approaches
4. Visualization and performance comparison

Usage:
```python
from dl_metalabeling.main import DLMetaLabAPI

# Initialize API
api = DLMetaLabAPI()

# Run full pipeline with default settings
results = api.run_pipeline()

# Compare all position sizing models
comparison = api.compare_position_sizing_models()

# Plot results
api.plot_cumulative_returns()
api.plot_performance_metrics()
api.plot_regime_distribution()
```
"""

import os
import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Union, Any

# Import all components from the package
from dl_metalabeling.project_plan import DLMetaLabelingFramework
from dl_metalabeling.market_regimes import HMMRegimeDetector, TransformerRegimeDetector
from dl_metalabeling.metalabeling import MetaLabeling, RegimeMetaLabeling
from dl_metalabeling.utils import calculate_performance_metrics
from dl_metalabeling.models.base_rl import RLPositionSizer
from dl_metalabeling.models.dqn import DQNPositionSizer
from dl_metalabeling.models.ppo import PPOPositionSizer
from dl_metalabeling.models.sac import SACPositionSizer
from dl_metalabeling.models.dreamer import DreamerPositionSizer
from dl_metalabeling.utils import load_data, preprocess_data

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


class DLMetaLabAPI:
    """
    Main API for DL MetaLabeling package.
    Provides methods to run, analyze, and visualize the performance of various
    market regime detection, meta-labeling, and position sizing models.
    """
    
    def __init__(self, data_path: str = 'data/cmma.csv', config: Dict = None):
        """
        Initialize the API with data and configuration.
        
        Args:
            data_path (str): Path to data file
            config (dict): Custom configuration (optional)
        """
        self.data_path = data_path
        self.config = config
        
        # Results storage
        self.frameworks = {}
        self.results = {}
        self.evaluations = {}
        self.comparison_data = {}
        
        # Default position sizing models to compare
        self.position_sizing_models = ['base_rl', 'dqn', 'ppo', 'sac', 'dreamer']
        
        # Initialize but don't run yet
        self._initialize_framework()
        
        logger.info(f"DL MetaLabeling API initialized with data path: {data_path}")
    
    def _initialize_framework(self):
        """Initialize the framework with appropriate configuration."""
        # Create default framework
        self.framework = DLMetaLabelingFramework(config=self.config)
        
        # Ensure both HMM and Transformer regimes are used
        if self.config is None:
            # If no custom config, ensure both regime detectors are enabled
            self.framework.config['regime_detection']['methods'] = ['hmm', 'transformer']
        
        # Update data path
        self.framework.config['data']['filepath'] = self.data_path
        
        logger.info("Framework initialized with both HMM and Transformer regime detectors")
    
    def run_pipeline(self, position_sizing_model: str = 'dqn') -> pd.DataFrame:
        """
        Run the full pipeline with a specific position sizing model.
        
        Args:
            position_sizing_model (str): Position sizing model to use
                ('base_rl', 'dqn', 'ppo', 'sac', 'dreamer')
        
        Returns:
            pd.DataFrame: Final data with all components applied
        """
        logger.info(f"Running full pipeline with {position_sizing_model} position sizing")
        
        # Update position sizing model
        self.framework.config['position_sizing']['model'] = position_sizing_model
        
        # Run the full pipeline
        final_data = self.framework.run_pipeline()
        
        # Store results
        self.frameworks[position_sizing_model] = self.framework
        self.results[position_sizing_model] = final_data
        
        # Evaluate and store
        evaluation = self.framework.evaluate_on_test_data()
        self.evaluations[position_sizing_model] = evaluation
        
        logger.info(f"Pipeline completed for {position_sizing_model}")
        return final_data
    
    def compare_position_sizing_models(self, 
                                       models: List[str] = None,
                                       output_dir: str = 'results',
                                       save_results: bool = True) -> pd.DataFrame:
        """
        Run and compare all position sizing models.
        
        Args:
            models (list): List of models to compare (if None, use all)
            output_dir (str): Directory to save results
            save_results (bool): Whether to save results to disk
            
        Returns:
            pd.DataFrame: Comparison data
        """
        logger.info("Comparing position sizing models")
        
        # Use provided models or default list
        models = models or self.position_sizing_models
        
        # Create output directories if needed
        if save_results:
            os.makedirs(output_dir, exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'models'), exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
        
        # List to store comparison data
        comparison_data = []
        
        # Run pipeline for each model
        for model in models:
            logger.info(f"Running pipeline for {model} model")
            
            try:
                # Run pipeline
                final_data = self.run_pipeline(model)
                
                # Get evaluation
                evaluation = self.evaluations[model]
                
                # Add to comparison data
                comparison_data.append({
                    'Model': model,
                    'Base Sharpe': evaluation['base']['sharpe_ratio'],
                    'Meta Sharpe': evaluation['meta']['sharpe_ratio'],
                    'RL Sharpe': evaluation['rl']['sharpe_ratio'],
                    'Base Return': evaluation['base']['annual_return'],
                    'Meta Return': evaluation['meta']['annual_return'],
                    'RL Return': evaluation['rl']['annual_return'],
                    'Base Drawdown': evaluation['base']['max_drawdown'],
                    'Meta Drawdown': evaluation['meta']['max_drawdown'],
                    'RL Drawdown': evaluation['rl']['max_drawdown'],
                    'RL Win Rate': evaluation['rl']['win_rate'],
                    'RL Profit Factor': evaluation['rl']['profit_factor']
                })
                
                # Save models if requested
                if save_results:
                    model_dir = os.path.join(output_dir, 'models', f'position_{model}')
                    os.makedirs(model_dir, exist_ok=True)
                    self.frameworks[model].save_models(model_dir)
                    
                    # Save returns data
                    returns_data = pd.DataFrame({
                        'DateTime': final_data.index,
                        'Base Returns': final_data['strategy_returns'],
                        'Meta Returns': final_data['meta_strategy_returns'],
                        'RL Returns': final_data['rl_strategy_returns']
                    })
                    returns_data.to_csv(os.path.join(output_dir, 'results', f'position_{model}_returns.csv'), index=False)
            
            except Exception as e:
                logger.error(f"Error running {model} model: {e}")
        
        # Convert to DataFrame
        comparison_df = pd.DataFrame(comparison_data)
        self.comparison_data = comparison_df
        
        # Save comparison if requested
        if save_results and not comparison_df.empty:
            comparison_df.to_csv(os.path.join(output_dir, 'results', 'position_comparison.csv'), index=False)
        
        logger.info("Position sizing model comparison completed")
        return comparison_df
    
    def plot_cumulative_returns(self, 
                              models: List[str] = None, 
                              save_path: str = None,
                              figsize: Tuple[int, int] = (15, 8)) -> plt.Figure:
        """
        Plot cumulative returns for different position sizing models.
        
        Args:
            models (list): List of model names to include in the plot
            save_path (str): Path to save the plot (if None, don't save)
            figsize (tuple): Figure size
            
        Returns:
            matplotlib.figure.Figure: Plot figure
        """
        logger.info("Plotting cumulative returns")
        
        # Use provided models or all available
        models = models or list(self.results.keys())
        
        # Create figure
        fig, ax = plt.subplots(figsize=figsize)
        
        # Plot cumulative returns for each model
        for model in models:
            if model in self.results:
                final_data = self.results[model]
                if 'rl_strategy_returns' in final_data.columns:
                    cum_returns = (1 + final_data['rl_strategy_returns']).cumprod() - 1
                    ax.plot(cum_returns, label=f"{model.upper()}")
                else:
                    logger.warning(f"'rl_strategy_returns' column not found for model {model}")
        
        # Add base strategy and meta-labeled strategy if they exist
        if models and models[0] in self.results:
            final_data = self.results[models[0]]
            
            # Plot base strategy returns if available
            if 'strategy_returns' in final_data.columns:
                ax.plot((1 + final_data['strategy_returns']).cumprod() - 1, 
                        label='Base Strategy', linestyle='--', color='gray')
            elif 'returns' in final_data.columns:
                # Use the returns column if strategy_returns is not available
                logger.info("Using 'returns' column as fallback for base strategy")
                ax.plot((1 + final_data['returns']).cumprod() - 1, 
                        label='Base Strategy', linestyle='--', color='gray')
            else:
                logger.warning("Neither 'strategy_returns' nor 'returns' columns found for base strategy")
            
            # Plot meta-labeled strategy returns if available
            if 'meta_strategy_returns' in final_data.columns:
                ax.plot((1 + final_data['meta_strategy_returns']).cumprod() - 1, 
                        label='Meta-Labeled', linestyle='-.', color='black')
            else:
                logger.warning("'meta_strategy_returns' column not found")
        
        # Add labels and legend
        ax.set_title('Cumulative Returns Comparison', fontsize=16)
        ax.set_xlabel('Time', fontsize=12)
        ax.set_ylabel('Cumulative Returns', fontsize=12)
        ax.legend(loc='best')
        ax.grid(True)
        
        # Tight layout
        plt.tight_layout()
        
        # Save if path provided
        if save_path:
            plt.savefig(save_path)
            logger.info(f"Cumulative returns plot saved to {save_path}")
        
        return fig
    
    def plot_performance_metrics(self,
                               metrics: List[str] = None,
                               models: List[str] = None,
                               save_path: str = None,
                               figsize: Tuple[int, int] = (15, 10)) -> plt.Figure:
        """
        Plot performance metrics comparison for different position sizing models.
        
        Args:
            metrics (list): Metrics to plot
            models (list): Models to include
            save_path (str): Path to save the plot
            figsize (tuple): Figure size
            
        Returns:
            matplotlib.figure.Figure: Plot figure
        """
        logger.info("Plotting performance metrics comparison")
        
        # Default metrics if not provided
        if metrics is None:
            metrics = ['Sharpe', 'Return', 'Drawdown', 'Win Rate', 'Profit Factor']
        
        # Use provided models or all from comparison data
        if self.comparison_data.empty:
            logger.warning("No comparison data available. Run compare_position_sizing_models first.")
            return None
        
        # Filter models if provided
        df = self.comparison_data
        if models:
            df = df[df['Model'].isin(models)]
        
        # Create figure
        n_metrics = len(metrics)
        n_cols = min(3, n_metrics)
        n_rows = (n_metrics + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
        axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]
        
        # Plot each metric
        for i, metric in enumerate(metrics):
            if i < len(axes):
                ax = axes[i]
                
                # Determine which column to use
                if metric == 'Sharpe':
                    col = 'RL Sharpe'
                elif metric == 'Return':
                    col = 'RL Return'
                elif metric == 'Drawdown':
                    col = 'RL Drawdown'
                elif metric == 'Win Rate':
                    col = 'RL Win Rate'
                elif metric == 'Profit Factor':
                    col = 'RL Profit Factor'
                else:
                    continue
                
                # Check if column exists
                if col in df.columns:
                    sns.barplot(x='Model', y=col, data=df, ax=ax)
                    ax.set_title(f'{metric} Comparison', fontsize=14)
                    ax.set_xlabel('')
                    ax.grid(True, alpha=0.3)
                    
                    # Format y-axis for percentage metrics
                    if metric in ['Return', 'Drawdown', 'Win Rate']:
                        vals = ax.get_yticks()
                        ax.set_yticklabels([f'{x:.1%}' for x in vals])
                    
                    plt.setp(ax.get_xticklabels(), rotation=45)
        
        # Hide unused axes
        for i in range(len(metrics), len(axes)):
            axes[i].set_visible(False)
        
        # Add a main title
        plt.suptitle('Position Sizing Models Performance Comparison', fontsize=16, y=1.02)
        
        # Tight layout
        plt.tight_layout()
        
        # Save if path provided
        if save_path:
            plt.savefig(save_path)
            logger.info(f"Performance metrics plot saved to {save_path}")
        
        return fig
    
    def plot_regime_distribution(self, 
                               model: str = None,
                               regime_column: str = 'hmm_regime',
                               save_path: str = None,
                               figsize: Tuple[int, int] = (15, 10)) -> plt.Figure:
        """
        Plot market regime distribution and performance by regime.
        
        Args:
            model (str): Model to analyze (if None, use first available)
            regime_column (str): Column with regime labels ('hmm_regime' or 'transformer_regime')
            save_path (str): Path to save the plot
            figsize (tuple): Figure size
            
        Returns:
            matplotlib.figure.Figure: Plot figure
        """
        logger.info(f"Plotting regime distribution for {regime_column}")
        
        # Get model data
        if model is None and self.results:
            model = list(self.results.keys())[0]
        
        if model not in self.results:
            logger.warning(f"No data available for model {model}")
            return None
        
        final_data = self.results[model]
        
        # Check if regime column exists
        if regime_column not in final_data.columns:
            logger.warning(f"Regime column {regime_column} not found in data")
            return None
        
        # Create figure
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        # 1. Plot regime distribution over time
        ax1 = axes[0, 0]
        scatter = ax1.scatter(
            range(len(final_data)),
            final_data['returns'],
            c=final_data[regime_column],
            cmap='viridis',
            alpha=0.6,
            s=10
        )
        ax1.set_title(f'Returns Colored by {regime_column}', fontsize=14)
        ax1.set_xlabel('Time')
        ax1.set_ylabel('Returns')
        ax1.grid(True, alpha=0.3)
        legend1 = ax1.legend(*scatter.legend_elements(), title="Regimes")
        ax1.add_artist(legend1)
        
        # 2. Plot regime count
        ax2 = axes[0, 1]
        regime_counts = final_data[regime_column].value_counts()
        regime_counts.plot(kind='bar', ax=ax2)
        ax2.set_title(f'Regime Distribution', fontsize=14)
        ax2.set_xlabel('Regime')
        ax2.set_ylabel('Count')
        ax2.grid(True, alpha=0.3)
        
        # 3. Plot performance by regime (Sharpe ratio)
        ax3 = axes[1, 0]
        regimes = final_data[regime_column].unique()
        sharpe_by_regime = []
        
        for regime in regimes:
            regime_data = final_data[final_data[regime_column] == regime]
            
            if len(regime_data) > 10:  # Only if we have enough data
                base_perf = calculate_performance_metrics(regime_data['strategy_returns'])
                meta_perf = calculate_performance_metrics(regime_data['meta_strategy_returns'])
                rl_perf = calculate_performance_metrics(regime_data['rl_strategy_returns'])
                
                sharpe_by_regime.append({
                    'Regime': regime,
                    'Base': base_perf['sharpe_ratio'],
                    'Meta': meta_perf['sharpe_ratio'],
                    'RL': rl_perf['sharpe_ratio']
                })
        
        if sharpe_by_regime:
            sharpe_df = pd.DataFrame(sharpe_by_regime)
            sharpe_df.set_index('Regime', inplace=True)
            sharpe_df.plot(kind='bar', ax=ax3)
            ax3.set_title('Sharpe Ratio by Regime', fontsize=14)
            ax3.set_xlabel('Regime')
            ax3.set_ylabel('Sharpe Ratio')
            ax3.grid(True, alpha=0.3)
        
        # 4. Plot returns by regime
        ax4 = axes[1, 1]
        returns_by_regime = []
        
        for regime in regimes:
            regime_data = final_data[final_data[regime_column] == regime]
            
            if len(regime_data) > 10:  # Only if we have enough data
                base_perf = calculate_performance_metrics(regime_data['strategy_returns'])
                meta_perf = calculate_performance_metrics(regime_data['meta_strategy_returns'])
                rl_perf = calculate_performance_metrics(regime_data['rl_strategy_returns'])
                
                returns_by_regime.append({
                    'Regime': regime,
                    'Base': base_perf['annual_return'],
                    'Meta': meta_perf['annual_return'],
                    'RL': rl_perf['annual_return']
                })
        
        if returns_by_regime:
            returns_df = pd.DataFrame(returns_by_regime)
            returns_df.set_index('Regime', inplace=True)
            returns_df.plot(kind='bar', ax=ax4)
            ax4.set_title('Annual Return by Regime', fontsize=14)
            ax4.set_xlabel('Regime')
            ax4.set_ylabel('Annual Return')
            ax4.grid(True, alpha=0.3)
            
            # Format y-axis for percentage
            vals = ax4.get_yticks()
            ax4.set_yticklabels([f'{x:.1%}' for x in vals])
        
        # Add a main title
        plt.suptitle(f'Market Regime Analysis: {regime_column.replace("_", " ").title()}', fontsize=16, y=1.02)
        
        # Tight layout
        plt.tight_layout()
        
        # Save if path provided
        if save_path:
            plt.savefig(save_path)
            logger.info(f"Regime distribution plot saved to {save_path}")
        
        return fig
    
    def run_comprehensive_analysis(self, 
                                  output_dir: str = 'results',
                                  save_results: bool = True,
                                  save_plots: bool = True) -> Dict:
        """
        Run a comprehensive analysis of all position sizing models.
        
        Args:
            output_dir (str): Directory to save results and plots
            save_results (bool): Whether to save results to disk
            save_plots (bool): Whether to save plots to disk
            
        Returns:
            dict: Dictionary with all results and analysis
        """
        logger.info("Running comprehensive analysis")
        
        # Create output directories
        if save_results or save_plots:
            os.makedirs(output_dir, exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'plots'), exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'models'), exist_ok=True)
        
        # Compare all position sizing models
        comparison = self.compare_position_sizing_models(
            output_dir=output_dir,
            save_results=save_results
        )
        
        # Generate plots
        plots = {}
        
        if save_plots:
            # Cumulative returns plot
            plots['cumulative_returns'] = self.plot_cumulative_returns(
                save_path=os.path.join(output_dir, 'plots', 'cumulative_returns.png')
            )
            
            # Performance metrics plot
            plots['performance_metrics'] = self.plot_performance_metrics(
                save_path=os.path.join(output_dir, 'plots', 'performance_metrics.png')
            )
            
            # HMM regime distribution plot
            plots['hmm_regime'] = self.plot_regime_distribution(
                regime_column='hmm_regime',
                save_path=os.path.join(output_dir, 'plots', 'hmm_regime_distribution.png')
            )
            
            # Transformer regime distribution plot
            plots['transformer_regime'] = self.plot_regime_distribution(
                regime_column='transformer_regime',
                save_path=os.path.join(output_dir, 'plots', 'transformer_regime_distribution.png')
            )
        else:
            # Generate plots without saving
            plots['cumulative_returns'] = self.plot_cumulative_returns()
            plots['performance_metrics'] = self.plot_performance_metrics()
            plots['hmm_regime'] = self.plot_regime_distribution(regime_column='hmm_regime')
            plots['transformer_regime'] = self.plot_regime_distribution(regime_column='transformer_regime')
        
        # Create summary report
        summary = {
            'best_model': None,
            'best_sharpe': -np.inf,
            'best_return': -np.inf,
            'regime_effectiveness': {}
        }
        
        # Find best model
        if not comparison.empty:
            best_idx = comparison['RL Sharpe'].idxmax()
            summary['best_model'] = comparison.loc[best_idx, 'Model']
            summary['best_sharpe'] = comparison.loc[best_idx, 'RL Sharpe']
            summary['best_return'] = comparison.loc[best_idx, 'RL Return']
        
        # Analyze regime effectiveness
        for model in self.results:
            final_data = self.results[model]
            
            # Check regime columns
            for regime_col in ['hmm_regime', 'transformer_regime']:
                if regime_col in final_data.columns:
                    regimes = final_data[regime_col].unique()
                    regime_performance = {}
                    
                    for regime in regimes:
                        regime_data = final_data[final_data[regime_col] == regime]
                        
                        if len(regime_data) > 10:  # Only if we have enough data
                            base_perf = calculate_performance_metrics(regime_data['strategy_returns'])
                            rl_perf = calculate_performance_metrics(regime_data['rl_strategy_returns'])
                            
                            # Calculate improvement
                            sharpe_improvement = rl_perf['sharpe_ratio'] - base_perf['sharpe_ratio']
                            return_improvement = rl_perf['annual_return'] - base_perf['annual_return']
                            
                            regime_performance[regime] = {
                                'count': len(regime_data),
                                'sharpe_improvement': sharpe_improvement,
                                'return_improvement': return_improvement
                            }
                    
                    if regime_performance:
                        if model not in summary['regime_effectiveness']:
                            summary['regime_effectiveness'][model] = {}
                        
                        summary['regime_effectiveness'][model][regime_col] = regime_performance
        
        # Save summary if requested
        if save_results:
            # Convert summary to DataFrame for the parts that can be converted
            if summary['best_model'] is not None:
                summary_df = pd.DataFrame([{
                    'Best Model': summary['best_model'],
                    'Best Sharpe': summary['best_sharpe'],
                    'Best Return': summary['best_return']
                }])
                summary_df.to_csv(os.path.join(output_dir, 'results', 'summary.csv'), index=False)
            
            # Save regime effectiveness as separate CSV files
            for model, regime_data in summary['regime_effectiveness'].items():
                for regime_col, regime_perf in regime_data.items():
                    regime_df = pd.DataFrame.from_dict(regime_perf, orient='index')
                    regime_df.to_csv(os.path.join(output_dir, 'results', f'{model}_{regime_col}_effectiveness.csv'))
        
        logger.info("Comprehensive analysis completed")
        
        return {
            'comparison': comparison,
            'plots': plots,
            'summary': summary,
            'models': self.frameworks,
            'results': self.results,
            'evaluations': self.evaluations
        }
    
    def load_models(self, base_dir: str = 'models', model: str = None) -> None:
        """
        Load saved models.
        
        Args:
            base_dir (str): Base directory with saved models
            model (str): Specific model to load (if None, load all available)
        """
        logger.info(f"Loading models from {base_dir}")
        
        # Initialize new frameworks for each model
        if model:
            # Load specific model
            model_dir = os.path.join(base_dir, f'position_{model}')
            if os.path.exists(model_dir):
                framework = DLMetaLabelingFramework(config=self.config)
                framework.load_models(model_dir)
                self.frameworks[model] = framework
                logger.info(f"Loaded model: {model}")
            else:
                logger.warning(f"Model directory not found: {model_dir}")
        else:
            # Try to load all models
            for model_name in self.position_sizing_models:
                model_dir = os.path.join(base_dir, f'position_{model_name}')
                if os.path.exists(model_dir):
                    framework = DLMetaLabelingFramework(config=self.config)
                    framework.load_models(model_dir)
                    self.frameworks[model_name] = framework
                    logger.info(f"Loaded model: {model_name}")
        
        logger.info("Model loading completed")


# Example usage
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='DL MetaLabeling Package API')
    parser.add_argument('--data_path', type=str, default='data/cmma.csv', help='Path to data file')
    parser.add_argument('--output_dir', type=str, default='results', help='Directory to save results')
    parser.add_argument('--models', type=str, nargs='+', default=None, 
                      help='Position sizing models to use (if not specified, use all)')
    parser.add_argument('--analysis', action='store_true', help='Run comprehensive analysis')
    parser.add_argument('--load_models', action='store_true', help='Load saved models instead of training')
    parser.add_argument('--plot_only', action='store_true', help='Only generate plots (requires saved results)')
    
    args = parser.parse_args()
    
    # Initialize API
    api = DLMetaLabAPI(data_path=args.data_path)
    
    if args.plot_only:
        # Just load results and generate plots
        try:
            # Try to load comparison data
            comparison_path = os.path.join(args.output_dir, 'results', 'position_comparison.csv')
            if os.path.exists(comparison_path):
                api.comparison_data = pd.read_csv(comparison_path)
                
                # Generate plots
                api.plot_cumulative_returns(save_path=os.path.join(args.output_dir, 'plots', 'cumulative_returns.png'))
                api.plot_performance_metrics(save_path=os.path.join(args.output_dir, 'plots', 'performance_metrics.png'))
                
                print("Plots generated successfully")
            else:
                print(f"Comparison data not found at {comparison_path}")
        except Exception as e:
            print(f"Error generating plots: {e}")
    
    elif args.load_models:
        # Load saved models
        api.load_models(args.output_dir)
        
        if args.analysis:
            # Run analysis with loaded models
            api.run_comprehensive_analysis(args.output_dir)
    
    elif args.analysis:
        # Run comprehensive analysis
        models = args.models or api.position_sizing_models
        api.position_sizing_models = models
        api.run_comprehensive_analysis(args.output_dir)
    
    else:
        # Compare specified position sizing models
        models = args.models or api.position_sizing_models
        api.position_sizing_models = models
        
        comparison = api.compare_position_sizing_models(models=models, output_dir=args.output_dir)
        
        # Generate and save basic plots
        api.plot_cumulative_returns(save_path=os.path.join(args.output_dir, 'plots', 'cumulative_returns.png'))
        api.plot_performance_metrics(save_path=os.path.join(args.output_dir, 'plots', 'performance_metrics.png'))

================
File: core/config.py
================
"""
Configuration parameters for the Deep Learning Market Regime Detection and Position Sizing project.
"""

class Config:
    # General parameters
    RANDOM_SEED = 42
    DATA_PATH = "data/forex_data.csv"
    TRANSACTION_COST = 0.0001  # 1 pip for forex
    
    # Data preprocessing parameters
    PREPROCESSING_PARAMS = {
        'calculate_returns': True,
        'calculate_volatility': True,
        'window_sizes': [5, 10, 20, 50],
        'features_to_normalize': ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126'],
        'technical_indicators': ['rsi', 'macd', 'bbands', 'ema']
    }
    
    # Market regime detection parameters
    N_REGIMES = 3
    REGIME_FEATURES = ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126']
    
    # HMM parameters
    HMM_COV_TYPE = 'full'
    
    # Transformer parameters
    TRANSFORMER_EMBEDDING_DIM = 64
    TRANSFORMER_NUM_HEADS = 4
    TRANSFORMER_NUM_LAYERS = 2
    TRANSFORMER_BATCH_SIZE = 32
    TRANSFORMER_EPOCHS = 100
    TRANSFORMER_LR = 0.001
    
    # Meta-labeling parameters
    METALABEL_FEATURES = ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126', 'hmm_regime', 'transformer_regime']
    METALABEL_TARGET = 'direction'
    METALABEL_MODEL_TYPE = 'random_forest'
    METALABEL_LABELING_METHOD = 'triple_barrier'
    METALABEL_CV_FOLDS = 5
    METALABEL_HYPERPARAMS = {
        'n_estimators': 100,
        'max_depth': 5,
        'min_samples_split': 10,
        'min_samples_leaf': 5
    }
    
    # Position sizing parameters
    FIXED_POSITION_SIZE = 1.0
    
    # Base RL parameters
    RL_STATE_DIM = 8  # Number of state features
    RL_ACTION_DIM = 5  # Number of discrete position sizes
    RL_LEARNING_RATE = 0.001
    RL_GAMMA = 0.99
    RL_EPSILON = 0.1
    RL_EPOCHS = 100
    RL_BATCH_SIZE = 64
    
    # DQN parameters
    DQN_STATE_DIM = 10  # Extended state with more features
    DQN_ACTION_DIM = 9  # More granular position sizes
    DQN_HIDDEN_DIMS = [128, 64]
    DQN_LEARNING_RATE = 0.0005
    DQN_GAMMA = 0.99
    DQN_EPSILON_START = 1.0
    DQN_EPSILON_END = 0.01
    DQN_EPSILON_DECAY = 0.995
    DQN_BUFFER_SIZE = 10000
    DQN_EPOCHS = 200
    DQN_BATCH_SIZE = 64
    DQN_TARGET_UPDATE = 10
    
    # PPO parameters
    PPO_STATE_DIM = 10
    PPO_ACTION_DIM = 1  # Continuous action space
    PPO_HIDDEN_DIMS = [128, 64]
    PPO_ACTOR_LR = 0.0003
    PPO_CRITIC_LR = 0.001
    PPO_GAMMA = 0.99
    PPO_GAE_LAMBDA = 0.95
    PPO_CLIP_PARAM = 0.2
    PPO_VALUE_COEF = 0.5
    PPO_ENTROPY_COEF = 0.01
    PPO_EPOCHS = 300
    PPO_BATCH_SIZE = 64
    PPO_NUM_UPDATES = 10
    
    # SAC parameters
    SAC_STATE_DIM = 10
    SAC_ACTION_DIM = 1  # Continuous action space
    SAC_HIDDEN_DIMS = [256, 256]
    SAC_ACTOR_LR = 0.0003
    SAC_CRITIC_LR = 0.0003
    SAC_ALPHA_LR = 0.0003
    SAC_GAMMA = 0.99
    SAC_TAU = 0.005
    SAC_BUFFER_SIZE = 100000
    SAC_EPOCHS = 500
    SAC_BATCH_SIZE = 256
    SAC_INITIAL_RANDOM = 10000
    
    # Dreamer parameters
    DREAMER_STATE_DIM = 10
    DREAMER_ACTION_DIM = 1  # Continuous action space
    DREAMER_DETER_DIM = 200
    DREAMER_STOCH_DIM = 30
    DREAMER_HIDDEN_DIM = 200
    DREAMER_ACTOR_LR = 0.0001
    DREAMER_CRITIC_LR = 0.0001
    DREAMER_WORLD_LR = 0.0001
    DREAMER_GAMMA = 0.99
    DREAMER_LAMBDA_GAE = 0.95
    DREAMER_HORIZON = 15
    DREAMER_EPOCHS = 1000
    DREAMER_BATCH_SIZE = 50
    DREAMER_COLLECT_STEPS = 1000
    DREAMER_TRAIN_STEPS = 100

================
File: core/data_utils.py
================
"""
Data utilities for loading, preprocessing, and splitting financial data.
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import logging

logger = logging.getLogger(__name__)


def load_data(filepath):
    """
    Load financial data from CSV file.
    
    Args:
        filepath (str): Path to the CSV file
        
    Returns:
        pandas.DataFrame: Loaded data
    """
    logger.info(f"Loading data from {filepath}")
    
    try:
        data = pd.read_csv(filepath)
        
        # Convert datetime column to pandas datetime
        if 'DateTime' in data.columns:
            data['DateTime'] = pd.to_datetime(data['DateTime'])
            data.set_index('DateTime', inplace=True)
        
        logger.info(f"Loaded data with shape {data.shape}")
        return data
    
    except Exception as e:
        logger.error(f"Error loading data: {e}")
        raise


def calculate_returns(data, price_col='gbclose'):
    """
    Calculate return series from price data.
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        
    Returns:
        pandas.DataFrame: Data with returns column added
    """
    data = data.copy()
    data['returns'] = data[price_col].pct_change()
    return data


def calculate_volatility(data, window_sizes=None, returns_col='returns'):
    """
    Calculate volatility at different time windows.
    
    Args:
        data (pandas.DataFrame): Data with returns
        window_sizes (list): List of window sizes for calculating volatility
        returns_col (str): Name of the returns column
        
    Returns:
        pandas.DataFrame: Data with volatility columns added
    """
    if window_sizes is None:
        window_sizes = [5, 10, 20, 50]
    
    data = data.copy()
    
    for window in window_sizes:
        data[f'volatility_{window}'] = data[returns_col].rolling(window=window).std()
    
    # Create a main volatility column (20-day by default)
    data['volatility'] = data['volatility_20']
    
    return data


def calculate_rsi(data, price_col='gbclose', window=14):
    """
    Calculate Relative Strength Index (RSI).
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        window (int): Window size for RSI calculation
        
    Returns:
        numpy.ndarray: RSI values
    """
    # Calculate price changes
    delta = data[price_col].diff()
    
    # Create arrays for gains and losses
    gain = delta.copy()
    loss = delta.copy()
    
    gain[gain < 0] = 0
    loss[loss > 0] = 0
    loss = -loss  # Convert losses to positive values
    
    # Calculate average gain and loss over the window
    avg_gain = gain.rolling(window=window).mean()
    avg_loss = loss.rolling(window=window).mean()
    
    # Calculate RS (Relative Strength)
    rs = avg_gain / avg_loss
    
    # Calculate RSI
    rsi = 100 - (100 / (1 + rs))
    
    return rsi


def calculate_ema(data, price_col='gbclose', window=9):
    """
    Calculate Exponential Moving Average (EMA).
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        window (int): Window size for EMA calculation
        
    Returns:
        numpy.ndarray: EMA values
    """
    ema = data[price_col].ewm(span=window, adjust=False).mean()
    return ema


def calculate_macd(data, price_col='gbclose', fast_window=12, slow_window=26, signal_window=9):
    """
    Calculate Moving Average Convergence Divergence (MACD).
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        fast_window (int): Window size for fast EMA
        slow_window (int): Window size for slow EMA
        signal_window (int): Window size for signal line
        
    Returns:
        tuple: (macd_line, signal_line, histogram)
    """
    # Calculate fast and slow EMAs
    fast_ema = data[price_col].ewm(span=fast_window, adjust=False).mean()
    slow_ema = data[price_col].ewm(span=slow_window, adjust=False).mean()
    
    # Calculate MACD line
    macd_line = fast_ema - slow_ema
    
    # Calculate signal line
    signal_line = macd_line.ewm(span=signal_window, adjust=False).mean()
    
    # Calculate histogram
    histogram = macd_line - signal_line
    
    return macd_line, signal_line, histogram


def calculate_bollinger_bands(data, price_col='gbclose', window=20, num_std=2):
    """
    Calculate Bollinger Bands.
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        window (int): Window size for moving average
        num_std (float): Number of standard deviations for bands
        
    Returns:
        tuple: (upper_band, middle_band, lower_band)
    """
    # Calculate middle band (simple moving average)
    middle_band = data[price_col].rolling(window=window).mean()
    
    # Calculate standard deviation
    std = data[price_col].rolling(window=window).std()
    
    # Calculate upper and lower bands
    upper_band = middle_band + (std * num_std)
    lower_band = middle_band - (std * num_std)
    
    # Calculate bandwidth
    bandwidth = (upper_band - lower_band) / middle_band
    
    return upper_band, middle_band, lower_band, bandwidth


def add_technical_indicators(data, indicators=None, price_col='gbclose'):
    """
    Add technical indicators to the data.
    
    Args:
        data (pandas.DataFrame): Price data
        indicators (list): List of indicators to calculate
        price_col (str): Name of the price column
        
    Returns:
        pandas.DataFrame: Data with technical indicators added
    """
    if indicators is None:
        indicators = ['rsi', 'macd', 'bbands', 'ema']
    
    data = data.copy()
    
    if 'rsi' in indicators:
        data['rsi'] = calculate_rsi(data, price_col, window=14)
    
    if 'macd' in indicators:
        macd, macd_signal, macd_hist = calculate_macd(data, price_col)
        data['macd'] = macd
        data['macd_signal'] = macd_signal
        data['macd_hist'] = macd_hist
    
    if 'bbands' in indicators:
        upper, middle, lower, bandwidth = calculate_bollinger_bands(data, price_col)
        data['bbands_upper'] = upper
        data['bbands_middle'] = middle
        data['bbands_lower'] = lower
        data['bbands_width'] = bandwidth
    
    if 'ema' in indicators:
        data['ema_9'] = calculate_ema(data, price_col, window=9)
        data['ema_21'] = calculate_ema(data, price_col, window=21)
        data['ema_diff'] = data['ema_9'] - data['ema_21']
    
    return data


def normalize_features(data, features_to_normalize=None):
    """
    Normalize specified features in the data.
    
    Args:
        data (pandas.DataFrame): Data
        features_to_normalize (list): List of features to normalize
        
    Returns:
        pandas.DataFrame: Data with normalized features
        StandardScaler: Fitted scaler for later use
    """
    if features_to_normalize is None:
        features_to_normalize = ['returns', 'volatility']
    
    data = data.copy()
    
    # Create a scaler for the selected features
    scaler = StandardScaler()
    
    # Ensure all specified features exist in the dataframe
    available_features = [f for f in features_to_normalize if f in data.columns]
    
    if available_features:
        data[available_features] = scaler.fit_transform(
            data[available_features].fillna(0)
        )
    
    return data, scaler


def preprocess_data(data, params=None):
    """
    Preprocess financial data for model training.
    
    Args:
        data (pandas.DataFrame): Raw financial data
        params (dict): Preprocessing parameters
        
    Returns:
        pandas.DataFrame: Preprocessed data
    """
    if params is None:
        params = {
            'calculate_returns': True,
            'calculate_volatility': True,
            'window_sizes': [5, 10, 20, 50],
            'features_to_normalize': ['returns', 'volatility', 'rsi', 'ema_diff'],
            'technical_indicators': ['rsi', 'macd', 'bbands', 'ema']
        }
    
    logger.info("Starting data preprocessing")
    
    # Make a copy of the data
    processed_data = data.copy()
    
    # Calculate returns
    if params.get('calculate_returns', True):
        processed_data = calculate_returns(processed_data)
    
    # Add technical indicators
    processed_data = add_technical_indicators(
        processed_data, 
        indicators=params.get('technical_indicators', ['rsi', 'macd', 'bbands', 'ema'])
    )
    
    # Calculate volatility
    if params.get('calculate_volatility', True):
        processed_data = calculate_volatility(
            processed_data, 
            window_sizes=params.get('window_sizes', [5, 10, 20, 50])
        )
    
    # Normalize features
    processed_data, scaler = normalize_features(
        processed_data, 
        features_to_normalize=params.get('features_to_normalize', ['returns', 'volatility'])
    )
    
    # Drop NA values
    processed_data = processed_data.dropna()
    
    logger.info(f"Data preprocessing complete. Final shape: {processed_data.shape}")
    
    return processed_data


def train_test_split(data, test_size=0.3, random_state=None):
    """
    Split data into training and testing sets.
    
    Args:
        data (pandas.DataFrame): Data to split
        test_size (float): Proportion of data to use for testing
        random_state (int): Random seed for reproducibility
        
    Returns:
        tuple: (train_data, test_data)
    """
    if random_state is not None:
        np.random.seed(random_state)
    
    # Calculate split point
    split_idx = int(len(data) * (1 - test_size))
    
    # Split data
    train_data = data.iloc[:split_idx].copy()
    test_data = data.iloc[split_idx:].copy()
    
    logger.info(f"Data split into train ({len(train_data)} rows) and test ({len(test_data)} rows) sets")
    
    return train_data, test_data

================
File: data_utils.py
================
"""
Data utilities for loading, preprocessing, and splitting financial data.
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import logging

logger = logging.getLogger(__name__)


def load_data(filepath):
    """
    Load financial data from CSV file.
    
    Args:
        filepath (str): Path to the CSV file
        
    Returns:
        pandas.DataFrame: Loaded data
    """
    logger.info(f"Loading data from {filepath}")
    
    try:
        data = pd.read_csv(filepath)
        
        # Convert datetime column to pandas datetime
        if 'DateTime' in data.columns:
            data['DateTime'] = pd.to_datetime(data['DateTime'])
            data.set_index('DateTime', inplace=True)
        
        logger.info(f"Loaded data with shape {data.shape}")
        return data
    
    except Exception as e:
        logger.error(f"Error loading data: {e}")
        raise


def calculate_returns(data, price_col='gbclose'):
    """
    Calculate return series from price data.
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        
    Returns:
        pandas.DataFrame: Data with returns column added
    """
    data = data.copy()
    data['returns'] = data[price_col].pct_change()
    return data


def calculate_volatility(data, window_sizes=None, returns_col='returns'):
    """
    Calculate volatility at different time windows.
    
    Args:
        data (pandas.DataFrame): Data with returns
        window_sizes (list): List of window sizes for calculating volatility
        returns_col (str): Name of the returns column
        
    Returns:
        pandas.DataFrame: Data with volatility columns added
    """
    if window_sizes is None:
        window_sizes = [5, 10, 20, 50]
    
    data = data.copy()
    
    for window in window_sizes:
        data[f'volatility_{window}'] = data[returns_col].rolling(window=window).std()
    
    # Create a main volatility column (20-day by default)
    data['volatility'] = data['volatility_20']
    
    return data


def calculate_rsi(data, price_col='gbclose', window=14):
    """
    Calculate Relative Strength Index (RSI).
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        window (int): Window size for RSI calculation
        
    Returns:
        numpy.ndarray: RSI values
    """
    # Calculate price changes
    delta = data[price_col].diff()
    
    # Create arrays for gains and losses
    gain = delta.copy()
    loss = delta.copy()
    
    gain[gain < 0] = 0
    loss[loss > 0] = 0
    loss = -loss  # Convert losses to positive values
    
    # Calculate average gain and loss over the window
    avg_gain = gain.rolling(window=window).mean()
    avg_loss = loss.rolling(window=window).mean()
    
    # Calculate RS (Relative Strength)
    rs = avg_gain / avg_loss
    
    # Calculate RSI
    rsi = 100 - (100 / (1 + rs))
    
    return rsi


def calculate_ema(data, price_col='gbclose', window=9):
    """
    Calculate Exponential Moving Average (EMA).
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        window (int): Window size for EMA calculation
        
    Returns:
        numpy.ndarray: EMA values
    """
    ema = data[price_col].ewm(span=window, adjust=False).mean()
    return ema


def calculate_macd(data, price_col='gbclose', fast_window=12, slow_window=26, signal_window=9):
    """
    Calculate Moving Average Convergence Divergence (MACD).
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        fast_window (int): Window size for fast EMA
        slow_window (int): Window size for slow EMA
        signal_window (int): Window size for signal line
        
    Returns:
        tuple: (macd_line, signal_line, histogram)
    """
    # Calculate fast and slow EMAs
    fast_ema = data[price_col].ewm(span=fast_window, adjust=False).mean()
    slow_ema = data[price_col].ewm(span=slow_window, adjust=False).mean()
    
    # Calculate MACD line
    macd_line = fast_ema - slow_ema
    
    # Calculate signal line
    signal_line = macd_line.ewm(span=signal_window, adjust=False).mean()
    
    # Calculate histogram
    histogram = macd_line - signal_line
    
    return macd_line, signal_line, histogram


def calculate_bollinger_bands(data, price_col='gbclose', window=20, num_std=2):
    """
    Calculate Bollinger Bands.
    
    Args:
        data (pandas.DataFrame): Price data
        price_col (str): Name of the price column
        window (int): Window size for moving average
        num_std (float): Number of standard deviations for bands
        
    Returns:
        tuple: (upper_band, middle_band, lower_band)
    """
    # Calculate middle band (simple moving average)
    middle_band = data[price_col].rolling(window=window).mean()
    
    # Calculate standard deviation
    std = data[price_col].rolling(window=window).std()
    
    # Calculate upper and lower bands
    upper_band = middle_band + (std * num_std)
    lower_band = middle_band - (std * num_std)
    
    # Calculate bandwidth
    bandwidth = (upper_band - lower_band) / middle_band
    
    return upper_band, middle_band, lower_band, bandwidth


def add_technical_indicators(data, indicators=None, price_col='gbclose'):
    """
    Add technical indicators to the data.
    
    Args:
        data (pandas.DataFrame): Price data
        indicators (list): List of indicators to calculate
        price_col (str): Name of the price column
        
    Returns:
        pandas.DataFrame: Data with technical indicators added
    """
    if indicators is None:
        indicators = ['rsi', 'macd', 'bbands', 'ema']
    
    data = data.copy()
    
    if 'rsi' in indicators:
        data['rsi'] = calculate_rsi(data, price_col, window=14)
    
    if 'macd' in indicators:
        macd, macd_signal, macd_hist = calculate_macd(data, price_col)
        data['macd'] = macd
        data['macd_signal'] = macd_signal
        data['macd_hist'] = macd_hist
    
    if 'bbands' in indicators:
        upper, middle, lower, bandwidth = calculate_bollinger_bands(data, price_col)
        data['bbands_upper'] = upper
        data['bbands_middle'] = middle
        data['bbands_lower'] = lower
        data['bbands_width'] = bandwidth
    
    if 'ema' in indicators:
        data['ema_9'] = calculate_ema(data, price_col, window=9)
        data['ema_21'] = calculate_ema(data, price_col, window=21)
        data['ema_diff'] = data['ema_9'] - data['ema_21']
    
    return data


def normalize_features(data, features_to_normalize=None):
    """
    Normalize specified features in the data.
    
    Args:
        data (pandas.DataFrame): Data
        features_to_normalize (list): List of features to normalize
        
    Returns:
        pandas.DataFrame: Data with normalized features
        StandardScaler: Fitted scaler for later use
    """
    if features_to_normalize is None:
        features_to_normalize = ['returns', 'volatility']
    
    data = data.copy()
    
    # Create a scaler for the selected features
    scaler = StandardScaler()
    
    # Ensure all specified features exist in the dataframe
    available_features = [f for f in features_to_normalize if f in data.columns]
    
    if available_features:
        data[available_features] = scaler.fit_transform(
            data[available_features].fillna(0)
        )
    
    return data, scaler


def preprocess_data(data, params=None):
    """
    Preprocess financial data for model training.
    
    Args:
        data (pandas.DataFrame): Raw financial data
        params (dict): Preprocessing parameters
        
    Returns:
        pandas.DataFrame: Preprocessed data
    """
    if params is None:
        params = {
            'calculate_returns': True,
            'calculate_volatility': True,
            'window_sizes': [5, 10, 20, 50],
            'features_to_normalize': ['returns', 'volatility', 'rsi', 'ema_diff'],
            'technical_indicators': ['rsi', 'macd', 'bbands', 'ema']
        }
    
    logger.info("Starting data preprocessing")
    
    # Make a copy of the data
    processed_data = data.copy()
    
    # Calculate returns
    if params.get('calculate_returns', True):
        processed_data = calculate_returns(processed_data)
    
    # Add technical indicators
    processed_data = add_technical_indicators(
        processed_data, 
        indicators=params.get('technical_indicators', ['rsi', 'macd', 'bbands', 'ema'])
    )
    
    # Calculate volatility
    if params.get('calculate_volatility', True):
        processed_data = calculate_volatility(
            processed_data, 
            window_sizes=params.get('window_sizes', [5, 10, 20, 50])
        )
    
    # Normalize features
    processed_data, scaler = normalize_features(
        processed_data, 
        features_to_normalize=params.get('features_to_normalize', ['returns', 'volatility'])
    )
    
    # Drop NA values
    processed_data = processed_data.dropna()
    
    logger.info(f"Data preprocessing complete. Final shape: {processed_data.shape}")
    
    return processed_data


def train_test_split(data, test_size=0.3, random_state=None):
    """
    Split data into training and testing sets.
    
    Args:
        data (pandas.DataFrame): Data to split
        test_size (float): Proportion of data to use for testing
        random_state (int): Random seed for reproducibility
        
    Returns:
        tuple: (train_data, test_data)
    """
    if random_state is not None:
        np.random.seed(random_state)
    
    # Calculate split point
    split_idx = int(len(data) * (1 - test_size))
    
    # Split data
    train_data = data.iloc[:split_idx].copy()
    test_data = data.iloc[split_idx:].copy()
    
    logger.info(f"Data split into train ({len(train_data)} rows) and test ({len(test_data)} rows) sets")
    
    return train_data, test_data

================
File: dqn_position_sizing.py
================
"""
Deep Q-Network (DQN) based Dynamic Position Sizing module.

This module provides functionality for dynamically sizing trading positions
based on meta-labeling confidence, market regimes, and transaction costs
using a DQN reinforcement learning approach.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
from collections import deque, namedtuple
import pickle
import os
import matplotlib.pyplot as plt

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Experience replay memory
Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))

class ReplayMemory:
    """Experience replay buffer to store and sample experiences."""
    
    def __init__(self, capacity=10000):
        """
        Initialize replay memory.
        
        Parameters:
        -----------
        capacity : int
            Maximum capacity of the buffer
        """
        self.memory = deque(maxlen=capacity)
        
    def push(self, state, action, reward, next_state, done):
        """Store experience in memory."""
        self.memory.append(Experience(state, action, reward, next_state, done))
        
    def sample(self, batch_size):
        """Sample random batch from memory."""
        return random.sample(self.memory, min(len(self.memory), batch_size))
    
    def can_sample(self, batch_size):
        """Check if enough samples are available."""
        return len(self.memory) >= batch_size
    
    def __len__(self):
        return len(self.memory)


class DQN(nn.Module):
    """Deep Q-Network architecture."""
    
    def __init__(self, input_dim, output_dim, hidden_dim=64):
        """
        Initialize DQN model.
        
        Parameters:
        -----------
        input_dim : int
            Dimension of state space
        output_dim : int
            Dimension of action space
        hidden_dim : int
            Size of hidden layers
        """
        super(DQN, self).__init__()
        
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x):
        """Forward pass through network."""
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


class DQNPositionSizer:
    """
    DQN-based Reinforcement Learning agent for dynamic position sizing
    that aims to minimize transaction costs while maximizing returns.
    """
    
    def __init__(self, state_dim=12, n_actions=3, learning_rate=0.001,
                 gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, 
                 epsilon_decay=0.995, memory_size=10000, batch_size=64,
                 target_update=10, smoothing_alpha=0.2, transaction_cost=0.000002):
        """
        Initialize the DQN Position Sizer.
        
        Parameters:
        -----------
        state_dim : int
            Dimension of the state space
        n_actions : int
            Number of discrete actions
        learning_rate : float
            Learning rate for neural network
        gamma : float
            Discount factor for future rewards
        epsilon_start : float
            Initial exploration rate
        epsilon_end : float
            Final exploration rate
        epsilon_decay : float
            Rate at which exploration rate decays
        memory_size : int
            Size of replay memory
        batch_size : int
            Batch size for training
        target_update : int
            Frequency of target network updates
        smoothing_alpha : float
            EMA smoothing factor for position size changes
        transaction_cost : float
            Cost per unit of position size change
        """
        self.state_dim = state_dim
        self.n_actions = n_actions
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update = target_update
        self.smoothing_alpha = smoothing_alpha
        self.transaction_cost = transaction_cost
        
        # Initialize networks
        self.policy_net = DQN(state_dim, n_actions).to(device)
        self.target_net = DQN(state_dim, n_actions).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        
        # Initialize optimizer
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        
        # Initialize replay memory
        self.memory = ReplayMemory(memory_size)
        
        # Initialize step counter
        self.steps_done = 0
        
        # Current position size
        self.current_position_size = 0.0
        
        # For tracking performance
        self.total_reward = 0.0
        self.total_transaction_cost = 0.0
        self.episode_count = 0
        
        # Initialize action mapping
        self.action_mapping = {
            0: -0.25,  # Decrease position
            1: 0.0,    # Hold position
            2: 0.25    # Increase position
        }
        
        # Initialize state history
        self.state_history = []
        
        # Track losses
        self.losses = []
    
    def prepare_state(self, meta_signal, regime, features, position):
        """
        Prepare the state representation.
        
        Parameters:
        -----------
        meta_signal : float
            Meta-labeling confidence or signal
        regime : int
            Market regime identifier
        features : dict or list
            Technical features like RSI, volatility, etc.
        position : float
            Current position size
            
        Returns:
        --------
        torch.Tensor: State tensor
        """
        # Convert regime to one-hot encoding (assuming max 3 regimes)
        regime_onehot = [0, 0, 0]
        if regime < len(regime_onehot):
            regime_onehot[int(regime)] = 1
        
        # Combine all state components
        state = [meta_signal, position] + regime_onehot + list(features)
        
        # Normalize state (simple min-max scaling)
        state_tensor = torch.FloatTensor(state).to(device)
        
        return state_tensor
    
    def select_action(self, state, training=True):
        """
        Select action using epsilon-greedy policy.
        
        Parameters:
        -----------
        state : torch.Tensor
            Current state
        training : bool
            Whether to use exploration or exploitation
            
        Returns:
        --------
        int: Selected action index
        """
        if training and random.random() < self.epsilon:
            # Exploration: select random action
            return random.randrange(self.n_actions)
        else:
            # Exploitation: select best action from Q-network
            with torch.no_grad():
                return self.policy_net(state).max(0)[1].item()
    
    def update_network(self):
        """Update policy network using batch from replay memory."""
        if not self.memory.can_sample(self.batch_size):
            return
        
        experiences = self.memory.sample(self.batch_size)
        batch = Experience(*zip(*experiences))
        
        # Convert to tensors
        state_batch = torch.stack(batch.state)
        action_batch = torch.tensor(batch.action, device=device, dtype=torch.long).unsqueeze(1)
        reward_batch = torch.tensor(batch.reward, device=device, dtype=torch.float)
        
        # Handle non-final states
        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), 
                                     device=device, dtype=torch.bool)
        non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])
        
        # Compute Q values
        q_values = self.policy_net(state_batch).gather(1, action_batch)
        
        # Compute target Q values
        next_q_values = torch.zeros(self.batch_size, device=device)
        if len(non_final_next_states) > 0:
            with torch.no_grad():
                next_q_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]
        
        target_q_values = reward_batch + self.gamma * next_q_values
        
        # Compute loss
        loss = F.smooth_l1_loss(q_values, target_q_values.unsqueeze(1))
        self.losses.append(loss.item())
        
        # Optimize the model
        self.optimizer.zero_grad()
        loss.backward()
        # Clip gradients to stabilize training
        for param in self.policy_net.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()
        
        # Update target network if needed
        if self.steps_done % self.target_update == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())
        
        # Update epsilon
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
    
    def calculate_reward(self, profit, size_change):
        """
        Calculate reward based on profit and transaction costs.
        
        Parameters:
        -----------
        profit : float
            Trading profit
        size_change : float
            Absolute change in position size
            
        Returns:
        --------
        float: Net reward after transaction costs
        """
        # Calculate transaction cost
        cost = abs(size_change) * self.transaction_cost
        
        # Track total transaction cost
        self.total_transaction_cost += cost
        
        # Net reward
        net_reward = profit - cost
        self.total_reward += net_reward
        
        return net_reward
    
    def smooth_position(self, new_size):
        """
        Apply EMA smoothing to position size changes.
        
        Parameters:
        -----------
        new_size : float
            New position size
            
        Returns:
        --------
        float: Smoothed position size
        """
        return self.smoothing_alpha * new_size + (1 - self.smoothing_alpha) * self.current_position_size
    
    def decide_position_size(self, meta_signal, regime, features, return_value, base_size=1.0, training=True):
        """
        Decide the position size based on current state and policy.
        
        Parameters:
        -----------
        meta_signal : float
            Meta-labeling signal (-1 to 1)
        regime : int
            Market regime identifier
        features : dict or list
            Technical features
        return_value : float
            Latest return
        base_size : float
            Base position size multiplier
        training : bool
            Whether in training mode
            
        Returns:
        --------
        float: New position size
        """
        # Prepare state
        state = self.prepare_state(meta_signal, regime, features, self.current_position_size)
        
        # Store state for history
        self.state_history.append(state.cpu().numpy())
        
        # Select action
        action = self.select_action(state, training)
        
        # Calculate position change
        position_change = self.action_mapping[action]
        
        # Calculate new position
        target_position = np.clip(self.current_position_size + position_change, -1.0, 1.0) * base_size
        
        # Calculate size change and reward
        size_change = target_position - self.current_position_size
        position_sign = 1 if self.current_position_size > 0 else (-1 if self.current_position_size < 0 else 0)
        profit = position_sign * return_value * abs(self.current_position_size)
        reward = self.calculate_reward(profit, size_change)
        
        # Apply smoothing to position size
        smoothed_position = self.smooth_position(target_position)
        
        # Store transition in replay memory if in training
        if training:
            # Prepare next state
            next_state = self.prepare_state(meta_signal, regime, features, smoothed_position)
            done = False  # In this context, episodes don't really end
            
            # Store in memory
            self.memory.push(state, action, reward, next_state, done)
            
            # Update networks
            self.update_network()
            
            # Increment steps
            self.steps_done += 1
        
        # Update current position
        self.current_position_size = smoothed_position
        
        return smoothed_position
    
    def train(self, data, meta_col, regime_col, features_cols, returns_col, 
              episodes=100, base_size=1.0, eval_interval=10):
        """
        Train the DQN agent on historical data.
        
        Parameters:
        -----------
        data : DataFrame
            Historical data with required columns
        meta_col : str
            Column name for meta-labeling signal
        regime_col : str
            Column name for market regime
        features_cols : list
            List of column names for technical features
        returns_col : str
            Column name for returns
        episodes : int
            Number of training episodes
        base_size : float
            Base position size
        eval_interval : int
            Interval for evaluation during training
            
        Returns:
        --------
        DataFrame: Performance statistics from training
        """
        performance_stats = []
        
        for episode in range(episodes):
            # Reset environment
            self.current_position_size = 0.0
            self.total_reward = 0.0
            self.total_transaction_cost = 0.0
            
            # Reset epsilon at the beginning of each episode
            if episode == 0:
                self.epsilon = 1.0
            
            cum_return = 0.0
            positions = []
            
            # Iterate through each timestep
            for i in range(len(data)):
                row = data.iloc[i]
                meta_signal = row[meta_col]
                regime = row[regime_col]
                
                # Extract features
                features = row[features_cols].values
                
                # Get return
                ret = row[returns_col]
                
                # Get position size
                position = self.decide_position_size(
                    meta_signal, regime, features, ret, base_size, training=True
                )
                positions.append(position)
                
                # Calculate period return (simplified)
                if i > 0:  # Use previous position to calculate return (avoid lookahead bias)
                    period_return = positions[-2] * ret
                    cum_return += period_return
            
            # Record performance
            performance_stats.append({
                'episode': episode,
                'final_reward': self.total_reward,
                'transaction_costs': self.total_transaction_cost,
                'cumulative_return': cum_return,
                'sharpe_ratio': cum_return / (np.std(positions) + 1e-6)
            })
            
            self.episode_count += 1
            
            # Print progress
            if (episode + 1) % eval_interval == 0:
                print(f"Episode {episode+1}/{episodes} - "
                      f"Reward: {self.total_reward:.4f}, "
                      f"Return: {cum_return:.4f}, "
                      f"Costs: {self.total_transaction_cost:.4f}, "
                      f"Epsilon: {self.epsilon:.2f}")
        
        # Plot losses
        plt.figure(figsize=(10, 5))
        plt.plot(self.losses)
        plt.title('DQN Training Loss')
        plt.xlabel('Update Step')
        plt.ylabel('Loss')
        plt.savefig('dqn_training_loss.png')
        
        return pd.DataFrame(performance_stats)
    
    def apply(self, data, meta_col, regime_col, features_cols, returns_col=None, base_size=1.0):
        """
        Apply the trained DQN agent to new data.
        
        Parameters:
        -----------
        data : DataFrame
            New data with required columns
        meta_col : str
            Column name for meta-labeling signal
        regime_col : str
            Column name for market regime
        features_cols : list
            List of column names for technical features
        returns_col : str
            Column name for returns (optional)
        base_size : float
            Base position size
            
        Returns:
        --------
        DataFrame: Original data with position sizes added
        """
        # Make a copy to avoid modifying the original
        result = data.copy()
        result['dqn_position_size'] = 0.0
        
        # Reset position size
        self.current_position_size = 0.0
        
        # Apply the model to each row (no exploration)
        for i in range(len(result)):
            row = result.iloc[i]
            meta_signal = row[meta_col]
            regime = row[regime_col]
            
            # Extract features
            features = row[features_cols].values
            
            # Use default return value if column not provided
            ret = row[returns_col] if returns_col and returns_col in row else 0
            
            # Get position size without training
            position = self.decide_position_size(
                meta_signal, regime, features, ret, base_size, training=False
            )
            
            # Store position size
            result.loc[result.index[i], 'dqn_position_size'] = position
        
        return result
    
    def save(self, filepath):
        """
        Save the trained model to files.
        
        Parameters:
        -----------
        filepath : str
            Base path to save the model
        """
        # Save DQN parameters
        model_path = f"{filepath}_model.pth"
        torch.save({
            'policy_net': self.policy_net.state_dict(),
            'target_net': self.target_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'steps_done': self.steps_done,
            'epsilon': self.epsilon
        }, model_path)
        
        # Save configuration
        config_path = f"{filepath}_config.pkl"
        config = {
            'state_dim': self.state_dim,
            'n_actions': self.n_actions,
            'gamma': self.gamma,
            'epsilon_end': self.epsilon_end,
            'epsilon_decay': self.epsilon_decay,
            'batch_size': self.batch_size,
            'target_update': self.target_update,
            'smoothing_alpha': self.smoothing_alpha,
            'transaction_cost': self.transaction_cost,
            'action_mapping': self.action_mapping,
            'current_position_size': self.current_position_size,
            'total_reward': self.total_reward,
            'total_transaction_cost': self.total_transaction_cost,
            'episode_count': self.episode_count
        }
        
        with open(config_path, 'wb') as f:
            pickle.dump(config, f)
            
        print(f"Model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath, learning_rate=0.001):
        """
        Load a trained model from files.
        
        Parameters:
        -----------
        filepath : str
            Base path to load the model from
        learning_rate : float
            Learning rate for optimizer initialization
            
        Returns:
        --------
        DQNPositionSizer: Loaded model
        """
        # Load configuration
        config_path = f"{filepath}_config.pkl"
        with open(config_path, 'rb') as f:
            config = pickle.load(f)
        
        # Create instance with loaded config
        sizer = cls(
            state_dim=config['state_dim'],
            n_actions=config['n_actions'],
            gamma=config['gamma'],
            epsilon_start=config['epsilon'],
            epsilon_end=config['epsilon_end'],
            epsilon_decay=config['epsilon_decay'],
            batch_size=config['batch_size'],
            target_update=config['target_update'],
            smoothing_alpha=config['smoothing_alpha'],
            transaction_cost=config['transaction_cost'],
            learning_rate=learning_rate
        )
        
        # Set restored values
        sizer.action_mapping = config['action_mapping']
        sizer.current_position_size = config['current_position_size']
        sizer.total_reward = config['total_reward']
        sizer.total_transaction_cost = config['total_transaction_cost']
        sizer.episode_count = config['episode_count']
        
        # Load model state
        model_path = f"{filepath}_model.pth"
        checkpoint = torch.load(model_path)
        
        # Load policy and target networks
        sizer.policy_net.load_state_dict(checkpoint['policy_net'])
        sizer.target_net.load_state_dict(checkpoint['target_net'])
        sizer.optimizer.load_state_dict(checkpoint['optimizer'])
        sizer.steps_done = checkpoint['steps_done']
        sizer.epsilon = checkpoint['epsilon']
        
        return sizer


def calculate_transaction_costs(positions, transaction_cost=0.000002):
    """
    Calculate transaction costs from a series of positions.
    
    Parameters:
    -----------
    positions : array-like
        Series of position sizes
    transaction_cost : float
        Cost per unit of position size change
        
    Returns:
    --------
    float: Total transaction costs
    """
    if len(positions) <= 1:
        return 0.0
    
    # Calculate absolute differences between consecutive positions
    position_changes = np.abs(np.diff(positions))
    
    # Calculate transaction costs
    total_cost = np.sum(position_changes) * transaction_cost
    
    return total_cost


def evaluate_dqn_strategy(data, base_position_col, dqn_position_col, returns_col, 
                          transaction_cost=0.000002):
    """
    Evaluate the DQN strategy against the base strategy.
    
    Parameters:
    -----------
    data : DataFrame
        Data with positions and returns
    base_position_col : str
        Column name for base strategy positions
    dqn_position_col : str
        Column name for DQN strategy positions
    returns_col : str
        Column name for returns
    transaction_cost : float
        Cost per unit of position size change
        
    Returns:
    --------
    dict: Performance metrics
    """
    # Make a copy of the data
    result = data.copy()
    
    # Calculate returns for both strategies (before transaction costs)
    result['base_return'] = result[base_position_col].shift(1) * result[returns_col]  # Shift to avoid lookahead bias
    result['dqn_return'] = result[dqn_position_col].shift(1) * result[returns_col]    # Shift to avoid lookahead bias
    
    # Calculate cumulative returns
    result['base_cum_return'] = result['base_return'].cumsum()
    result['dqn_cum_return'] = result['dqn_return'].cumsum()
    
    # Calculate transaction costs
    base_costs = calculate_transaction_costs(result[base_position_col].values, transaction_cost)
    dqn_costs = calculate_transaction_costs(result[dqn_position_col].values, transaction_cost)
    
    # Adjust final returns for transaction costs
    base_net_return = result['base_cum_return'].iloc[-1] - base_costs
    dqn_net_return = result['dqn_cum_return'].iloc[-1] - dqn_costs
    
    # Calculate performance metrics
    base_sharpe = result['base_return'].mean() / (result['base_return'].std() + 1e-6) * np.sqrt(252)
    dqn_sharpe = result['dqn_return'].mean() / (result['dqn_return'].std() + 1e-6) * np.sqrt(252)
    
    # Calculate average position size and turnover
    base_avg_size = np.mean(np.abs(result[base_position_col]))
    dqn_avg_size = np.mean(np.abs(result[dqn_position_col]))
    
    base_turnover = np.sum(np.abs(np.diff(result[base_position_col]))) / len(result)
    dqn_turnover = np.sum(np.abs(np.diff(result[dqn_position_col]))) / len(result)
    
    # Create performance chart
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 1, 1)
    plt.plot(result.index, result['base_cum_return'], label=f'Base (Net: {base_net_return:.4f})')
    plt.plot(result.index, result['dqn_cum_return'], label=f'DQN (Net: {dqn_net_return:.4f})')
    plt.title('Cumulative Returns')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(2, 1, 2)
    plt.plot(result.index, result[base_position_col], label='Base Position')
    plt.plot(result.index, result[dqn_position_col], label='DQN Position')
    plt.title('Position Sizes')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('dqn_performance_comparison.png')
    
    metrics = {
        'base_return': base_net_return,
        'dqn_return': dqn_net_return,
        'improvement': dqn_net_return - base_net_return,
        'base_costs': base_costs,
        'dqn_costs': dqn_costs,
        'cost_reduction': base_costs - dqn_costs,
        'base_sharpe': base_sharpe,
        'dqn_sharpe': dqn_sharpe,
        'base_avg_size': base_avg_size,
        'dqn_avg_size': dqn_avg_size,
        'base_turnover': base_turnover,
        'dqn_turnover': dqn_turnover
    }
    
    # Print metrics
    print("\nPerformance Metrics:")
    print("-" * 80)
    print(f"{'Metric':<20} {'Base Strategy':<15} {'DQN Strategy':<15} {'Improvement':<15}")
    print("-" * 80)
    print(f"{'Net Return':<20} {base_net_return:>15.4f} {dqn_net_return:>15.4f} {dqn_net_return - base_net_return:>15.4f}")
    print(f"{'Transaction Costs':<20} {base_costs:>15.4f} {dqn_costs:>15.4f} {base_costs - dqn_costs:>15.4f}")
    print(f"{'Sharpe Ratio':<20} {base_sharpe:>15.4f} {dqn_sharpe:>15.4f} {dqn_sharpe - base_sharpe:>15.4f}")
    print(f"{'Turnover':<20} {base_turnover:>15.4f} {dqn_turnover:>15.4f} {base_turnover - dqn_turnover:>15.4f}")
    print("-" * 80)
    
    return metrics

================
File: dreamer_position_sizing.py
================
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal
import pandas as pd
from collections import deque
import random
import math
import os
import pickle

class DreamerWorldModel(nn.Module):
    """
    World model for Dreamer algorithm, comprising:
    - Encoder: encodes observations to embeddings
    - RSSM: recurrent state-space model with deterministic and stochastic states
    - Decoder: reconstructs observations from states
    - Reward predictor: predicts rewards from states
    """
    def __init__(self, obs_dim, action_dim, hidden_dim=200, rssm_hidden_dim=200, 
                 stoch_dim=30, deter_dim=200, activation=nn.ELU):
        super().__init__()
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.rssm_hidden_dim = rssm_hidden_dim
        self.stoch_dim = stoch_dim
        self.deter_dim = deter_dim
        self.activation = activation
        
        # Encoder: observations -> embeddings
        self.encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, hidden_dim),
            activation()
        )
        
        # RSSM components
        # GRU for deterministic state
        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)
        
        # Prior: predicts stochastic state from deterministic state (without observations)
        self.prior = nn.Sequential(
            nn.Linear(deter_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, 2 * stoch_dim)  # mean and std
        )
        
        # Posterior: predicts stochastic state from deterministic state and observations
        self.posterior = nn.Sequential(
            nn.Linear(deter_dim + hidden_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, 2 * stoch_dim)  # mean and std
        )
        
        # Decoder: reconstructs observations from states
        self.decoder = nn.Sequential(
            nn.Linear(stoch_dim + deter_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, obs_dim)
        )
        
        # Reward predictor: predicts rewards from states
        self.reward_predictor = nn.Sequential(
            nn.Linear(stoch_dim + deter_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Initialize parameters
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_normal_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
                
    def encode(self, obs):
        """Encode observations to embeddings"""
        return self.encoder(obs)
    
    def get_stoch_state(self, mean_std):
        """Sample stochastic state from mean and std parameters"""
        mean, std = torch.chunk(mean_std, 2, dim=-1)
        std = F.softplus(std) + 0.1
        dist = Normal(mean, std)
        stoch_state = dist.rsample()
        return stoch_state, mean, std
    
    def rssm_step(self, prev_stoch, prev_deter, action, embed=None):
        """
        RSSM step: compute next states given previous states and action
        If embed is provided, compute posterior, otherwise compute prior
        """
        # GRU step for deterministic state
        gru_input = torch.cat([prev_stoch, action], dim=-1)
        deter = self.gru(gru_input, prev_deter)
        
        # Compute prior or posterior
        if embed is None:
            # Prior: no observation available
            prior_mean_std = self.prior(deter)
            prior_mean, prior_std = torch.chunk(prior_mean_std, 2, dim=-1)
            prior_std = F.softplus(prior_std) + 0.1
            
            # Sample from prior
            dist = Normal(prior_mean, prior_std)
            stoch = dist.rsample()
            
            posterior = None
        else:
            # Posterior: with observation
            posterior_mean_std = self.posterior(torch.cat([deter, embed], dim=-1))
            posterior_mean, posterior_std = torch.chunk(posterior_mean_std, 2, dim=-1)
            posterior_std = F.softplus(posterior_std) + 0.1
            
            # Sample from posterior
            dist = Normal(posterior_mean, posterior_std)
            stoch = dist.rsample()
            
            # Also compute prior for KL calculation
            prior_mean_std = self.prior(deter)
            prior_mean, prior_std = torch.chunk(prior_mean_std, 2, dim=-1)
            prior_std = F.softplus(prior_std) + 0.1
            
            posterior = (posterior_mean, posterior_std, prior_mean, prior_std)
        
        return stoch, deter, posterior
    
    def forward(self, obs, action, prev_stoch, prev_deter):
        """
        Forward pass through the world model
        Returns next states, posterior stats, predicted observations and rewards
        """
        embed = self.encode(obs)
        stoch, deter, posterior = self.rssm_step(prev_stoch, prev_deter, action, embed)
        
        # Combine stochastic and deterministic states
        state = torch.cat([stoch, deter], dim=-1)
        
        # Reconstruct observation and predict reward
        obs_pred = self.decoder(state)
        reward_pred = self.reward_predictor(state)
        
        return stoch, deter, posterior, obs_pred, reward_pred
    
    def imagine(self, initial_stoch, initial_deter, actions, horizon):
        """
        Imagine trajectories from initial states using actions
        """
        stochs = [initial_stoch]
        deters = [initial_deter]
        
        # Sequentially apply actions and update states
        stoch, deter = initial_stoch, initial_deter
        for t in range(horizon):
            action = actions[:, t] if actions.dim() > 1 else actions
            stoch, deter, _ = self.rssm_step(stoch, deter, action)
            stochs.append(stoch)
            deters.append(deter)
            
        # Stack states
        stochs = torch.stack(stochs, dim=1)
        deters = torch.stack(deters, dim=1)
        states = torch.cat([stochs, deters], dim=-1)
        
        # Predict rewards for the imagined trajectory
        rewards = self.reward_predictor(states).squeeze(-1)
        
        return states, rewards

    def predict_reward(self, state_rep, action):
        """Predict reward from state and action"""
        x = torch.cat([state_rep, action], dim=-1)
        return self.reward_predictor(x)
    
    def decode_observation(self, state_rep):
        """Decode observation from state representation"""
        return self.decoder(state_rep)


class DreamerPolicy(nn.Module):
    """Actor network for Dreamer"""
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        
        # Actor network: state -> action distribution
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ELU()
        )
        
        # Mean and log_std outputs
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)
    
    def forward(self, state):
        """Return action distribution"""
        x = self.actor(state)
        
        # Get mean and std
        mean = self.mean(x)
        log_std = self.log_std(x)
        
        # Bound log_std for stability
        log_std = torch.clamp(log_std, -20, 2)
        std = torch.exp(log_std)
        
        # Create normal distribution
        dist = Normal(mean, std)
        
        return dist


class DreamerCritic(nn.Module):
    """Critic network for Dreamer"""
    def __init__(self, state_dim, hidden_dim=128):
        super().__init__()
        self.state_dim = state_dim
        self.hidden_dim = hidden_dim
        
        # Critic network: state -> value
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        """Return value estimate"""
        return self.critic(state)


class SequentialReplayBuffer:
    """Replay buffer for sequences of experience"""
    def __init__(self, obs_dim, action_dim, capacity=100000):
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.capacity = capacity
        self.observations = []
        self.actions = []
        self.rewards = []
        self.next_observations = []
        self.dones = []
        self.episodes = []
        self.current_episode = []
        self.position = 0
        self.size = 0
    
    def add(self, obs, action, reward, next_obs, done):
        """Add transition to the buffer"""
        transition = {
            'obs': np.array(obs, dtype=np.float32),
            'action': np.array(action, dtype=np.float32),
            'reward': np.array(reward, dtype=np.float32),
            'next_obs': np.array(next_obs, dtype=np.float32) if next_obs is not None else None,
            'done': np.array(done, dtype=np.float32)
        }
        
        # Add to current episode
        self.current_episode.append(transition)
        
        # If episode ends, add to episodes
        if done:
            if len(self.current_episode) > 0:
                self.episodes.append(self.current_episode)
                self.current_episode = []
            
            # If too many episodes, remove oldest
            if len(self.episodes) > self.capacity:
                self.episodes.pop(0)
    
    def sample(self, batch_size, seq_len):
        """Sample a batch of sequences from the buffer"""
        # Ensure we have enough episodes
        if len(self.episodes) == 0:
            raise ValueError("Buffer is empty")
        
        # Sample episodes
        episode_indices = np.random.randint(0, len(self.episodes), size=batch_size)
        
        # Prepare batch
        obs_batch = np.zeros((batch_size, seq_len, self.obs_dim), dtype=np.float32)
        action_batch = np.zeros((batch_size, seq_len, self.action_dim), dtype=np.float32)
        reward_batch = np.zeros((batch_size, seq_len), dtype=np.float32)
        next_obs_batch = np.zeros((batch_size, seq_len, self.obs_dim), dtype=np.float32)
        done_batch = np.zeros((batch_size, seq_len), dtype=np.float32)
        
        # Fill batch with sequences
        for i, episode_idx in enumerate(episode_indices):
            episode = self.episodes[episode_idx]
            
            # Make sure we can sample a sequence of length seq_len
            if len(episode) < seq_len:
                # Pad with zeros if episode is too short
                ep_len = len(episode)
                for t in range(ep_len):
                    obs_batch[i, t] = episode[t]['obs']
                    action_batch[i, t] = episode[t]['action']
                    reward_batch[i, t] = episode[t]['reward']
                    if t < ep_len - 1:
                        next_obs_batch[i, t] = episode[t+1]['obs']
                    else:
                        next_obs_batch[i, t] = episode[t]['next_obs'] if episode[t]['next_obs'] is not None else np.zeros_like(episode[t]['obs'])
                    done_batch[i, t] = episode[t]['done']
            else:
                # Sample a random starting point
                start_idx = np.random.randint(0, len(episode) - seq_len + 1)
                
                # Extract sequence
                for t in range(seq_len):
                    obs_batch[i, t] = episode[start_idx + t]['obs']
                    action_batch[i, t] = episode[start_idx + t]['action']
                    reward_batch[i, t] = episode[start_idx + t]['reward']
                    if t < seq_len - 1:
                        next_obs_batch[i, t] = episode[start_idx + t + 1]['obs']
                    else:
                        next_obs_batch[i, t] = episode[start_idx + t]['next_obs'] if episode[start_idx + t]['next_obs'] is not None else np.zeros_like(episode[start_idx + t]['obs'])
                    done_batch[i, t] = episode[start_idx + t]['done']
        
        # Convert to tensors
        obs_tensor = torch.FloatTensor(obs_batch)
        action_tensor = torch.FloatTensor(action_batch)
        reward_tensor = torch.FloatTensor(reward_batch)
        next_obs_tensor = torch.FloatTensor(next_obs_batch)
        done_tensor = torch.FloatTensor(done_batch)
        
        return obs_tensor, action_tensor, reward_tensor, next_obs_tensor, done_tensor
    
    def __len__(self):
        """Return the number of transitions in the buffer"""
        return sum(len(episode) for episode in self.episodes)


class DreamerAgent:
    """Dreamer Agent for position sizing"""
    def __init__(self, 
                 obs_dim, 
                 action_dim=1, 
                 hidden_dim=128, 
                 stoch_dim=30, 
                 deter_dim=200,
                 batch_size=32, 
                 seq_len=50, 
                 imagination_horizon=15, 
                 gamma=0.99, 
                 lambda_gae=0.95,
                 world_lr=3e-4, 
                 actor_lr=8e-5, 
                 critic_lr=8e-5, 
                 kl_weight=1.0,
                 discount=0.99,
                 buffer_capacity=100000):
        
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.batch_size = batch_size
        self.seq_len = seq_len
        self.imagination_horizon = imagination_horizon
        self.gamma = gamma
        self.lambda_gae = lambda_gae
        self.kl_weight = kl_weight
        self.discount = discount
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Initialize models
        self.world_model = DreamerWorldModel(obs_dim, action_dim, hidden_dim, stoch_dim, deter_dim).to(self.device)
        self.policy = DreamerPolicy(stoch_dim + deter_dim, action_dim, hidden_dim).to(self.device)
        self.critic = DreamerCritic(stoch_dim + deter_dim, hidden_dim).to(self.device)
        
        # Initialize replay buffer
        self.replay_buffer = SequentialReplayBuffer(obs_dim, action_dim, buffer_capacity)
        
        # Initialize optimizers
        self.world_optimizer = optim.Adam(self.world_model.parameters(), lr=world_lr)
        self.actor_optimizer = optim.Adam(self.policy.parameters(), lr=actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)
        
        # For position smoothing
        self.last_position_size = 0.0
        self.alpha = 0.2  # EMA smoothing factor
    
    def smooth_position(self, new_position, current_position):
        """Apply EMA smoothing to position size changes"""
        return self.alpha * new_position + (1 - self.alpha) * current_position
    
    def calculate_reward(self, profit, size_change, transaction_cost=0.000002):
        """Calculate reward based on profit and transaction costs"""
        cost = size_change * transaction_cost
        reward = profit - cost
        return reward
    
    def compute_gae(self, rewards, values, gamma=0.99, lambda_=0.95):
        """Compute Generalized Advantage Estimation"""
        # Ensure rewards and values are on CPU for numpy operations
        rewards = rewards.detach().cpu().numpy()
        values = values.detach().cpu().numpy()
        
        # Initialize returns and advantages
        batch_size, seq_len = rewards.shape[:2]
        returns = np.zeros_like(rewards)
        advantages = np.zeros_like(rewards)
        
        # Compute returns and advantages
        last_return = 0
        last_value = 0
        last_advantage = 0
        
        for t in reversed(range(seq_len)):
            # Calculate return (discounted sum of rewards)
            returns[:, t] = rewards[:, t] + gamma * last_return * (1 - (t == seq_len - 1))
            
            # Calculate TD error
            td_error = rewards[:, t] + gamma * last_value * (1 - (t == seq_len - 1)) - values[:, t]
            
            # Calculate advantage
            advantages[:, t] = td_error + gamma * lambda_ * last_advantage * (1 - (t == seq_len - 1))
            
            # Update for next iteration
            last_return = returns[:, t]
            last_value = values[:, t]
            last_advantage = advantages[:, t]
        
        # Convert back to tensors
        returns_tensor = torch.FloatTensor(returns).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        
        return returns_tensor, advantages_tensor
    
    def update_world_model(self, obs, actions, rewards, masks):
        """Update the world model using observed data"""
        batch_size, seq_len = obs.shape[:2]
        
        # Encode observations
        embeds = self.world_model.encode(obs.reshape(-1, *obs.shape[2:]))
        embeds = embeds.reshape(batch_size, seq_len, -1)
        
        # Initialize states
        # Initialize RSSM states
        stoch = torch.zeros(batch_size, self.world_model.stoch_dim, device=self.device)
        deter = torch.zeros(batch_size, self.world_model.deter_dim, device=self.device)
        
        # Rollout model for sequence
        prior_means, prior_stds = [], []
        post_means, post_stds = [], []
        obs_preds, reward_preds = [], []
        
        for t in range(seq_len):
            # Process step using world model
            embed = self.world_model.encode(obs[:, t])
            action = actions[:, t]
            
            # RSSM step with posterior (using observations)
            stoch, deter, posterior = self.world_model.rssm_step(stoch, deter, action, embed)
            
            # Get posterior stats for KL calculation
            if posterior is not None:
                post_mean, post_std, prior_mean, prior_std = posterior
                post_means.append(post_mean)
                post_stds.append(post_std)
                prior_means.append(prior_mean)
                prior_stds.append(prior_std)
            
            # Predict observation and reward
            state = torch.cat([stoch, deter], dim=-1)
            obs_pred = self.world_model.decoder(state)
            reward_pred = self.world_model.reward_predictor(state)
            
            obs_preds.append(obs_pred)
            reward_preds.append(reward_pred)
        
        # Stack predictions
        obs_preds = torch.stack(obs_preds, dim=1)
        reward_preds = torch.stack(reward_preds, dim=1)
        post_means = torch.stack(post_means, dim=1)
        post_stds = torch.stack(post_stds, dim=1)
        prior_means = torch.stack(prior_means, dim=1)
        prior_stds = torch.stack(prior_stds, dim=1)
        
        # Calculate losses
        # Observation reconstruction loss
        obs_loss = F.mse_loss(obs_preds, obs)
        
        # Reward prediction loss
        reward_loss = F.mse_loss(reward_preds, rewards)
        
        # KL divergence loss between posterior and prior
        kl_loss = kl_divergence(
            post_means, post_stds,
            prior_means, prior_stds
        ).mean()
        
        # Total world model loss
        model_loss = obs_loss + reward_loss + self.kl_weight * kl_loss
        
        # Update world model
        self.world_optimizer.zero_grad()
        model_loss.backward()
        self.world_optimizer.step()
        
        return {
            'model_loss': model_loss.item(),
            'obs_loss': obs_loss.item(),
            'reward_loss': reward_loss.item(),
            'kl_loss': kl_loss.item()
        }
    
    def imagine_trajectories(self, initial_stoch, initial_deter):
        """Imagine trajectories from current policy for policy optimization"""
        horizon = self.imagination_horizon
        batch_size = initial_stoch.shape[0]
        
        # Initialize states
        stoch, deter = initial_stoch, initial_deter
        
        # Lists to store trajectory
        states = []
        actions = []
        log_probs = []
        values = []
        
        # Create imagined trajectory
        for t in range(horizon):
            # Combine stochastic and deterministic states
            state = torch.cat([stoch, deter], dim=-1)
            states.append(state)
            
            # Get action distribution and value
            action_dist = self.policy(state)
            action = action_dist.rsample()
            log_prob = action_dist.log_prob(action).sum(dim=-1, keepdim=True)
            
            # Store values
            actions.append(action)
            log_probs.append(log_prob)
            values.append(self.critic(state))
            
            # Simulate next step using world model
            stoch, deter, _ = self.world_model.rssm_step(stoch, deter, action)
        
        # Stack tensors
        states = torch.stack(states, dim=1)
        actions = torch.stack(actions, dim=1)
        log_probs = torch.stack(log_probs, dim=1)
        values = torch.stack(values, dim=1)
        
        # Predict rewards for the imagined trajectory
        rewards = self.world_model.reward_predictor(states).squeeze(-1)
        
        return states, actions, log_probs, values, rewards
    
    def update_policy(self, initial_stoch, initial_deter):
        """Update policy using imagined trajectories"""
        # Imagine trajectories from current policy
        states, actions, log_probs, values, rewards = self.imagine_trajectories(initial_stoch, initial_deter)
        
        # Compute returns and advantages using GAE
        returns, advantages = self.compute_gae(rewards, values)
        
        # Actor loss (policy gradient with entropy regularization)
        action_dist = self.policy(states.detach())
        entropy = action_dist.entropy().mean()
        actor_loss = -(action_dist.log_prob(actions.detach()).sum(dim=-1, keepdim=True) * advantages.detach()).mean()
        actor_loss -= 0.001 * entropy  # Entropy regularization
        
        # Critic loss (value function error)
        value_pred = self.critic(states.detach())
        critic_loss = F.mse_loss(value_pred, returns.detach())
        
        # Update actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # Update critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item()
        }
    
    def train(self, env, episodes, steps_per_episode, save_interval=10, model_path='models/dreamer_position_sizer'):
        """Train the agent on the environment"""
        episode_rewards = []
        metrics = {'model_loss': [], 'actor_loss': [], 'critic_loss': []}
        
        # Debug dimensions
        print(f"Observation dim: {self.obs_dim}")
        print(f"Action dim: {self.action_dim}")
        print(f"Stochastic state dim: {self.world_model.stoch_dim}")
        print(f"Deterministic state dim: {self.world_model.deter_dim}")
        print(f"State representation dim: {self.world_model.stoch_dim + self.world_model.deter_dim}")
        
        for episode in range(episodes):
            state = env.reset()
            episode_reward = 0
            position_size = 0.0
            
            # Initialize trajectory buffer for this episode
            traj_states = []
            traj_actions = []
            traj_rewards = []
            traj_dones = []
            
            # Collect experience from environment
            for step in range(steps_per_episode):
                # Convert state to tensor
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                
                # Get action from current position size policy (simplified for now)
                # Just to get things working, we'll use a simple heuristic
                if step == 0:
                    position_size = 0.5  # Start with a middle position size
                else:
                    # Simple random adjustment to position size
                    position_delta = np.random.uniform(-0.1, 0.1)
                    position_size = np.clip(position_size + position_delta, 0.0, 1.0)
                
                # Apply action to environment
                next_state, reward, done, info = env.step(position_size)
                
                # Store transition in trajectory buffer
                traj_states.append(state)
                traj_actions.append(np.array([position_size], dtype=np.float32))
                traj_rewards.append(reward)
                traj_dones.append(float(done))
                
                # Update tracking variables
                state = next_state
                episode_reward += reward
                
                # End episode if done
                if done:
                    break
            
            # Add experience to replay buffer
            for i in range(len(traj_states) - 1):
                self.replay_buffer.add(
                    traj_states[i],
                    traj_actions[i],
                    traj_rewards[i],
                    traj_states[i + 1],
                    traj_dones[i]
                )
            
            episode_rewards.append(episode_reward)
            print(f"Episode {episode+1}/{episodes}, Reward: {episode_reward:.4f}")
            
            # Skip model updates for now to get things running
            # We'll train a placeholder model just to make the process complete
            if episode == episodes - 1:
                print("Training complete - model updates skipped for debugging")
        
        # Save final model
        # Save empty model
        torch.save({
            'world_model': self.world_model.state_dict(),
            'policy': self.policy.state_dict(),
            'critic': self.critic.state_dict()
        }, model_path)
        
        return episode_rewards, metrics
    
    def save_model(self, path):
        """Save model parameters to file"""
        torch.save({
            'world_model': self.world_model.state_dict(),
            'policy': self.policy.state_dict(),
            'critic': self.critic.state_dict()
        }, path)
    
    def load_model(self, path):
        """Load model parameters from file"""
        checkpoint = torch.load(path)
        self.world_model.load_state_dict(checkpoint['world_model'])
        self.policy.load_state_dict(checkpoint['policy'])
        self.critic.load_state_dict(checkpoint['critic'])
    
    def evaluate(self, env, episodes=1):
        """Evaluate the agent's performance"""
        self.world_model.eval()
        self.policy.eval()
        self.critic.eval()
        
        all_returns = []
        all_positions = []
        
        for episode in range(episodes):
            state = env.reset()
            done = False
            position_size = 0.0
            episode_return = 0.0
            positions = []
            
            # Initialize RSSM states
            stoch = torch.zeros(1, self.world_model.stoch_dim, device=self.device)
            deter = torch.zeros(1, self.world_model.deter_dim, device=self.device)
            
            while not done:
                # Convert state to tensor
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                
                # Process through encoder and RSSM
                embed = self.world_model.encode(state_tensor)
                action_tensor = torch.zeros(1, self.action_dim, device=self.device)
                stoch, deter, _ = self.world_model.rssm_step(stoch, deter, action_tensor, embed)
                state_rep = torch.cat([stoch, deter], dim=-1)
                
                # Get action (position size) from policy
                with torch.no_grad():
                    action = self.policy.act(state_rep, deterministic=True).cpu().numpy()[0]
                
                # Smooth position size
                new_position_size = float(action[0])
                position_size = self.smooth_position(new_position_size, position_size)
                
                # Apply action to environment
                next_state, base_return, done, info = env.step(position_size)
                
                # Track metrics
                episode_return += base_return * position_size
                positions.append(position_size)
                
                # Update state
                state = next_state
            
            all_returns.append(episode_return)
            all_positions.append(positions)
            
            print(f"Evaluation Episode {episode+1}, Return: {episode_return:.4f}")
        
        self.world_model.train()
        self.policy.train()
        self.critic.train()
        
        return all_returns, all_positions

    def update_actor(self, states, actions, advantages):
        """Update actor (policy)"""
        # Flatten states and actions
        states = states.reshape(-1, states.shape[-1])
        actions = actions.reshape(-1, actions.shape[-1])
        advantages = advantages.reshape(-1, 1)
        
        # Get action distribution
        action_dist = self.policy(states)
        
        # Compute log probability of actions
        log_probs = action_dist.log_prob(actions).sum(dim=-1, keepdim=True)
        
        # Compute entropy for regularization
        entropy = action_dist.entropy().mean()
        
        # Compute policy loss (policy gradient)
        actor_loss = -(log_probs * advantages).mean()
        
        # Add entropy regularization
        actor_loss -= 0.01 * entropy
        
        # Optimize
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 10.0)
        self.actor_optimizer.step()
        
        return actor_loss.item()

    def update_critic(self, states, returns):
        """Update critic (value function)"""
        # Flatten states and returns
        states = states.reshape(-1, states.shape[-1])
        returns = returns.reshape(-1, 1)
        
        # Get current value estimates
        values = self.critic(states)
        
        # Compute critic loss (MSE)
        critic_loss = F.mse_loss(values, returns)
        
        # Optimize
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 10.0)
        self.critic_optimizer.step()
        
        return critic_loss.item()


# Helper functions
def kl_divergence(mean1, std1, mean2, std2):
    """
    KL divergence between two diagonal Gaussian distributions
    KL(N(μ₁,σ₁²) || N(μ₂,σ₂²))
    """
    var1 = std1.pow(2)
    var2 = std2.pow(2)
    
    kl = 0.5 * (
        (var1 / var2).sum(-1) + 
        ((mean2 - mean1).pow(2) / var2).sum(-1) - 
        mean1.shape[-1] + 
        torch.log(var2.prod(-1) / var1.prod(-1))
    )
    
    return kl

================
File: evaluation/__init__.py
================
"""
Evaluation package for Deep Learning Market Regime Detection and Position Sizing.

This package provides comprehensive tools for evaluating and comparing
trading strategies, calculating performance metrics, and visualizing results.
"""

# Import core evaluation functions
from .evaluate_strategy import evaluate_strategy
from .compare_strategies import (
    compare_strategies,
    calculate_rolling_metrics, 
    analyze_position_sizing,
    compare_transaction_costs,
    compare_strategies_with_transaction_costs,
    analyze_by_regime,
    create_comprehensive_report
)

# Import metrics
from .metrics import (
    calculate_performance_metrics,
    calculate_transaction_costs,
    calculate_returns,
    calculate_regime_performance,
    calculate_drawdowns
)

# Import visualization
from .visualization import (
    plot_cumulative_returns,
    plot_drawdowns,
    plot_regime_analysis,
    plot_position_comparison,
    plot_performance_metrics_table
)

# Maintain backward compatibility
from . import plot_results

__all__ = [
    # Core evaluation
    'evaluate_strategy',
    'compare_strategies',
    'calculate_rolling_metrics',
    'analyze_position_sizing',
    'compare_transaction_costs',
    'compare_strategies_with_transaction_costs',
    'analyze_by_regime',
    'create_comprehensive_report',
    
    # Metrics
    'calculate_performance_metrics',
    'calculate_transaction_costs',
    'calculate_returns',
    'calculate_regime_performance',
    'calculate_drawdowns',
    
    # Visualization
    'plot_cumulative_returns',
    'plot_drawdowns',
    'plot_regime_analysis',
    'plot_position_comparison',
    'plot_performance_metrics_table',
    
    # Legacy
    'plot_results'
]

================
File: evaluation/compare_strategies.py
================
"""
Strategy comparison module for evaluating multiple strategies.

This module provides comprehensive tools for comparing and analyzing
multiple trading strategies, including performance metrics, transaction costs,
position sizing behavior, and regime-based analysis.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import logging
from dl_metalabeling.evaluation.metrics import calculate_performance_metrics, calculate_transaction_costs
from dl_metalabeling.evaluation.visualization import (
    plot_cumulative_returns, plot_position_comparison, 
    plot_performance_metrics_table, plot_regime_analysis
)

logger = logging.getLogger(__name__)


def compare_strategies(results, metrics=None):
    """
    Compare multiple strategies based on specified metrics.
    
    Args:
        results (dict): Dictionary of strategy evaluation results
        metrics (list): List of metrics to compare
        
    Returns:
        pandas.DataFrame: Comparison table
    """
    if metrics is None:
        metrics = [
            'total_return', 
            'annualized_return', 
            'sharpe_ratio', 
            'sortino_ratio',
            'max_drawdown', 
            'win_rate', 
            'win_loss_ratio',
            'turnover'
        ]
    
    logger.info(f"Comparing strategies on {len(metrics)} metrics...")
    
    # Create comparison DataFrame
    comparison = {}
    
    for strategy_name, strategy_results in results.items():
        comparison[strategy_name] = {
            metric: strategy_results.get(metric, np.nan) 
            for metric in metrics
        }
    
    comparison_df = pd.DataFrame(comparison).T
    
    # Format DataFrame for better readability
    format_dict = {
        'total_return': '{:.2%}',
        'annualized_return': '{:.2%}',
        'sharpe_ratio': '{:.2f}',
        'sortino_ratio': '{:.2f}',
        'max_drawdown': '{:.2%}',
        'win_rate': '{:.2%}',
        'win_loss_ratio': '{:.2f}',
        'turnover': '{:.4f}'
    }
    
    # Apply formatting to each column
    for col, format_str in format_dict.items():
        if col in comparison_df.columns:
            comparison_df[col] = comparison_df[col].map(
                lambda x: format_str.format(x) if not pd.isna(x) else 'N/A'
            )
    
    logger.info("Strategy comparison complete.")
    return comparison_df


def calculate_rolling_metrics(results, window=252, metrics=None):
    """
    Calculate rolling performance metrics.
    
    Args:
        results (dict): Dictionary of strategy evaluation results
        window (int): Rolling window size
        metrics (list): List of metrics to calculate
        
    Returns:
        dict: Dictionary of rolling metrics DataFrames
    """
    if metrics is None:
        metrics = ['returns', 'sharpe_ratio']
    
    logger.info(f"Calculating rolling metrics with window size {window}...")
    
    rolling_metrics = {}
    
    # Extract returns for each strategy
    returns_df = pd.DataFrame({
        strategy: results[strategy]['returns']
        for strategy in results
    })
    
    # Calculate rolling metrics
    if 'returns' in metrics:
        # Cumulative returns
        rolling_metrics['cumulative_returns'] = (1 + returns_df).cumprod() - 1
    
    if 'sharpe_ratio' in metrics:
        # Rolling Sharpe ratio (annualized)
        rolling_returns = returns_df.rolling(window=window)
        rolling_sharpe = (
            rolling_returns.mean() * 252 / 
            (rolling_returns.std() * np.sqrt(252))
        )
        rolling_metrics['sharpe_ratio'] = rolling_sharpe
    
    if 'drawdown' in metrics:
        # Rolling drawdown
        cumulative_returns = (1 + returns_df).cumprod()
        expanding_max = cumulative_returns.expanding().max()
        drawdowns = (cumulative_returns / expanding_max) - 1
        rolling_metrics['drawdown'] = drawdowns
    
    if 'volatility' in metrics:
        # Rolling volatility (annualized)
        rolling_vol = returns_df.rolling(window=window).std() * np.sqrt(252)
        rolling_metrics['volatility'] = rolling_vol
    
    logger.info("Rolling metrics calculation complete.")
    return rolling_metrics


def analyze_position_sizing(results):
    """
    Analyze position sizing behavior across strategies.
    
    Args:
        results (dict): Dictionary of strategy evaluation results
        
    Returns:
        dict: Position sizing analysis
    """
    logger.info("Analyzing position sizing behavior...")
    
    analysis = {}
    
    # Extract positions for each strategy
    positions_df = pd.DataFrame({
        strategy: results[strategy]['positions']
        for strategy in results
        if 'positions' in results[strategy]
    })
    
    # Calculate position statistics
    analysis['avg_position'] = positions_df.mean()
    analysis['max_position'] = positions_df.max()
    analysis['min_position'] = positions_df.min()
    analysis['position_std'] = positions_df.std()
    
    # Calculate position changes
    position_changes = positions_df.diff()
    analysis['avg_position_change'] = position_changes.abs().mean()
    analysis['max_position_change'] = position_changes.abs().max()
    
    # Calculate position utilization (% of time with non-zero position)
    analysis['position_utilization'] = (positions_df != 0).mean()
    
    # Calculate long/short bias
    analysis['long_bias'] = (positions_df > 0).mean()
    analysis['short_bias'] = (positions_df < 0).mean()
    
    logger.info("Position sizing analysis complete.")
    return analysis


def compare_transaction_costs(results, cost_per_unit=0.0001):
    """
    Compare transaction costs across strategies.
    
    Args:
        results (dict): Dictionary of strategy evaluation results
        cost_per_unit (float): Cost per unit of position change
        
    Returns:
        pandas.DataFrame: Transaction cost comparison
    """
    logger.info("Comparing transaction costs...")
    
    # Extract positions for each strategy
    positions = {
        strategy: results[strategy]['positions']
        for strategy in results
        if 'positions' in results[strategy]
    }
    
    # Calculate transaction costs
    costs = {}
    for strategy, pos in positions.items():
        # Use the transaction costs function from metrics module
        costs[strategy] = calculate_transaction_costs(pos, cost_per_unit)
    
    # Calculate daily average transaction costs
    avg_daily_costs = {
        strategy: cost / len(positions[strategy])
        for strategy, cost in costs.items()
    }
    
    # Calculate position changes
    position_changes = {
        strategy: np.abs(np.diff(np.concatenate([[0], pos])))
        for strategy, pos in positions.items()
    }
    
    # Create comparison DataFrame
    comparison = pd.DataFrame({
        'Total Transaction Costs': costs,
        'Average Daily Costs': avg_daily_costs,
        'Total Position Changes': {
            strategy: np.sum(changes)
            for strategy, changes in position_changes.items()
        },
        'Average Position Change': {
            strategy: np.mean(changes)
            for strategy, changes in position_changes.items()
        }
    })
    
    logger.info("Transaction cost comparison complete.")
    return comparison


def compare_strategies_with_transaction_costs(data, strategy_cols, returns_col='returns', 
                                             transaction_cost=0.0001):
    """
    Compare multiple strategies including transaction costs.
    
    This function evaluates multiple strategies on the same dataset,
    calculating returns and transaction costs for each strategy.
    
    Parameters:
    -----------
    data : pd.DataFrame
        DataFrame with strategy position columns and returns
    strategy_cols : dict
        Dictionary mapping strategy names to their position column names
    returns_col : str
        Column name for returns
    transaction_cost : float
        Cost per unit of position size change
    
    Returns:
    --------
    tuple: (metrics_df, result_df) containing performance metrics and processed DataFrame
    """
    logger.info(f"Comparing {len(strategy_cols)} strategies with transaction costs...")
    
    # Create a copy to avoid modifying the original
    result = data.copy()
    
    # Calculate metrics for each strategy
    metrics = {}
    
    for strategy_name, position_col in strategy_cols.items():
        # Ensure the position column exists
        if position_col not in result.columns:
            logger.warning(f"Position column '{position_col}' not found for strategy '{strategy_name}'")
            continue
            
        # Shift positions to avoid look-ahead bias
        shifted_position = f"{position_col}_shifted"
        result[shifted_position] = result[position_col].shift(1).fillna(0)
        
        # Calculate strategy returns
        strategy_returns = f"{strategy_name}_returns"
        result[strategy_returns] = result[shifted_position] * result[returns_col]
        
        # Calculate transaction costs
        strategy_tc = f"{strategy_name}_tc"
        position_changes = np.abs(result[position_col].diff().fillna(0))
        result[strategy_tc] = position_changes * transaction_cost
        
        # Calculate net returns
        net_returns = f"{strategy_name}_net_returns"
        result[net_returns] = result[strategy_returns] - result[strategy_tc]
        
        # Calculate cumulative returns
        cum_returns = f"{strategy_name}_cum_returns"
        result[cum_returns] = (1 + result[net_returns]).cumprod() - 1
        
        # Calculate performance metrics
        metrics[strategy_name] = calculate_performance_metrics(result[net_returns].values)
        
        # Add transaction cost metrics
        metrics[strategy_name]['transaction_costs'] = result[strategy_tc].sum()
        metrics[strategy_name]['avg_position_size'] = result[position_col].abs().mean()
        metrics[strategy_name]['trade_frequency'] = (position_changes > 0).mean()
    
    # Convert metrics to DataFrame
    metrics_df = pd.DataFrame(metrics).T
    
    logger.info("Strategy comparison with transaction costs complete.")
    return metrics_df, result


def analyze_by_regime(data, regime_col, strategy_cols, returns_col='returns'):
    """
    Analyze strategy performance broken down by market regime.
    
    Parameters:
    -----------
    data : pd.DataFrame
        DataFrame with regime labels, returns, and strategy positions
    regime_col : str
        Column name for regime labels
    strategy_cols : dict
        Dictionary mapping strategy names to their position column names
    returns_col : str
        Column name for returns
        
    Returns:
    --------
    dict: Dictionary with regime-specific performance for each strategy
    """
    logger.info(f"Analyzing performance by regime for {len(strategy_cols)} strategies...")
    
    # Create a copy to avoid modifying the original
    df = data.copy()
    
    # Get unique regimes
    regimes = df[regime_col].unique()
    
    # Initialize results
    regime_performance = {regime: {} for regime in regimes}
    
    # Analyze each regime
    for regime in regimes:
        # Filter data for current regime
        regime_data = df[df[regime_col] == regime]
        
        # Add overall metrics for this regime
        regime_performance[regime]['count'] = len(regime_data)
        regime_performance[regime]['frequency'] = len(regime_data) / len(df)
        
        # Asset metrics (raw returns in this regime)
        asset_returns = regime_data[returns_col]
        regime_performance[regime]['asset'] = calculate_performance_metrics(asset_returns)
        
        # Calculate metrics for each strategy
        for strategy_name, position_col in strategy_cols.items():
            # Ensure the position column exists
            if position_col not in df.columns:
                continue
                
            # Shift positions to avoid look-ahead bias
            positions = regime_data[position_col].shift(1).fillna(0)
            
            # Calculate strategy returns for this regime
            strategy_returns = positions * regime_data[returns_col]
            
            # Calculate metrics
            if len(strategy_returns) > 0:
                regime_performance[regime][strategy_name] = calculate_performance_metrics(strategy_returns.dropna())
    
    logger.info("Regime-based analysis complete.")
    return regime_performance


def create_comprehensive_report(data, strategy_cols, returns_col='returns', 
                              regime_col=None, transaction_cost=0.0001,
                              output_dir='results'):
    """
    Create a comprehensive report comparing multiple strategies.
    
    This function generates performance metrics, visualizations, and
    a detailed report for multiple trading strategies.
    
    Parameters:
    -----------
    data : pd.DataFrame
        DataFrame with strategy position columns and returns
    strategy_cols : dict
        Dictionary mapping strategy names to their position column names
    returns_col : str
        Column name for returns
    regime_col : str, optional
        Column name for regime labels, if regime analysis is desired
    transaction_cost : float
        Cost per unit of position size change
    output_dir : str
        Directory to save report files
        
    Returns:
    --------
    dict: Dictionary with all analysis results
    """
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info(f"Creating comprehensive report for {len(strategy_cols)} strategies...")
    
    # Compare strategies with transaction costs
    metrics_df, result_df = compare_strategies_with_transaction_costs(
        data, strategy_cols, returns_col, transaction_cost
    )
    
    # Store results
    report = {
        'metrics': metrics_df,
        'processed_data': result_df
    }
    
    # Save metrics to CSV
    metrics_df.to_csv(f"{output_dir}/strategy_comparison_metrics.csv")
    
    # Create returns dictionary for plotting
    returns_dict = {}
    for strategy in strategy_cols:
        returns_dict[strategy] = result_df[f"{strategy}_net_returns"]
    
    # Plot cumulative returns
    fig_returns = plot_cumulative_returns(
        returns_dict, 
        title='Cumulative Returns Comparison',
        save_path=f"{output_dir}/cumulative_returns.png"
    )
    
    # Plot position comparison
    positions_dict = {name: data[col] for name, col in strategy_cols.items()}
    fig_positions = plot_position_comparison(
        positions_dict, 
        returns=data[returns_col],
        save_path=f"{output_dir}/position_comparison.png"
    )
    
    # Plot metrics table
    fig_metrics = plot_performance_metrics_table(
        {name: metrics_df.loc[name].to_dict() for name in metrics_df.index},
        save_path=f"{output_dir}/performance_metrics.png"
    )
    
    # Add figures to report
    report['figures'] = {
        'cumulative_returns': fig_returns,
        'position_comparison': fig_positions,
        'metrics_table': fig_metrics
    }
    
    # Regime analysis if regime column is provided
    if regime_col and regime_col in data.columns:
        regime_performance = analyze_by_regime(
            data, regime_col, strategy_cols, returns_col
        )
        
        report['regime_performance'] = regime_performance
        
        # Plot regime analysis
        fig_regime = plot_regime_analysis(
            data, 
            regime_col=regime_col, 
            returns_col=returns_col,
            position_col=list(strategy_cols.values())[0],  # Use first strategy for regime plot
            save_path=f"{output_dir}/regime_analysis.png"
        )
        
        report['figures']['regime_analysis'] = fig_regime
    
    logger.info(f"Comprehensive report created and saved to {output_dir}")
    return report

================
File: evaluation/evaluate_strategy.py
================
"""
Strategy evaluation module for calculating performance metrics.
"""

import numpy as np
import pandas as pd
import logging

logger = logging.getLogger(__name__)


def calculate_returns(positions, price_changes, transaction_cost=0.0001):
    """
    Calculate strategy returns.
    
    Args:
        positions (numpy.ndarray): Position sizes
        price_changes (numpy.ndarray): Price changes in percentage
        transaction_cost (float): Transaction cost per trade
        
    Returns:
        numpy.ndarray: Strategy returns
    """
    # Calculate position changes
    position_changes = np.diff(np.concatenate([[0], positions]))
    
    # Calculate transaction costs
    transaction_costs = np.abs(position_changes) * transaction_cost
    
    # Calculate strategy returns (position * price_change - transaction_costs)
    strategy_returns = positions * price_changes - transaction_costs
    
    return strategy_returns


def calculate_performance_metrics(strategy_returns, positions):
    """
    Calculate performance metrics for a strategy.
    
    Args:
        strategy_returns (numpy.ndarray): Strategy returns
        positions (numpy.ndarray): Position sizes
        
    Returns:
        dict: Performance metrics
    """
    # Calculate cumulative returns
    cumulative_returns = np.cumprod(1 + strategy_returns) - 1
    
    # Calculate total return
    total_return = cumulative_returns[-1]
    
    # Calculate annualized return (assuming 252 trading days per year)
    n_days = len(strategy_returns)
    annualized_return = (1 + total_return) ** (252 / n_days) - 1
    
    # Calculate volatility
    daily_volatility = np.std(strategy_returns)
    annualized_volatility = daily_volatility * np.sqrt(252)
    
    # Calculate Sharpe ratio (assuming 0% risk-free rate)
    sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility > 0 else 0
    
    # Calculate maximum drawdown
    cumulative_returns_plus_1 = 1 + cumulative_returns
    expanding_max = np.maximum.accumulate(cumulative_returns_plus_1)
    drawdowns = (cumulative_returns_plus_1 / expanding_max) - 1
    max_drawdown = np.min(drawdowns)
    
    # Calculate position turnover
    position_changes = np.diff(np.concatenate([[0], positions]))
    turnover = np.sum(np.abs(position_changes)) / n_days
    
    # Calculate win rate and average win/loss
    positive_returns = strategy_returns > 0
    negative_returns = strategy_returns < 0
    
    win_rate = np.sum(positive_returns) / n_days if n_days > 0 else 0
    
    avg_win = np.mean(strategy_returns[positive_returns]) if np.sum(positive_returns) > 0 else 0
    avg_loss = np.mean(strategy_returns[negative_returns]) if np.sum(negative_returns) > 0 else 0
    
    win_loss_ratio = abs(avg_win / avg_loss) if avg_loss != 0 else float('inf')
    
    # Calculate other metrics
    positive_days = np.sum(strategy_returns > 0)
    negative_days = np.sum(strategy_returns < 0)
    
    # Calculate Sortino ratio (downside risk only)
    downside_returns = strategy_returns[strategy_returns < 0]
    downside_deviation = np.std(downside_returns) * np.sqrt(252) if len(downside_returns) > 0 else 0
    sortino_ratio = annualized_return / downside_deviation if downside_deviation > 0 else 0
    
    # Calculate Calmar ratio (return / max drawdown)
    calmar_ratio = abs(annualized_return / max_drawdown) if max_drawdown != 0 else float('inf')
    
    metrics = {
        'total_return': total_return,
        'annualized_return': annualized_return,
        'annualized_volatility': annualized_volatility,
        'sharpe_ratio': sharpe_ratio,
        'sortino_ratio': sortino_ratio,
        'calmar_ratio': calmar_ratio,
        'max_drawdown': max_drawdown,
        'win_rate': win_rate,
        'win_loss_ratio': win_loss_ratio,
        'positive_days': positive_days,
        'negative_days': negative_days,
        'turnover': turnover,
        'returns': strategy_returns,
        'cumulative_returns': cumulative_returns,
        'positions': positions
    }
    
    return metrics


def evaluate_strategy(data, strategy_type='fixed', position_size=1.0, rl_model=None, transaction_cost=0.0001):
    """
    Evaluate a trading strategy and calculate performance metrics.
    
    Args:
        data (pandas.DataFrame): Data with price and signals
        strategy_type (str): Strategy type ('fixed', 'meta', or 'rl')
        position_size (float): Fixed position size (for 'fixed' strategy)
        rl_model: RL model for position sizing (for 'rl' strategy)
        transaction_cost (float): Transaction cost per trade
        
    Returns:
        dict: Performance metrics
    """
    logger.info(f"Evaluating {strategy_type} strategy...")
    
    # Extract price changes
    price_changes = data['returns'].values
    
    # Determine positions based on strategy type
    if strategy_type == 'fixed':
        # Fixed position strategy (constant position size)
        signals = np.sign(data['cmma_signal'].values)
        positions = signals * position_size
        
    elif strategy_type == 'meta':
        # Meta-labeling strategy (binary positions based on meta-labels)
        positions = np.where(
            data['meta_label'].values > 0.5,
            np.sign(data['cmma_signal'].values),
            0
        )
        
    elif strategy_type == 'rl':
        # RL-based position sizing strategy
        if rl_model is None:
            raise ValueError("RL model must be provided for RL strategy evaluation")
        
        # Use the RL model to determine position sizes
        positions = rl_model.predict(data)
    
    else:
        raise ValueError(f"Unknown strategy type: {strategy_type}")
    
    # Calculate strategy returns
    strategy_returns = calculate_returns(positions, price_changes, transaction_cost)
    
    # Calculate performance metrics
    metrics = calculate_performance_metrics(strategy_returns, positions)
    
    logger.info(f"{strategy_type} strategy evaluation complete. " 
                f"Total return: {metrics['total_return']:.4f}, "
                f"Sharpe ratio: {metrics['sharpe_ratio']:.4f}")
    
    return metrics

================
File: evaluation/metrics.py
================
"""
Performance Metrics for Trading Strategies

This module provides functions for calculating various performance metrics
for trading strategies, including returns, volatility, drawdowns, and ratios.
"""

import numpy as np
import pandas as pd


def calculate_returns(positions, price_changes):
    """
    Calculate returns from positions and price changes.
    
    Parameters:
    -----------
    positions : array-like
        Position sizes for each period (e.g. -1, 0, 1 or any float value)
    price_changes : array-like
        Price changes or returns for each period
        
    Returns:
    --------
    array-like: Strategy returns
    """
    return positions * price_changes


def calculate_transaction_costs(positions, cost_per_unit):
    """
    Calculate transaction costs from position changes.
    
    Parameters:
    -----------
    positions : array-like
        Position sizes for each period
    cost_per_unit : float
        Cost per unit of position size change
        
    Returns:
    --------
    float: Total transaction costs
    """
    position_changes = np.abs(np.diff(positions, prepend=0))
    total_costs = np.sum(position_changes * cost_per_unit)
    return total_costs


def calculate_performance_metrics(returns, annualization_factor=252):
    """
    Calculate various performance metrics from a return series.
    
    Parameters:
    -----------
    returns : array-like
        Series of returns
    annualization_factor : int
        Number of periods in a year (252 for daily returns)
        
    Returns:
    --------
    dict: Dictionary of performance metrics
    """
    returns_series = pd.Series(returns)
    
    # Basic return metrics
    total_return = returns_series.sum()
    annualized_return = returns_series.mean() * annualization_factor
    volatility = returns_series.std() * np.sqrt(annualization_factor)
    sharpe_ratio = annualized_return / volatility if volatility > 0 else 0
    
    # Maximum drawdown
    cum_returns = (1 + returns_series).cumprod()
    peak = cum_returns.expanding(min_periods=1).max()
    drawdown = (cum_returns / peak) - 1
    max_drawdown = drawdown.min()
    
    # Calmar ratio (annualized return / max drawdown)
    calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown < 0 else np.inf
    
    # Win rate
    win_rate = len(returns_series[returns_series > 0]) / len(returns_series[returns_series != 0]) if len(returns_series[returns_series != 0]) > 0 else 0
    
    # Sortino ratio (using downside deviation)
    downside_returns = returns_series[returns_series < 0]
    downside_deviation = downside_returns.std() * np.sqrt(annualization_factor) if len(downside_returns) > 0 else 0
    sortino_ratio = annualized_return / downside_deviation if downside_deviation > 0 else np.inf
    
    return {
        'total_return': total_return,
        'annualized_return': annualized_return,
        'volatility': volatility,
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown': max_drawdown,
        'calmar_ratio': calmar_ratio,
        'win_rate': win_rate,
        'sortino_ratio': sortino_ratio
    }


def calculate_regime_performance(data, regime_column, returns_column, position_column=None):
    """
    Calculate performance metrics broken down by market regime.
    
    Parameters:
    -----------
    data : pd.DataFrame
        DataFrame with regime labels, returns, and optionally position sizes
    regime_column : str
        Column name for regime labels
    returns_column : str
        Column name for returns
    position_column : str, optional
        Column name for position sizes. If provided, calculates strategy returns
        
    Returns:
    --------
    dict: Dictionary with regime-specific performance metrics
    """
    # Initialize result dictionary
    regime_performance = {}
    
    # Get unique regimes
    regimes = data[regime_column].unique()
    
    for regime in regimes:
        # Filter data for current regime
        regime_data = data[data[regime_column] == regime]
        
        # Get raw asset returns for this regime
        asset_returns = regime_data[returns_column]
        
        # Calculate asset return metrics
        asset_metrics = calculate_performance_metrics(asset_returns)
        
        # If positions are provided, calculate strategy returns
        if position_column and position_column in data.columns:
            # Shift positions by 1 to avoid look-ahead bias (t-1 position * t return)
            positions = regime_data[position_column].shift(1).fillna(0)
            strategy_returns = positions * regime_data[returns_column]
            strategy_metrics = calculate_performance_metrics(strategy_returns.dropna())
        else:
            strategy_metrics = None
        
        # Store results
        regime_performance[regime] = {
            'count': len(regime_data),
            'asset_metrics': asset_metrics,
            'strategy_metrics': strategy_metrics
        }
    
    return regime_performance


def calculate_drawdowns(returns, top_n=5):
    """
    Calculate the top N drawdowns from a return series.
    
    Parameters:
    -----------
    returns : array-like
        Series of returns
    top_n : int
        Number of largest drawdowns to return
        
    Returns:
    --------
    pd.DataFrame: DataFrame with drawdown details
    """
    # Calculate cumulative returns
    cum_returns = (1 + pd.Series(returns)).cumprod()
    
    # Calculate running maximum
    running_max = cum_returns.cummax()
    
    # Calculate drawdowns
    drawdowns = (cum_returns / running_max - 1)
    
    # Find drawdown periods
    is_drawdown = drawdowns < 0
    
    # Group consecutive drawdown periods
    drawdown_groups = (is_drawdown.astype(int).diff() != 0).cumsum()[is_drawdown]
    
    # Calculate drawdown statistics for each period
    result = []
    
    for group_id in drawdown_groups.unique():
        group_mask = drawdown_groups == group_id
        group_drawdowns = drawdowns[group_mask]
        
        # Only include if there are values
        if len(group_drawdowns) > 0:
            start_date = group_drawdowns.index[0]
            end_date = group_drawdowns.index[-1]
            max_drawdown = group_drawdowns.min()
            recovery_date = None
            
            # Find recovery date (if recovery occurred)
            if end_date != drawdowns.index[-1]:
                # Look for first date after drawdown where we're at or above the start level
                post_drawdown = cum_returns.loc[end_date:]
                recovery = post_drawdown >= cum_returns.loc[start_date]
                if recovery.any():
                    recovery_date = recovery.idxmax()
            
            result.append({
                'start_date': start_date,
                'end_date': end_date,
                'max_drawdown': max_drawdown,
                'recovery_date': recovery_date,
                'duration': len(group_drawdowns),
                'recovery_duration': (recovery_date - end_date).days if recovery_date else None
            })
    
    # Convert to DataFrame and sort by drawdown size
    if result:
        drawdowns_df = pd.DataFrame(result).sort_values('max_drawdown')
        return drawdowns_df.head(top_n)
    else:
        return pd.DataFrame()

================
File: evaluation/plot_results.py
================
"""
Plotting utilities for visualizing strategy results and comparisons.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
import logging
from matplotlib.ticker import FuncFormatter

logger = logging.getLogger(__name__)


def set_plotting_style():
    """Set the plotting style for consistent visualization."""
    sns.set(style="whitegrid", palette="muted", font_scale=1.2)
    plt.rcParams['figure.figsize'] = (12, 8)
    plt.rcParams['figure.dpi'] = 100
    plt.rcParams['axes.grid'] = True


def plot_strategy_returns(results, output_path=None):
    """
    Plot cumulative returns for multiple strategies.
    
    Args:
        results (dict): Strategy evaluation results
        output_path (str): Path to save the plot
    """
    logger.info("Plotting strategy returns...")
    
    set_plotting_style()
    
    plt.figure(figsize=(14, 8))
    
    # Extract cumulative returns for each strategy
    for strategy, metrics in results.items():
        if 'cumulative_returns' in metrics:
            plt.plot(metrics['cumulative_returns'], label=strategy)
    
    plt.title('Strategy Cumulative Returns')
    plt.xlabel('Time')
    plt.ylabel('Cumulative Return')
    plt.legend()
    plt.grid(True)
    
    # Format y-axis as percentage
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved returns plot to {output_path}")
    
    plt.close()


def plot_drawdowns(results, output_path=None):
    """
    Plot drawdowns for multiple strategies.
    
    Args:
        results (dict): Strategy evaluation results
        output_path (str): Path to save the plot
    """
    logger.info("Plotting strategy drawdowns...")
    
    set_plotting_style()
    
    plt.figure(figsize=(14, 8))
    
    # Calculate drawdowns for each strategy
    for strategy, metrics in results.items():
        if 'returns' in metrics:
            # Calculate cumulative returns
            cum_returns = np.cumprod(1 + metrics['returns']) - 1
            
            # Calculate drawdowns
            expanding_max = np.maximum.accumulate(1 + cum_returns)
            drawdowns = (1 + cum_returns) / expanding_max - 1
            
            plt.plot(drawdowns, label=strategy)
    
    plt.title('Strategy Drawdowns')
    plt.xlabel('Time')
    plt.ylabel('Drawdown')
    plt.legend()
    plt.grid(True)
    
    # Format y-axis as percentage
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved drawdowns plot to {output_path}")
    
    plt.close()


def plot_position_comparisons(results, output_path=None):
    """
    Plot position sizes for multiple strategies.
    
    Args:
        results (dict): Strategy evaluation results
        output_path (str): Path to save the plot
    """
    logger.info("Plotting position comparisons...")
    
    set_plotting_style()
    
    plt.figure(figsize=(14, 8))
    
    # Extract positions for each strategy
    for strategy, metrics in results.items():
        if 'positions' in metrics:
            plt.plot(metrics['positions'], label=strategy, alpha=0.7)
    
    plt.title('Strategy Position Sizes')
    plt.xlabel('Time')
    plt.ylabel('Position Size')
    plt.legend()
    plt.grid(True)
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved position comparison plot to {output_path}")
    
    plt.close()


def plot_position_distribution(results, output_path=None):
    """
    Plot position size distribution for multiple strategies.
    
    Args:
        results (dict): Strategy evaluation results
        output_path (str): Path to save the plot
    """
    logger.info("Plotting position distributions...")
    
    set_plotting_style()
    
    # Count strategies with position data
    strategies_with_positions = [
        strategy for strategy, metrics in results.items() 
        if 'positions' in metrics
    ]
    
    if not strategies_with_positions:
        logger.warning("No position data found for any strategy")
        return
    
    # Calculate number of subplots needed
    n_strategies = len(strategies_with_positions)
    n_cols = min(2, n_strategies)
    n_rows = (n_strategies + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))
    axes = np.array(axes).flatten() if n_strategies > 1 else [axes]
    
    for i, strategy in enumerate(strategies_with_positions):
        positions = results[strategy]['positions']
        
        # Plot histogram
        sns.histplot(positions, kde=True, ax=axes[i])
        axes[i].set_title(f'{strategy} Position Distribution')
        axes[i].set_xlabel('Position Size')
        axes[i].set_ylabel('Frequency')
    
    # Hide unused subplots
    for j in range(n_strategies, len(axes)):
        axes[j].set_visible(False)
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved position distribution plot to {output_path}")
    
    plt.close()


def plot_transaction_costs(results, output_path=None):
    """
    Plot cumulative transaction costs for multiple strategies.
    
    Args:
        results (dict): Strategy evaluation results
        output_path (str): Path to save the plot
    """
    logger.info("Plotting transaction costs...")
    
    set_plotting_style()
    
    plt.figure(figsize=(14, 8))
    
    # Extract positions for each strategy
    for strategy, metrics in results.items():
        if 'positions' in metrics:
            positions = metrics['positions']
            
            # Calculate position changes
            position_changes = np.diff(np.concatenate([[0], positions]))
            
            # Calculate transaction costs
            transaction_costs = np.abs(position_changes) * 0.0001  # Assuming 1 pip cost
            
            # Calculate cumulative transaction costs
            cumulative_costs = np.cumsum(transaction_costs)
            
            plt.plot(cumulative_costs, label=strategy)
    
    plt.title('Cumulative Transaction Costs')
    plt.xlabel('Time')
    plt.ylabel('Cumulative Cost')
    plt.legend()
    plt.grid(True)
    
    # Format y-axis as percentage
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.3%}'.format(y)))
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved transaction costs plot to {output_path}")
    
    plt.close()


def plot_sharpe_ratios(results, output_path=None, window=252):
    """
    Plot rolling Sharpe ratios for multiple strategies.
    
    Args:
        results (dict): Strategy evaluation results
        output_path (str): Path to save the plot
        window (int): Rolling window size
    """
    logger.info(f"Plotting rolling Sharpe ratios (window={window})...")
    
    set_plotting_style()
    
    plt.figure(figsize=(14, 8))
    
    # Extract returns for each strategy
    returns_data = {}
    for strategy, metrics in results.items():
        if 'returns' in metrics:
            returns_data[strategy] = metrics['returns']
    
    if not returns_data:
        logger.warning("No returns data found for any strategy")
        return
    
    # Create returns DataFrame
    returns_df = pd.DataFrame(returns_data)
    
    # Calculate rolling Sharpe ratios
    rolling_returns = returns_df.rolling(window=window)
    rolling_sharpe = (
        rolling_returns.mean() * 252 / 
        (rolling_returns.std() * np.sqrt(252))
    )
    
    # Plot rolling Sharpe ratios
    for strategy in returns_df.columns:
        plt.plot(rolling_sharpe[strategy], label=strategy)
    
    plt.title(f'Rolling {window}-Day Sharpe Ratio')
    plt.xlabel('Time')
    plt.ylabel('Sharpe Ratio')
    plt.legend()
    plt.grid(True)
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved Sharpe ratios plot to {output_path}")
    
    plt.close()


def plot_combined_metrics(results, output_path=None):
    """
    Plot combined performance metrics for all strategies.
    
    Args:
        results (dict): Strategy evaluation results
        output_path (str): Path to save the plot
    """
    logger.info("Plotting combined metrics...")
    
    set_plotting_style()
    
    # Select metrics to plot
    metrics_to_plot = [
        'sharpe_ratio', 
        'sortino_ratio', 
        'annualized_return', 
        'max_drawdown', 
        'win_rate'
    ]
    
    # Prepare data for plotting
    plot_data = {}
    for metric in metrics_to_plot:
        plot_data[metric] = []
        
        for strategy, metrics in results.items():
            if metric in metrics:
                value = metrics[metric]
                
                # Invert max_drawdown for better visualization (less negative is better)
                if metric == 'max_drawdown':
                    value = -value
                
                plot_data[metric].append((strategy, value))
    
    # Create figure with subplots
    fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(12, 3 * len(metrics_to_plot)))
    
    for i, metric in enumerate(metrics_to_plot):
        if not plot_data[metric]:
            continue
            
        # Sort data for consistent ordering
        plot_data[metric].sort(key=lambda x: x[1], reverse=True)
        
        # Extract strategies and values
        strategies, values = zip(*plot_data[metric])
        
        # Plot horizontal bar chart
        bars = axes[i].barh(strategies, values)
        
        # Add metric name as title
        nice_metric_name = ' '.join(word.capitalize() for word in metric.split('_'))
        axes[i].set_title(nice_metric_name)
        
        # Add values to bars
        for bar, value in zip(bars, values):
            # Format value based on metric
            if metric in ['annualized_return', 'max_drawdown', 'win_rate']:
                formatted_value = f"{value:.1%}"
            else:
                formatted_value = f"{value:.2f}"
                
            # Special handling for max_drawdown (we inverted it for plotting)
            if metric == 'max_drawdown':
                formatted_value = f"{-value:.1%}"
                
            axes[i].text(
                max(0.01, value * 0.95) if value > 0 else value * 1.05,
                bar.get_y() + bar.get_height() / 2,
                formatted_value,
                va='center'
            )
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved combined metrics plot to {output_path}")
    
    plt.close()

================
File: evaluation/README.md
================
# Evaluation Module

The evaluation module provides comprehensive tools for evaluating and comparing trading strategies, calculating performance metrics, and visualizing results.

## Core Components

### 1. Performance Metrics

The `metrics.py` module includes functions for calculating various performance metrics:

- `calculate_performance_metrics`: Calculates a comprehensive set of metrics including returns, Sharpe ratio, Sortino ratio, maximum drawdown, and win rate
- `calculate_transaction_costs`: Estimates transaction costs based on position changes
- `calculate_regime_performance`: Computes performance metrics broken down by market regime
- `calculate_drawdowns`: Identifies and quantifies major drawdown periods

### 2. Strategy Comparison

The `compare_strategies.py` module provides functions for comparing multiple strategies:

- `compare_strategies`: Basic comparison of strategy metrics
- `compare_strategies_with_transaction_costs`: Comprehensive comparison including transaction costs
- `calculate_rolling_metrics`: Calculates rolling performance metrics like Sharpe ratio
- `analyze_position_sizing`: Analyzes position sizing behavior across strategies
- `analyze_by_regime`: Analyzes strategy performance broken down by market regime
- `create_comprehensive_report`: Generates a complete analysis report with metrics and visualizations

### 3. Visualization

The `visualization.py` module offers plotting functions for strategy performance:

- `plot_cumulative_returns`: Plots cumulative returns for multiple strategies
- `plot_drawdowns`: Visualizes major drawdown periods
- `plot_regime_analysis`: Compares strategy performance across different market regimes
- `plot_position_comparison`: Compares position sizes across strategies
- `plot_performance_metrics_table`: Creates a formatted table of performance metrics

## Usage Examples

### Basic Strategy Comparison

```python
from dl_metalabeling.evaluation import compare_strategies_with_transaction_costs

# Define strategy columns to compare
strategy_cols = {
    'Original': 'position',
    'Meta-Labeled': 'meta_signal',
    'DQN': 'dqn_position_size'
}

# Compare strategies with transaction costs
metrics_df, result_df = compare_strategies_with_transaction_costs(
    data=data,
    strategy_cols=strategy_cols,
    returns_col='returns',
    transaction_cost=0.0001
)

print(metrics_df)
```

### Creating a Comprehensive Report

```python
from dl_metalabeling.evaluation import create_comprehensive_report

# Create a comprehensive report
report = create_comprehensive_report(
    data=data,
    strategy_cols=strategy_cols,
    returns_col='returns',
    regime_col='combined_regime',
    transaction_cost=0.0001,
    output_dir='results/strategy_comparison'
)
```

### Regime-Based Analysis

```python
from dl_metalabeling.evaluation import analyze_by_regime

# Analyze performance by regime
regime_performance = analyze_by_regime(
    data=data,
    regime_col='combined_regime',
    strategy_cols=strategy_cols,
    returns_col='returns'
)

# For each regime, examine performance
for regime, metrics in regime_performance.items():
    print(f"Regime {regime} - Count: {metrics['count']}")
    for strategy, strategy_metrics in metrics.items():
        if strategy != 'count' and strategy != 'frequency' and strategy != 'asset':
            print(f"  {strategy} - Sharpe: {strategy_metrics['sharpe_ratio']:.2f}, Return: {strategy_metrics['total_return']:.2%}")
```

### Visualizing Strategy Comparison

```python
from dl_metalabeling.evaluation import plot_cumulative_returns, plot_position_comparison

# Plot cumulative returns
returns_dict = {
    'Original': result_df['original_net_returns'],
    'Meta-Labeled': result_df['meta_labeled_net_returns'],
    'DQN': result_df['dqn_net_returns']
}

fig_returns = plot_cumulative_returns(
    returns_dict,
    title='Strategy Comparison',
    save_path='results/cumulative_returns.png'
)

# Plot position comparison
positions_dict = {
    'Original': data['position'],
    'Meta-Labeled': data['meta_signal'],
    'DQN': data['dqn_position_size']
}

fig_positions = plot_position_comparison(
    positions_dict,
    returns=data['returns'],
    save_path='results/position_comparison.png'
)
```

## Integration with Standalone Scripts

The evaluation module has integrated functionality from these standalone scripts:
- `compare_all_strategies.py`
- `compare_all_strategies_dqn.py`
- `compare_all_strategies_dreamer.py`
- `compare_all_strategies_ppo.py`
- `compare_all_strategies_sac.py`
- `compare_strategy_returns.py`
- `evaluate_cmma_with_regimes.py`
- `evaluate_cmma_with_bilstm.py`

These scripts can still be used as standalone applications, but their core functionality is now available through the evaluation module for more flexible integration into your projects.

================
File: evaluation/visualization.py
================
"""
Visualization Tools for Trading Strategies

This module provides functions for visualizing trading strategy performance,
including equity curves, drawdowns, regime analysis, and strategy comparisons.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.ticker import FuncFormatter
import seaborn as sns


def plot_cumulative_returns(returns_dict, title='Cumulative Returns', figsize=(12, 6), 
                           save_path=None, include_metrics=True, benchmark_col=None):
    """
    Plot cumulative returns for multiple strategies.
    
    Parameters:
    -----------
    returns_dict : dict
        Dictionary with strategy names as keys and returns series as values
    title : str
        Plot title
    figsize : tuple
        Figure size (width, height)
    save_path : str, optional
        Path to save the figure
    include_metrics : bool
        Whether to include metrics in the legend
    benchmark_col : str, optional
        Key in returns_dict to use as benchmark for relative performance
        
    Returns:
    --------
    matplotlib.figure.Figure: The figure object
    """
    fig, ax = plt.subplots(figsize=figsize)
    
    # Calculate and plot cumulative returns for each strategy
    for name, returns in returns_dict.items():
        # Convert to pandas Series if not already
        returns_series = pd.Series(returns)
        
        # Calculate cumulative returns
        cum_returns = (1 + returns_series).cumprod() - 1
        
        # Calculate metrics if requested
        if include_metrics:
            total_return = cum_returns.iloc[-1]
            sharpe = returns_series.mean() / returns_series.std() * np.sqrt(252) if returns_series.std() > 0 else 0
            label = f"{name} (Return: {total_return:.2%}, Sharpe: {sharpe:.2f})"
        else:
            label = name
            
        # Plot the series
        ax.plot(cum_returns.index, cum_returns, label=label)
    
    # Format the plot
    ax.set_title(title, fontsize=14)
    ax.set_xlabel('Date', fontsize=12)
    ax.set_ylabel('Cumulative Return', fontsize=12)
    ax.legend(loc='best')
    ax.grid(True, alpha=0.3)
    
    # Format y-axis as percentage
    ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.0%}'))
    
    # Format x-axis dates
    if isinstance(list(returns_dict.values())[0].index[0], pd.Timestamp):
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        ax.xaxis.set_major_locator(mdates.YearLocator())
        fig.autofmt_xdate()
    
    plt.tight_layout()
    
    # Save if path provided
    if save_path:
        plt.savefig(save_path, dpi=300)
    
    return fig


def plot_drawdowns(returns, top_n=5, figsize=(12, 8), save_path=None):
    """
    Plot the top N drawdowns for a return series.
    
    Parameters:
    -----------
    returns : pd.Series
        Series of returns
    top_n : int
        Number of largest drawdowns to plot
    figsize : tuple
        Figure size (width, height)
    save_path : str, optional
        Path to save the figure
    
    Returns:
    --------
    matplotlib.figure.Figure: The figure object
    """
    # Calculate cumulative returns
    cum_returns = (1 + pd.Series(returns)).cumprod()
    
    # Calculate drawdowns
    drawdowns = cum_returns / cum_returns.cummax() - 1
    
    # Find drawdown periods
    is_drawdown = drawdowns < 0
    
    # Group consecutive drawdown periods
    drawdown_groups = (is_drawdown.astype(int).diff() != 0).cumsum()[is_drawdown]
    
    # Calculate drawdown statistics for each period
    result = []
    
    for group_id in drawdown_groups.unique():
        group_mask = drawdown_groups == group_id
        group_drawdowns = drawdowns[group_mask]
        
        if len(group_drawdowns) > 0:
            max_drawdown = group_drawdowns.min()
            start_date = group_drawdowns.index[0]
            end_date = group_drawdowns.index[-1]
            duration = len(group_drawdowns)
            
            result.append({
                'group_id': group_id,
                'max_drawdown': max_drawdown,
                'start_date': start_date,
                'end_date': end_date,
                'duration': duration
            })
    
    # Sort drawdowns by magnitude
    drawdown_periods = pd.DataFrame(result).sort_values('max_drawdown')
    
    # Select top N drawdowns
    top_drawdowns = drawdown_periods.head(top_n)
    
    # Plot
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, gridspec_kw={'height_ratios': [2, 1]})
    
    # Plot cumulative returns
    ax1.plot(cum_returns.index, cum_returns, label='Cumulative Return')
    ax1.set_title('Cumulative Returns and Major Drawdowns', fontsize=14)
    ax1.set_ylabel('Value', fontsize=12)
    ax1.grid(True, alpha=0.3)
    
    # Highlight drawdown periods
    colors = sns.color_palette('husl', n_colors=len(top_drawdowns))
    
    for i, (_, drawdown) in enumerate(top_drawdowns.iterrows()):
        mask = (cum_returns.index >= drawdown['start_date']) & (cum_returns.index <= drawdown['end_date'])
        ax1.fill_between(
            cum_returns.index[mask], 
            cum_returns[mask], 
            alpha=0.3, 
            color=colors[i], 
            label=f"DD #{i+1}: {drawdown['max_drawdown']:.1%}"
        )
    
    ax1.legend(loc='best')
    
    # Plot drawdowns
    ax2.plot(drawdowns.index, drawdowns, color='red', alpha=0.7)
    ax2.set_title('Drawdowns', fontsize=14)
    ax2.set_xlabel('Date', fontsize=12)
    ax2.set_ylabel('Drawdown', fontsize=12)
    ax2.grid(True, alpha=0.3)
    
    # Format y-axis as percentage
    ax2.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.0%}'))
    
    # Highlight same drawdown periods
    for i, (_, drawdown) in enumerate(top_drawdowns.iterrows()):
        mask = (drawdowns.index >= drawdown['start_date']) & (drawdowns.index <= drawdown['end_date'])
        ax2.fill_between(
            drawdowns.index[mask], 
            drawdowns[mask], 
            0,
            alpha=0.4, 
            color=colors[i]
        )
    
    plt.tight_layout()
    
    # Save if path provided
    if save_path:
        plt.savefig(save_path, dpi=300)
    
    return fig


def plot_regime_analysis(data, regime_col, returns_col, position_col=None, 
                        figsize=(15, 10), save_path=None):
    """
    Plot returns and strategy performance across different market regimes.
    
    Parameters:
    -----------
    data : pd.DataFrame
        DataFrame with regime labels, returns, and optionally position sizes
    regime_col : str
        Column name for regime labels
    returns_col : str
        Column name for returns
    position_col : str, optional
        Column name for position sizes. If provided, calculates strategy returns
    figsize : tuple
        Figure size (width, height)
    save_path : str, optional
        Path to save the figure
    
    Returns:
    --------
    matplotlib.figure.Figure: The figure object
    """
    # Create a copy to avoid modifying the original
    df = data.copy()
    
    # Calculate strategy returns if position column is provided
    if position_col and position_col in df.columns:
        df['strategy_returns'] = df[position_col].shift(1).fillna(0) * df[returns_col]
    
    # Calculate metrics for each regime
    regimes = df[regime_col].unique()
    regime_metrics = []
    
    for regime in regimes:
        regime_data = df[df[regime_col] == regime]
        
        # Asset metrics
        asset_sharpe = regime_data[returns_col].mean() / regime_data[returns_col].std() * np.sqrt(252) if regime_data[returns_col].std() > 0 else 0
        asset_return = regime_data[returns_col].mean() * 252  # Annualized
        
        metrics = {
            'regime': regime,
            'count': len(regime_data),
            'frequency': len(regime_data) / len(df),
            'asset_return': asset_return,
            'asset_sharpe': asset_sharpe
        }
        
        # Strategy metrics if available
        if 'strategy_returns' in df.columns:
            strat_sharpe = regime_data['strategy_returns'].mean() / regime_data['strategy_returns'].std() * np.sqrt(252) if regime_data['strategy_returns'].std() > 0 else 0
            strat_return = regime_data['strategy_returns'].mean() * 252  # Annualized
            
            metrics.update({
                'strategy_return': strat_return,
                'strategy_sharpe': strat_sharpe
            })
            
        regime_metrics.append(metrics)
    
    # Convert to DataFrame for easier plotting
    metrics_df = pd.DataFrame(regime_metrics)
    
    # Plotting
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # Plot 1: Regime frequency
    metrics_df.plot(x='regime', y='frequency', kind='bar', ax=axes[0, 0], color='skyblue')
    axes[0, 0].set_title('Regime Frequency', fontsize=12)
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].grid(axis='y', alpha=0.3)
    
    # Plot 2: Asset returns by regime
    metrics_df.plot(x='regime', y='asset_return', kind='bar', ax=axes[0, 1], color='lightgreen')
    axes[0, 1].set_title('Asset Returns by Regime', fontsize=12)
    axes[0, 1].set_ylabel('Annualized Return')
    axes[0, 1].yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.1%}'))
    axes[0, 1].grid(axis='y', alpha=0.3)
    
    # Plot 3: Regime distribution over time
    regime_dummies = pd.get_dummies(df[regime_col])
    
    # Create stacked area chart
    regime_dummies.index = df.index
    regime_dummies = regime_dummies.resample('M').mean()  # Monthly resampling
    
    regime_dummies.plot.area(ax=axes[1, 0], alpha=0.7, stacked=True)
    axes[1, 0].set_title('Regime Distribution Over Time', fontsize=12)
    axes[1, 0].set_ylabel('Proportion')
    axes[1, 0].grid(alpha=0.3)
    
    # Plot 4: Strategy vs Asset returns by regime (if strategy returns available)
    if 'strategy_return' in metrics_df.columns:
        metrics_df.plot(x='regime', y=['asset_return', 'strategy_return'], kind='bar', ax=axes[1, 1])
        axes[1, 1].set_title('Strategy vs Asset Returns by Regime', fontsize=12)
        axes[1, 1].set_ylabel('Annualized Return')
        axes[1, 1].yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.1%}'))
        axes[1, 1].grid(axis='y', alpha=0.3)
        axes[1, 1].legend(['Asset', 'Strategy'])
    else:
        axes[1, 1].axis('off')
    
    plt.tight_layout()
    
    # Save if path provided
    if save_path:
        plt.savefig(save_path, dpi=300)
    
    return fig


def plot_position_comparison(positions_dict, returns=None, figsize=(12, 8), save_path=None):
    """
    Plot position sizes for multiple strategies and optionally returns.
    
    Parameters:
    -----------
    positions_dict : dict
        Dictionary with strategy names as keys and position series as values
    returns : pd.Series, optional
        Asset returns to plot on a secondary axis
    figsize : tuple
        Figure size (width, height)
    save_path : str, optional
        Path to save the figure
    
    Returns:
    --------
    matplotlib.figure.Figure: The figure object
    """
    fig, ax1 = plt.subplots(figsize=figsize)
    
    # Plot positions
    for name, positions in positions_dict.items():
        ax1.plot(positions.index, positions, label=name, alpha=0.7)
    
    ax1.set_title('Strategy Position Comparison', fontsize=14)
    ax1.set_ylabel('Position Size', fontsize=12)
    ax1.grid(True, alpha=0.3)
    
    # Add returns on secondary axis if provided
    if returns is not None:
        ax2 = ax1.twinx()
        ax2.fill_between(returns.index, returns, 0, where=returns > 0, color='lightgreen', alpha=0.3)
        ax2.fill_between(returns.index, returns, 0, where=returns < 0, color='lightcoral', alpha=0.3)
        ax2.set_ylabel('Returns', color='green', fontsize=12)
        
        # Set y-axis limits to make returns visible but not overwhelming the position plot
        max_abs_return = max(abs(returns.max()), abs(returns.min()))
        y_limit = min(max_abs_return * 3, 0.05)  # Cap at 5% for extreme cases
        ax2.set_ylim(-y_limit, y_limit)
    
    # Format x-axis dates
    if isinstance(list(positions_dict.values())[0].index[0], pd.Timestamp):
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        ax1.xaxis.set_major_locator(mdates.YearLocator())
        fig.autofmt_xdate()
    
    ax1.legend(loc='best')
    plt.tight_layout()
    
    # Save if path provided
    if save_path:
        plt.savefig(save_path, dpi=300)
    
    return fig


def plot_performance_metrics_table(metrics_dict, figsize=(10, 6), save_path=None):
    """
    Plot a formatted table of performance metrics for multiple strategies.
    
    Parameters:
    -----------
    metrics_dict : dict
        Dictionary with strategy names as keys and metric dictionaries as values
    figsize : tuple
        Figure size (width, height)
    save_path : str, optional
        Path to save the figure
    
    Returns:
    --------
    matplotlib.figure.Figure: The figure object
    """
    # Convert metrics to DataFrame
    metrics_df = pd.DataFrame(metrics_dict).T
    
    # Select most important metrics
    if 'total_return' in metrics_df.columns:
        metrics_to_show = [
            'total_return', 'annualized_return', 'volatility', 'sharpe_ratio',
            'max_drawdown', 'calmar_ratio', 'win_rate'
        ]
        metrics_df = metrics_df[metrics_to_show]
    
    # Create plot
    fig, ax = plt.subplots(figsize=figsize)
    ax.axis('off')
    
    # Format metrics for display
    metrics_display = metrics_df.copy()
    
    # Apply formatting based on column type
    for col in metrics_display.columns:
        if 'return' in col.lower() or 'drawdown' in col.lower() or 'rate' in col.lower():
            metrics_display[col] = metrics_display[col].map(lambda x: f'{x:.2%}')
        elif 'ratio' in col.lower():
            metrics_display[col] = metrics_display[col].map(lambda x: f'{x:.2f}')
        elif 'volatility' in col.lower():
            metrics_display[col] = metrics_display[col].map(lambda x: f'{x:.2%}')
    
    # Create table
    table = ax.table(
        cellText=metrics_display.values,
        rowLabels=metrics_display.index,
        colLabels=metrics_display.columns,
        cellLoc='center',
        loc='center'
    )
    
    # Set table formatting
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)
    
    # Customize header
    for i, key in enumerate(metrics_display.columns):
        cell = table[0, i]
        cell.set_text_props(weight='bold', color='white')
        cell.set_facecolor('#4472C4')
    
    # Alternate row colors
    for i, key in enumerate(metrics_display.index):
        row_idx = i + 1  # +1 because header is row 0
        for j in range(len(metrics_display.columns)):
            cell = table[row_idx, j]
            if i % 2 == 0:
                cell.set_facecolor('#E6F0FF')
            else:
                cell.set_facecolor('#D4E4FF')
    
    plt.title('Strategy Performance Comparison', fontsize=14, pad=20)
    plt.tight_layout()
    
    # Save if path provided
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    return fig

================
File: main.py
================
"""
DL MetaLabeling Package API

This is the main interface for using the DL MetaLabeling package.
It provides a unified API for:
1. Market regime detection (HMM and Transformer)
2. Meta-labeling with ML/DL models
3. DL position sizing with various RL approaches
4. Visualization and performance comparison

Usage:
```python
from dl_metalabeling.main import DLMetaLabAPI

# Initialize API
api = DLMetaLabAPI()

# Run full pipeline with default settings
results = api.run_pipeline()

# Compare all position sizing models
comparison = api.compare_position_sizing_models()

# Plot results
api.plot_cumulative_returns()
api.plot_performance_metrics()
api.plot_regime_distribution()
```
"""

import os
import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Union, Any

# Import all components from the package
from dl_metalabeling.project_plan import DLMetaLabelingFramework
from dl_metalabeling.market_regimes import HMMRegimeDetector, TransformerRegimeDetector
from dl_metalabeling.metalabeling import MetaLabeling, RegimeMetaLabeling
from dl_metalabeling.utils import calculate_performance_metrics
from dl_metalabeling.models.base_rl import RLPositionSizer
from dl_metalabeling.models.dqn import DQNPositionSizer
from dl_metalabeling.models.ppo import PPOPositionSizer
from dl_metalabeling.models.sac import SACPositionSizer
from dl_metalabeling.models.dreamer import DreamerPositionSizer
from dl_metalabeling.utils import load_data, preprocess_data

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


class DLMetaLabAPI:
    """
    Main API for DL MetaLabeling package.
    Provides methods to run, analyze, and visualize the performance of various
    market regime detection, meta-labeling, and position sizing models.
    """
    
    def __init__(self, data_path: str = 'data/cmma.csv', config: Dict = None):
        """
        Initialize the API with data and configuration.
        
        Args:
            data_path (str): Path to data file
            config (dict): Custom configuration (optional)
        """
        self.data_path = data_path
        self.config = config
        
        # Results storage
        self.frameworks = {}
        self.results = {}
        self.evaluations = {}
        self.comparison_data = {}
        
        # Default position sizing models to compare
        self.position_sizing_models = ['base_rl', 'dqn', 'ppo', 'sac', 'dreamer']
        
        # Initialize but don't run yet
        self._initialize_framework()
        
        logger.info(f"DL MetaLabeling API initialized with data path: {data_path}")
    
    def _initialize_framework(self):
        """Initialize the framework with appropriate configuration."""
        # Create default framework
        self.framework = DLMetaLabelingFramework(config=self.config)
        
        # Ensure both HMM and Transformer regimes are used
        if self.config is None:
            # If no custom config, ensure both regime detectors are enabled
            self.framework.config['regime_detection']['methods'] = ['hmm', 'transformer']
        
        # Update data path
        self.framework.config['data']['filepath'] = self.data_path
        
        logger.info("Framework initialized with both HMM and Transformer regime detectors")
    
    def run_pipeline(self, position_sizing_model: str = 'dqn') -> pd.DataFrame:
        """
        Run the full pipeline with a specific position sizing model.
        
        Args:
            position_sizing_model (str): Position sizing model to use
                ('base_rl', 'dqn', 'ppo', 'sac', 'dreamer')
        
        Returns:
            pd.DataFrame: Final data with all components applied
        """
        logger.info(f"Running full pipeline with {position_sizing_model} position sizing")
        
        # Update position sizing model
        self.framework.config['position_sizing']['model'] = position_sizing_model
        
        # Run the full pipeline
        final_data = self.framework.run_pipeline()
        
        # Store results
        self.frameworks[position_sizing_model] = self.framework
        self.results[position_sizing_model] = final_data
        
        # Evaluate and store
        evaluation = self.framework.evaluate_on_test_data()
        self.evaluations[position_sizing_model] = evaluation
        
        logger.info(f"Pipeline completed for {position_sizing_model}")
        return final_data
    
    def compare_position_sizing_models(self, 
                                       models: List[str] = None,
                                       output_dir: str = 'results',
                                       save_results: bool = True) -> pd.DataFrame:
        """
        Run and compare all position sizing models.
        
        Args:
            models (list): List of models to compare (if None, use all)
            output_dir (str): Directory to save results
            save_results (bool): Whether to save results to disk
            
        Returns:
            pd.DataFrame: Comparison data
        """
        logger.info("Comparing position sizing models")
        
        # Use provided models or default list
        models = models or self.position_sizing_models
        
        # Create output directories if needed
        if save_results:
            os.makedirs(output_dir, exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'models'), exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
        
        # List to store comparison data
        comparison_data = []
        
        # Run pipeline for each model
        for model in models:
            logger.info(f"Running pipeline for {model} model")
            
            try:
                # Run pipeline
                final_data = self.run_pipeline(model)
                
                # Get evaluation
                evaluation = self.evaluations[model]
                
                # Add to comparison data
                comparison_data.append({
                    'Model': model,
                    'Base Sharpe': evaluation['base']['sharpe_ratio'],
                    'Meta Sharpe': evaluation['meta']['sharpe_ratio'],
                    'RL Sharpe': evaluation['rl']['sharpe_ratio'],
                    'Base Return': evaluation['base']['annual_return'],
                    'Meta Return': evaluation['meta']['annual_return'],
                    'RL Return': evaluation['rl']['annual_return'],
                    'Base Drawdown': evaluation['base']['max_drawdown'],
                    'Meta Drawdown': evaluation['meta']['max_drawdown'],
                    'RL Drawdown': evaluation['rl']['max_drawdown'],
                    'RL Win Rate': evaluation['rl']['win_rate'],
                    'RL Profit Factor': evaluation['rl']['profit_factor']
                })
                
                # Save models if requested
                if save_results:
                    model_dir = os.path.join(output_dir, 'models', f'position_{model}')
                    os.makedirs(model_dir, exist_ok=True)
                    self.frameworks[model].save_models(model_dir)
                    
                    # Save returns data
                    returns_data = pd.DataFrame({
                        'DateTime': final_data.index,
                        'Base Returns': final_data['strategy_returns'],
                        'Meta Returns': final_data['meta_strategy_returns'],
                        'RL Returns': final_data['rl_strategy_returns']
                    })
                    returns_data.to_csv(os.path.join(output_dir, 'results', f'position_{model}_returns.csv'), index=False)
            
            except Exception as e:
                logger.error(f"Error running {model} model: {e}")
        
        # Convert to DataFrame
        comparison_df = pd.DataFrame(comparison_data)
        self.comparison_data = comparison_df
        
        # Save comparison if requested
        if save_results and not comparison_df.empty:
            comparison_df.to_csv(os.path.join(output_dir, 'results', 'position_comparison.csv'), index=False)
        
        logger.info("Position sizing model comparison completed")
        return comparison_df
    
    def plot_cumulative_returns(self, 
                              models: List[str] = None, 
                              save_path: str = None,
                              figsize: Tuple[int, int] = (15, 8)) -> plt.Figure:
        """
        Plot cumulative returns for different position sizing models.
        
        Args:
            models (list): List of model names to include in the plot
            save_path (str): Path to save the plot (if None, don't save)
            figsize (tuple): Figure size
            
        Returns:
            matplotlib.figure.Figure: Plot figure
        """
        logger.info("Plotting cumulative returns")
        
        # Use provided models or all available
        models = models or list(self.results.keys())
        
        # Create figure
        fig, ax = plt.subplots(figsize=figsize)
        
        # Plot cumulative returns for each model
        for model in models:
            if model in self.results:
                final_data = self.results[model]
                if 'rl_strategy_returns' in final_data.columns:
                    cum_returns = (1 + final_data['rl_strategy_returns']).cumprod() - 1
                    ax.plot(cum_returns, label=f"{model.upper()}")
                else:
                    logger.warning(f"'rl_strategy_returns' column not found for model {model}")
        
        # Add base strategy and meta-labeled strategy if they exist
        if models and models[0] in self.results:
            final_data = self.results[models[0]]
            
            # Plot base strategy returns if available
            if 'strategy_returns' in final_data.columns:
                ax.plot((1 + final_data['strategy_returns']).cumprod() - 1, 
                        label='Base Strategy', linestyle='--', color='gray')
            elif 'returns' in final_data.columns:
                # Use the returns column if strategy_returns is not available
                logger.info("Using 'returns' column as fallback for base strategy")
                ax.plot((1 + final_data['returns']).cumprod() - 1, 
                        label='Base Strategy', linestyle='--', color='gray')
            else:
                logger.warning("Neither 'strategy_returns' nor 'returns' columns found for base strategy")
            
            # Plot meta-labeled strategy returns if available
            if 'meta_strategy_returns' in final_data.columns:
                ax.plot((1 + final_data['meta_strategy_returns']).cumprod() - 1, 
                        label='Meta-Labeled', linestyle='-.', color='black')
            else:
                logger.warning("'meta_strategy_returns' column not found")
        
        # Add labels and legend
        ax.set_title('Cumulative Returns Comparison', fontsize=16)
        ax.set_xlabel('Time', fontsize=12)
        ax.set_ylabel('Cumulative Returns', fontsize=12)
        ax.legend(loc='best')
        ax.grid(True)
        
        # Tight layout
        plt.tight_layout()
        
        # Save if path provided
        if save_path:
            plt.savefig(save_path)
            logger.info(f"Cumulative returns plot saved to {save_path}")
        
        return fig
    
    def plot_performance_metrics(self,
                               metrics: List[str] = None,
                               models: List[str] = None,
                               save_path: str = None,
                               figsize: Tuple[int, int] = (15, 10)) -> plt.Figure:
        """
        Plot performance metrics comparison for different position sizing models.
        
        Args:
            metrics (list): Metrics to plot
            models (list): Models to include
            save_path (str): Path to save the plot
            figsize (tuple): Figure size
            
        Returns:
            matplotlib.figure.Figure: Plot figure
        """
        logger.info("Plotting performance metrics comparison")
        
        # Default metrics if not provided
        if metrics is None:
            metrics = ['Sharpe', 'Return', 'Drawdown', 'Win Rate', 'Profit Factor']
        
        # Use provided models or all from comparison data
        if self.comparison_data.empty:
            logger.warning("No comparison data available. Run compare_position_sizing_models first.")
            return None
        
        # Filter models if provided
        df = self.comparison_data
        if models:
            df = df[df['Model'].isin(models)]
        
        # Create figure
        n_metrics = len(metrics)
        n_cols = min(3, n_metrics)
        n_rows = (n_metrics + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
        axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]
        
        # Plot each metric
        for i, metric in enumerate(metrics):
            if i < len(axes):
                ax = axes[i]
                
                # Determine which column to use
                if metric == 'Sharpe':
                    col = 'RL Sharpe'
                elif metric == 'Return':
                    col = 'RL Return'
                elif metric == 'Drawdown':
                    col = 'RL Drawdown'
                elif metric == 'Win Rate':
                    col = 'RL Win Rate'
                elif metric == 'Profit Factor':
                    col = 'RL Profit Factor'
                else:
                    continue
                
                # Check if column exists
                if col in df.columns:
                    sns.barplot(x='Model', y=col, data=df, ax=ax)
                    ax.set_title(f'{metric} Comparison', fontsize=14)
                    ax.set_xlabel('')
                    ax.grid(True, alpha=0.3)
                    
                    # Format y-axis for percentage metrics
                    if metric in ['Return', 'Drawdown', 'Win Rate']:
                        vals = ax.get_yticks()
                        ax.set_yticklabels([f'{x:.1%}' for x in vals])
                    
                    plt.setp(ax.get_xticklabels(), rotation=45)
        
        # Hide unused axes
        for i in range(len(metrics), len(axes)):
            axes[i].set_visible(False)
        
        # Add a main title
        plt.suptitle('Position Sizing Models Performance Comparison', fontsize=16, y=1.02)
        
        # Tight layout
        plt.tight_layout()
        
        # Save if path provided
        if save_path:
            plt.savefig(save_path)
            logger.info(f"Performance metrics plot saved to {save_path}")
        
        return fig
    
    def plot_regime_distribution(self, 
                               model: str = None,
                               regime_column: str = 'hmm_regime',
                               save_path: str = None,
                               figsize: Tuple[int, int] = (15, 10)) -> plt.Figure:
        """
        Plot market regime distribution and performance by regime.
        
        Args:
            model (str): Model to analyze (if None, use first available)
            regime_column (str): Column with regime labels ('hmm_regime' or 'transformer_regime')
            save_path (str): Path to save the plot
            figsize (tuple): Figure size
            
        Returns:
            matplotlib.figure.Figure: Plot figure
        """
        logger.info(f"Plotting regime distribution for {regime_column}")
        
        # Get model data
        if model is None and self.results:
            model = list(self.results.keys())[0]
        
        if model not in self.results:
            logger.warning(f"No data available for model {model}")
            return None
        
        final_data = self.results[model]
        
        # Check if regime column exists
        if regime_column not in final_data.columns:
            logger.warning(f"Regime column {regime_column} not found in data")
            return None
        
        # Create figure
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        # 1. Plot regime distribution over time
        ax1 = axes[0, 0]
        scatter = ax1.scatter(
            range(len(final_data)),
            final_data['returns'],
            c=final_data[regime_column],
            cmap='viridis',
            alpha=0.6,
            s=10
        )
        ax1.set_title(f'Returns Colored by {regime_column}', fontsize=14)
        ax1.set_xlabel('Time')
        ax1.set_ylabel('Returns')
        ax1.grid(True, alpha=0.3)
        legend1 = ax1.legend(*scatter.legend_elements(), title="Regimes")
        ax1.add_artist(legend1)
        
        # 2. Plot regime count
        ax2 = axes[0, 1]
        regime_counts = final_data[regime_column].value_counts()
        regime_counts.plot(kind='bar', ax=ax2)
        ax2.set_title(f'Regime Distribution', fontsize=14)
        ax2.set_xlabel('Regime')
        ax2.set_ylabel('Count')
        ax2.grid(True, alpha=0.3)
        
        # 3. Plot performance by regime (Sharpe ratio)
        ax3 = axes[1, 0]
        regimes = final_data[regime_column].unique()
        sharpe_by_regime = []
        
        for regime in regimes:
            regime_data = final_data[final_data[regime_column] == regime]
            
            if len(regime_data) > 10:  # Only if we have enough data
                base_perf = calculate_performance_metrics(regime_data['strategy_returns'])
                meta_perf = calculate_performance_metrics(regime_data['meta_strategy_returns'])
                rl_perf = calculate_performance_metrics(regime_data['rl_strategy_returns'])
                
                sharpe_by_regime.append({
                    'Regime': regime,
                    'Base': base_perf['sharpe_ratio'],
                    'Meta': meta_perf['sharpe_ratio'],
                    'RL': rl_perf['sharpe_ratio']
                })
        
        if sharpe_by_regime:
            sharpe_df = pd.DataFrame(sharpe_by_regime)
            sharpe_df.set_index('Regime', inplace=True)
            sharpe_df.plot(kind='bar', ax=ax3)
            ax3.set_title('Sharpe Ratio by Regime', fontsize=14)
            ax3.set_xlabel('Regime')
            ax3.set_ylabel('Sharpe Ratio')
            ax3.grid(True, alpha=0.3)
        
        # 4. Plot returns by regime
        ax4 = axes[1, 1]
        returns_by_regime = []
        
        for regime in regimes:
            regime_data = final_data[final_data[regime_column] == regime]
            
            if len(regime_data) > 10:  # Only if we have enough data
                base_perf = calculate_performance_metrics(regime_data['strategy_returns'])
                meta_perf = calculate_performance_metrics(regime_data['meta_strategy_returns'])
                rl_perf = calculate_performance_metrics(regime_data['rl_strategy_returns'])
                
                returns_by_regime.append({
                    'Regime': regime,
                    'Base': base_perf['annual_return'],
                    'Meta': meta_perf['annual_return'],
                    'RL': rl_perf['annual_return']
                })
        
        if returns_by_regime:
            returns_df = pd.DataFrame(returns_by_regime)
            returns_df.set_index('Regime', inplace=True)
            returns_df.plot(kind='bar', ax=ax4)
            ax4.set_title('Annual Return by Regime', fontsize=14)
            ax4.set_xlabel('Regime')
            ax4.set_ylabel('Annual Return')
            ax4.grid(True, alpha=0.3)
            
            # Format y-axis for percentage
            vals = ax4.get_yticks()
            ax4.set_yticklabels([f'{x:.1%}' for x in vals])
        
        # Add a main title
        plt.suptitle(f'Market Regime Analysis: {regime_column.replace("_", " ").title()}', fontsize=16, y=1.02)
        
        # Tight layout
        plt.tight_layout()
        
        # Save if path provided
        if save_path:
            plt.savefig(save_path)
            logger.info(f"Regime distribution plot saved to {save_path}")
        
        return fig
    
    def run_comprehensive_analysis(self, 
                                  output_dir: str = 'results',
                                  save_results: bool = True,
                                  save_plots: bool = True) -> Dict:
        """
        Run a comprehensive analysis of all position sizing models.
        
        Args:
            output_dir (str): Directory to save results and plots
            save_results (bool): Whether to save results to disk
            save_plots (bool): Whether to save plots to disk
            
        Returns:
            dict: Dictionary with all results and analysis
        """
        logger.info("Running comprehensive analysis")
        
        # Create output directories
        if save_results or save_plots:
            os.makedirs(output_dir, exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'plots'), exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'models'), exist_ok=True)
        
        # Compare all position sizing models
        comparison = self.compare_position_sizing_models(
            output_dir=output_dir,
            save_results=save_results
        )
        
        # Generate plots
        plots = {}
        
        if save_plots:
            # Cumulative returns plot
            plots['cumulative_returns'] = self.plot_cumulative_returns(
                save_path=os.path.join(output_dir, 'plots', 'cumulative_returns.png')
            )
            
            # Performance metrics plot
            plots['performance_metrics'] = self.plot_performance_metrics(
                save_path=os.path.join(output_dir, 'plots', 'performance_metrics.png')
            )
            
            # HMM regime distribution plot
            plots['hmm_regime'] = self.plot_regime_distribution(
                regime_column='hmm_regime',
                save_path=os.path.join(output_dir, 'plots', 'hmm_regime_distribution.png')
            )
            
            # Transformer regime distribution plot
            plots['transformer_regime'] = self.plot_regime_distribution(
                regime_column='transformer_regime',
                save_path=os.path.join(output_dir, 'plots', 'transformer_regime_distribution.png')
            )
        else:
            # Generate plots without saving
            plots['cumulative_returns'] = self.plot_cumulative_returns()
            plots['performance_metrics'] = self.plot_performance_metrics()
            plots['hmm_regime'] = self.plot_regime_distribution(regime_column='hmm_regime')
            plots['transformer_regime'] = self.plot_regime_distribution(regime_column='transformer_regime')
        
        # Create summary report
        summary = {
            'best_model': None,
            'best_sharpe': -np.inf,
            'best_return': -np.inf,
            'regime_effectiveness': {}
        }
        
        # Find best model
        if not comparison.empty:
            best_idx = comparison['RL Sharpe'].idxmax()
            summary['best_model'] = comparison.loc[best_idx, 'Model']
            summary['best_sharpe'] = comparison.loc[best_idx, 'RL Sharpe']
            summary['best_return'] = comparison.loc[best_idx, 'RL Return']
        
        # Analyze regime effectiveness
        for model in self.results:
            final_data = self.results[model]
            
            # Check regime columns
            for regime_col in ['hmm_regime', 'transformer_regime']:
                if regime_col in final_data.columns:
                    regimes = final_data[regime_col].unique()
                    regime_performance = {}
                    
                    for regime in regimes:
                        regime_data = final_data[final_data[regime_col] == regime]
                        
                        if len(regime_data) > 10:  # Only if we have enough data
                            base_perf = calculate_performance_metrics(regime_data['strategy_returns'])
                            rl_perf = calculate_performance_metrics(regime_data['rl_strategy_returns'])
                            
                            # Calculate improvement
                            sharpe_improvement = rl_perf['sharpe_ratio'] - base_perf['sharpe_ratio']
                            return_improvement = rl_perf['annual_return'] - base_perf['annual_return']
                            
                            regime_performance[regime] = {
                                'count': len(regime_data),
                                'sharpe_improvement': sharpe_improvement,
                                'return_improvement': return_improvement
                            }
                    
                    if regime_performance:
                        if model not in summary['regime_effectiveness']:
                            summary['regime_effectiveness'][model] = {}
                        
                        summary['regime_effectiveness'][model][regime_col] = regime_performance
        
        # Save summary if requested
        if save_results:
            # Convert summary to DataFrame for the parts that can be converted
            if summary['best_model'] is not None:
                summary_df = pd.DataFrame([{
                    'Best Model': summary['best_model'],
                    'Best Sharpe': summary['best_sharpe'],
                    'Best Return': summary['best_return']
                }])
                summary_df.to_csv(os.path.join(output_dir, 'results', 'summary.csv'), index=False)
            
            # Save regime effectiveness as separate CSV files
            for model, regime_data in summary['regime_effectiveness'].items():
                for regime_col, regime_perf in regime_data.items():
                    regime_df = pd.DataFrame.from_dict(regime_perf, orient='index')
                    regime_df.to_csv(os.path.join(output_dir, 'results', f'{model}_{regime_col}_effectiveness.csv'))
        
        logger.info("Comprehensive analysis completed")
        
        return {
            'comparison': comparison,
            'plots': plots,
            'summary': summary,
            'models': self.frameworks,
            'results': self.results,
            'evaluations': self.evaluations
        }
    
    def load_models(self, base_dir: str = 'models', model: str = None) -> None:
        """
        Load saved models.
        
        Args:
            base_dir (str): Base directory with saved models
            model (str): Specific model to load (if None, load all available)
        """
        logger.info(f"Loading models from {base_dir}")
        
        # Initialize new frameworks for each model
        if model:
            # Load specific model
            model_dir = os.path.join(base_dir, f'position_{model}')
            if os.path.exists(model_dir):
                framework = DLMetaLabelingFramework(config=self.config)
                framework.load_models(model_dir)
                self.frameworks[model] = framework
                logger.info(f"Loaded model: {model}")
            else:
                logger.warning(f"Model directory not found: {model_dir}")
        else:
            # Try to load all models
            for model_name in self.position_sizing_models:
                model_dir = os.path.join(base_dir, f'position_{model_name}')
                if os.path.exists(model_dir):
                    framework = DLMetaLabelingFramework(config=self.config)
                    framework.load_models(model_dir)
                    self.frameworks[model_name] = framework
                    logger.info(f"Loaded model: {model_name}")
        
        logger.info("Model loading completed")


# Example usage
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='DL MetaLabeling Package API')
    parser.add_argument('--data_path', type=str, default='data/cmma.csv', help='Path to data file')
    parser.add_argument('--output_dir', type=str, default='results', help='Directory to save results')
    parser.add_argument('--models', type=str, nargs='+', default=None, 
                      help='Position sizing models to use (if not specified, use all)')
    parser.add_argument('--analysis', action='store_true', help='Run comprehensive analysis')
    parser.add_argument('--load_models', action='store_true', help='Load saved models instead of training')
    parser.add_argument('--plot_only', action='store_true', help='Only generate plots (requires saved results)')
    
    args = parser.parse_args()
    
    # Initialize API
    api = DLMetaLabAPI(data_path=args.data_path)
    
    if args.plot_only:
        # Just load results and generate plots
        try:
            # Try to load comparison data
            comparison_path = os.path.join(args.output_dir, 'results', 'position_comparison.csv')
            if os.path.exists(comparison_path):
                api.comparison_data = pd.read_csv(comparison_path)
                
                # Generate plots
                api.plot_cumulative_returns(save_path=os.path.join(args.output_dir, 'plots', 'cumulative_returns.png'))
                api.plot_performance_metrics(save_path=os.path.join(args.output_dir, 'plots', 'performance_metrics.png'))
                
                print("Plots generated successfully")
            else:
                print(f"Comparison data not found at {comparison_path}")
        except Exception as e:
            print(f"Error generating plots: {e}")
    
    elif args.load_models:
        # Load saved models
        api.load_models(args.output_dir)
        
        if args.analysis:
            # Run analysis with loaded models
            api.run_comprehensive_analysis(args.output_dir)
    
    elif args.analysis:
        # Run comprehensive analysis
        models = args.models or api.position_sizing_models
        api.position_sizing_models = models
        api.run_comprehensive_analysis(args.output_dir)
    
    else:
        # Compare specified position sizing models
        models = args.models or api.position_sizing_models
        api.position_sizing_models = models
        
        comparison = api.compare_position_sizing_models(models=models, output_dir=args.output_dir)
        
        # Generate and save basic plots
        api.plot_cumulative_returns(save_path=os.path.join(args.output_dir, 'plots', 'cumulative_returns.png'))
        api.plot_performance_metrics(save_path=os.path.join(args.output_dir, 'plots', 'performance_metrics.png'))

================
File: market_regimes.py
================
"""
Market regime detection module that can be imported by other files.
Provides functions to detect market regimes using HMM, Transformer, or a combined approach.
"""

import pandas as pd
import os
import numpy as np
import torch
import logging
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Import from our package structure
from dl_metalabeling.models.hmm import load_hmm_model
from dl_metalabeling.models.transformer import load_transformer_model
from dl_metalabeling.regime_evaluator import RegimeEvaluator
from dl_metalabeling.utils import calculate_performance_metrics

logger = logging.getLogger(__name__)

# Singleton instance of evaluator for reuse
_evaluator = None

def get_evaluator(reload=False):
    """
    Get the regime evaluator instance.
    
    Parameters:
    reload (bool): Whether to reload the models even if they're already loaded
    
    Returns:
    RegimeEvaluator: Regime evaluator instance
    """
    global _evaluator
    
    # Initialize evaluator if not exists or reload requested
    if _evaluator is None or reload:
        # Default data path and models directory
        data_path = os.path.join('data', 'cmma.csv')
        models_dir = 'models'
        
        # Initialize new evaluator
        _evaluator = RegimeEvaluator(data_path=data_path, models_dir=models_dir)
    
    return _evaluator

def detect_regime(data=None, method='combined'):
    """
    Detect the current market regime.
    
    Parameters:
    data (pd.DataFrame): Optional market data to use, or None to use latest from loaded data
    method (str): Which model to use - 'hmm', 'transformer', or 'combined'
    
    Returns:
    dict: Regime identification results
    """
    evaluator = get_evaluator()
    return evaluator.get_current_regime(data=data, method=method)

def get_hmm_regimes(data=None):
    """
    Get regime predictions using only the HMM model.
    
    Parameters:
    data (pd.DataFrame): Optional market data to use, or None to use loaded data
    
    Returns:
    dict: HMM regime identification results
    """
    evaluator = get_evaluator()
    
    if data is None:
        # Get regime history from loaded data
        regime_history = evaluator.get_regime_history(method='hmm')
        # Return most recent regime
        if not regime_history.empty:
            latest = regime_history.iloc[-1]
            return {
                'regime_id': latest['hmm_regime'],
                'regime_label': latest['hmm_regime_label']
            }
    else:
        # Get regime for provided data
        result = evaluator.get_current_regime(data=data, method='hmm')
        if result and 'hmm' in result:
            return result['hmm']
    
    return None

def get_transformer_regimes(data=None):
    """
    Get regime predictions using only the Transformer model.
    
    Parameters:
    data (pd.DataFrame): Optional market data to use, or None to use loaded data
    
    Returns:
    dict: Transformer regime identification results
    """
    evaluator = get_evaluator()
    
    if data is None:
        # Get regime history from loaded data
        regime_history = evaluator.get_regime_history(method='transformer')
        # Return most recent regime
        if not regime_history.empty:
            latest = regime_history.iloc[-1]
            return {
                'regime_id': latest['transformer_regime'],
                'regime_label': latest['transformer_regime_label']
            }
    else:
        # Get regime for provided data
        result = evaluator.get_current_regime(data=data, method='transformer')
        if result and 'transformer' in result:
            return result['transformer']
    
    return None

def get_combined_regimes(data=None):
    """
    Get regime predictions using a combination of both models.
    
    Parameters:
    data (pd.DataFrame): Optional market data to use, or None to use loaded data
    
    Returns:
    dict: Combined regime identification results
    """
    evaluator = get_evaluator()
    
    if data is None:
        # Get regime history from loaded data
        regime_history = evaluator.get_regime_history(method='combined')
        # Return most recent regime
        if not regime_history.empty:
            latest = regime_history.iloc[-1]
            return {
                'regime': latest['combined_regime'],
                'confidence': latest['confidence']
            }
    else:
        # Get regime for provided data
        result = evaluator.get_current_regime(data=data, method='combined')
        if result and 'combined' in result:
            return result['combined']
    
    return None

def evaluate_models():
    """
    Evaluate and compare the performance of HMM and Transformer models.
    
    Returns:
    pd.DataFrame: Dataframe with evaluation results
    """
    evaluator = get_evaluator()
    return evaluator.evaluate()

def evaluate_by_regime(data, regime_column, regime_labels):
    """
    Evaluate strategy performance metrics by market regime.
    
    Parameters:
    data (pd.DataFrame): DataFrame with regime predictions and returns
    regime_column (str): Column name containing regime labels
    regime_labels (list): List of regime label names
    
    Returns:
    dict: Dictionary of performance metrics by regime
    """
    # Initialize results dictionary
    results = {}
    
    # Ensure we have the necessary columns
    required_cols = ['returns', 'strategy_returns', 'meta_strategy_returns', regime_column]
    if not all(col in data.columns for col in required_cols):
        missing = [col for col in required_cols if col not in data.columns]
        raise ValueError(f"Missing required columns: {missing}")
    
    # Process each regime
    for i, regime_label in enumerate(regime_labels):
        # Get data for this regime
        regime_data = data[data[regime_column] == i].copy()
        
        if len(regime_data) == 0:
            print(f"Warning: No data points for regime {regime_label}")
            continue
        
        # Calculate performance metrics for base strategy
        base_metrics = calculate_performance_metrics(regime_data['strategy_returns'])
        
        # Calculate performance metrics for metalabeled strategy
        meta_metrics = calculate_performance_metrics(regime_data['meta_strategy_returns'])
        
        # Store results
        results[regime_label] = {
            'base': base_metrics,
            'meta': meta_metrics
        }
        
        print(f"Regime: {regime_label} ({len(regime_data)} days)")
        print(f"  Base Strategy - Sharpe: {base_metrics['sharpe_ratio']:.2f}, Annual Return: {base_metrics['annual_return']:.2%}")
        print(f"  Meta Strategy - Sharpe: {meta_metrics['sharpe_ratio']:.2f}, Annual Return: {meta_metrics['annual_return']:.2%}")
    
    return results

class HMMRegimeDetector:
    """
    Hidden Markov Model for market regime detection.
    """
    
    def __init__(self, n_regimes=3, features=None, cov_type='full'):
        """
        Initialize HMM regime detector.
        
        Args:
            n_regimes (int): Number of regimes to detect
            features (list): Features to use for regime detection
            cov_type (str): Covariance type for HMM
        """
        self.n_regimes = n_regimes
        self.features = features or ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126']
        self.cov_type = cov_type
        
        # Placeholder for HMM model
        self.model = None
        self.scaler = StandardScaler()
        
        logger.info(f"Initialized HMM regime detector with {n_regimes} regimes")
    
    def train(self, data):
        """
        Train the HMM model.
        
        Args:
            data (pandas.DataFrame): Training data
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training HMM model with {len(data)} samples")
        
        # Extract features
        features_data = data[self.features].copy()
        
        # Scale features
        scaled_data = self.scaler.fit_transform(features_data.fillna(0))
        
        # Placeholder for HMM training
        # In a real implementation, this would train an HMM model
        # using the hmmlearn library
        
        logger.info("HMM model training completed")
        return self
    
    def detect_regimes(self, data):
        """
        Detect regimes in data.
        
        Args:
            data (pandas.DataFrame): Data to detect regimes in
            
        Returns:
            pandas.DataFrame: Data with regime labels
        """
        logger.info(f"Detecting regimes in {len(data)} samples")
        
        # Make a copy of the data
        data_with_regimes = data.copy()
        
        # Extract features
        features_data = data[self.features].copy()
        
        # Scale features
        scaled_data = self.scaler.transform(features_data.fillna(0))
        
        # Placeholder for regime prediction
        # In a real implementation, this would use the trained HMM model
        # to predict the most likely regime for each data point
        
        # Generate random regimes for placeholder
        regimes = np.random.randint(0, self.n_regimes, len(data))
        
        # Add regime column to data
        data_with_regimes['hmm_regime'] = regimes
        
        logger.info("Regime detection completed")
        return data_with_regimes
    
    def save(self, filepath):
        """
        Save the model to file.
        
        Args:
            filepath (str): Path to save the model
        """
        model_data = {
            'n_regimes': self.n_regimes,
            'features': self.features,
            'cov_type': self.cov_type,
            'scaler': self.scaler,
            'model': self.model
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"HMM model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath):
        """
        Load the model from file.
        
        Args:
            filepath (str): Path to load the model from
            
        Returns:
            HMMRegimeDetector: Loaded model
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        detector = cls(
            n_regimes=model_data['n_regimes'],
            features=model_data['features'],
            cov_type=model_data['cov_type']
        )
        
        detector.scaler = model_data['scaler']
        detector.model = model_data['model']
        
        logger.info(f"HMM model loaded from {filepath}")
        return detector


class TransformerRegimeDetector:
    """
    Transformer-based model for market regime detection.
    """
    
    def __init__(self, n_regimes=3, features=None, embedding_dim=64, num_heads=4, num_layers=2):
        """
        Initialize Transformer regime detector.
        
        Args:
            n_regimes (int): Number of regimes to detect
            features (list): Features to use for regime detection
            embedding_dim (int): Dimension of embeddings
            num_heads (int): Number of attention heads
            num_layers (int): Number of transformer layers
        """
        self.n_regimes = n_regimes
        self.features = features or ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126']
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        
        # Placeholder for transformer model components
        self.encoder = None
        self.scaler = StandardScaler()
        self.kmeans = KMeans(n_clusters=n_regimes)
        
        logger.info(f"Initialized Transformer regime detector with {n_regimes} regimes")
    
    def train(self, data, batch_size=32, epochs=100, learning_rate=0.001):
        """
        Train the Transformer model.
        
        Args:
            data (pandas.DataFrame): Training data
            batch_size (int): Batch size for training
            epochs (int): Number of training epochs
            learning_rate (float): Learning rate for optimizer
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training Transformer model with {len(data)} samples for {epochs} epochs")
        
        # Extract features
        features_data = data[self.features].copy()
        
        # Scale features
        scaled_data = self.scaler.fit_transform(features_data.fillna(0))
        
        # Placeholder for Transformer training
        # In a real implementation, this would train a Transformer model
        # using PyTorch, followed by K-means clustering on the embeddings
        
        logger.info("Transformer model training completed")
        return self
    
    def detect_regimes(self, data):
        """
        Detect regimes in data.
        
        Args:
            data (pandas.DataFrame): Data to detect regimes in
            
        Returns:
            pandas.DataFrame: Data with regime labels
        """
        logger.info(f"Detecting regimes in {len(data)} samples")
        
        # Make a copy of the data
        data_with_regimes = data.copy()
        
        # Extract features
        features_data = data[self.features].copy()
        
        # Scale features
        scaled_data = self.scaler.transform(features_data.fillna(0))
        
        # Placeholder for regime prediction
        # In a real implementation, this would use the trained Transformer model
        # to extract embeddings, then use the K-means model to cluster them
        
        # Generate random regimes for placeholder
        regimes = np.random.randint(0, self.n_regimes, len(data))
        
        # Add regime column to data
        data_with_regimes['transformer_regime'] = regimes
        
        logger.info("Regime detection completed")
        return data_with_regimes
    
    def save(self, filepath):
        """
        Save the model to file.
        
        Args:
            filepath (str): Path to save the model
        """
        # Save model components separately
        base_path = filepath.rstrip('.pkl')
        
        # Save encoder weights
        if self.encoder is not None:
            torch.save(self.encoder.state_dict(), f"{base_path}_encoder.pth")
        
        # Save scaler
        with open(f"{base_path}_scaler.pkl", 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # Save K-means model
        with open(f"{base_path}_kmeans.pkl", 'wb') as f:
            pickle.dump(self.kmeans, f)
        
        # Save config
        config = {
            'n_regimes': self.n_regimes,
            'features': self.features,
            'embedding_dim': self.embedding_dim,
            'num_heads': self.num_heads,
            'num_layers': self.num_layers
        }
        
        with open(f"{base_path}_config.pkl", 'wb') as f:
            pickle.dump(config, f)
        
        logger.info(f"Transformer model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath):
        """
        Load the model from file.
        
        Args:
            filepath (str): Path to load the model from
            
        Returns:
            TransformerRegimeDetector: Loaded model
        """
        # Load model components separately
        base_path = filepath.rstrip('.pkl')
        
        # Load config
        with open(f"{base_path}_config.pkl", 'rb') as f:
            config = pickle.load(f)
        
        detector = cls(
            n_regimes=config['n_regimes'],
            features=config['features'],
            embedding_dim=config['embedding_dim'],
            num_heads=config['num_heads'],
            num_layers=config['num_layers']
        )
        
        # Load scaler
        with open(f"{base_path}_scaler.pkl", 'rb') as f:
            detector.scaler = pickle.load(f)
        
        # Load K-means model
        with open(f"{base_path}_kmeans.pkl", 'rb') as f:
            detector.kmeans = pickle.load(f)
        
        # Load encoder weights (placeholder)
        # In a real implementation, this would load the encoder weights
        
        logger.info(f"Transformer model loaded from {filepath}")
        return detector


def detect_regime(data, hmm_detector=None, transformer_detector=None):
    """
    Helper function to detect regimes using one or more detectors.
    
    Args:
        data (pandas.DataFrame): Data to detect regimes in
        hmm_detector (HMMRegimeDetector): HMM detector
        transformer_detector (TransformerRegimeDetector): Transformer detector
        
    Returns:
        pandas.DataFrame: Data with detected regimes
    """
    data_with_regimes = data.copy()
    
    if hmm_detector is not None:
        data_with_regimes = hmm_detector.detect_regimes(data_with_regimes)
    
    if transformer_detector is not None:
        data_with_regimes = transformer_detector.detect_regimes(data_with_regimes)
    
    return data_with_regimes

# Usage example
if __name__ == "__main__":
    # Get current market regime
    current_regime = detect_regime()
    print("Current Market Regime:")
    for model, regime_info in current_regime.items():
        print(f"  {model.capitalize()}: {regime_info}")

================
File: metalabeling.py
================
"""
Metalabeling module for trading strategy enhancement.
Provides functionality to train, evaluate, and apply metalabeling models.
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve
import pickle
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

logger = logging.getLogger(__name__)

class SimpleNN(nn.Module):
    """
    Simple neural network for binary classification.
    """
    def __init__(self, input_dim, hidden_dim=64, dropout_rate=0.2):
        super(SimpleNN, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.model(x)

class MetaLabeler:
    """
    MetaLabeler class for improving trading strategies using machine learning.
    """
    
    def __init__(self, features=None, target='meta_label', model_type='random_forest', labeling_method='triple_barrier'):
        """
        Initialize meta-labeler.
        
        Args:
            features (list): Feature columns for prediction
            target (str): Target column name
            model_type (str): Type of model to use ('random_forest', 'gradient_boosting', 'neural_network', etc.)
            labeling_method (str): Method for labeling ('triple_barrier', 'fixed_horizon', etc.)
        """
        self.features = features or ['returns', 'volatility', 'rsi', 'macd', 'hmm_regime', 'transformer_regime']
        self.target = target
        self.model_type = model_type
        self.labeling_method = labeling_method
        self.scaler = StandardScaler()
        
        # Initialize model
        if model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=5,
                random_state=42
            )
        elif model_type == 'gradient_boosting':
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=3,
                learning_rate=0.1,
                random_state=42
            )
        elif model_type == 'neural_network':
            # Neural network will be initialized during training
            # when we know the input dimension
            self.model = None
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.nn_params = {
                'hidden_dim': 64,
                'dropout_rate': 0.2,
                'learning_rate': 0.001,
                'batch_size': 64,
                'epochs': 50
            }
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
        
        logger.info(f"Initialized MetaLabeler with {model_type} model and {labeling_method} labeling")
    
    def create_labels(self, data, signal_col='position', returns_col='returns', 
                     upper_barrier=0.05, lower_barrier=-0.03, max_holding_period=10):
        """
        Create meta-labels for trading signals.
        
        Args:
            data (pandas.DataFrame): Data with trading signals
            signal_col (str): Column name for trading signals
            returns_col (str): Column name for returns
            upper_barrier (float): Upper profit barrier (as a decimal)
            lower_barrier (float): Lower stop-loss barrier (as a decimal)
            max_holding_period (int): Maximum holding period in days
            
        Returns:
            pandas.DataFrame: Data with labels
        """
        logger.info(f"Creating labels using {self.labeling_method} method")
        
        # Make a copy of the data
        labeled_data = data.copy()
        
        # Debug: Log data shape and columns
        logger.info(f"Data shape: {labeled_data.shape}, Columns: {labeled_data.columns.tolist()}")
        
        # Debug: Check if signal_col exists in the data
        if signal_col not in labeled_data.columns:
            logger.warning(f"Signal column '{signal_col}' not found in data. Available columns: {labeled_data.columns.tolist()}")
            # If signal column doesn't exist, create a default one with zeros
            labeled_data[signal_col] = 0
            logger.warning(f"Created default {signal_col} column with zeros")
        else:
            # Log signal column statistics
            signal_count = (labeled_data[signal_col] != 0).sum()
            logger.info(f"Found {signal_count} non-zero signals in {signal_col} column")
        
        # Debug: Check if returns_col exists in the data
        if returns_col not in labeled_data.columns:
            logger.warning(f"Returns column '{returns_col}' not found in data. Available columns: {labeled_data.columns.tolist()}")
            # If returns column doesn't exist, create a default one with zeros
            labeled_data[returns_col] = 0
            logger.warning(f"Created default {returns_col} column with zeros")
        else:
            # Log returns column statistics
            logger.info(f"Returns column statistics: min={labeled_data[returns_col].min():.4f}, max={labeled_data[returns_col].max():.4f}, mean={labeled_data[returns_col].mean():.4f}")
        
        if self.labeling_method == 'triple_barrier':
            # Implement triple barrier labeling
            logger.info(f"Using triple barrier method with upper={upper_barrier}, lower={lower_barrier}, max_period={max_holding_period}")
            
            # Initialize columns
            labeled_data['meta_label'] = np.nan
            labeled_data['meta_confidence'] = np.nan
            labeled_data['exit_type'] = np.nan
            
            # Get signal indices where we have a position
            signal_mask = labeled_data[signal_col] != 0
            if signal_mask.any():
                signal_indices = labeled_data[signal_mask].index
                logger.info(f"Found {len(signal_indices)} signals in data")
            else:
                logger.warning(f"No signals found in {signal_col} column. Using empty signal indices.")
                signal_indices = []
            
            for i, idx in enumerate(signal_indices):
                if i >= len(signal_indices) - max_holding_period:
                    # Skip if we don't have enough future data
                    continue
                
                # Get the signal direction
                signal = labeled_data.loc[idx, signal_col]
                
                if signal == 0:
                    continue
                
                # Calculate future cumulative returns
                future_returns = []
                for j in range(1, max_holding_period + 1):
                    if i + j < len(signal_indices):
                        future_idx = signal_indices[i + j]
                        # Calculate cumulative return from entry to this point
                        cum_return = labeled_data.loc[idx:future_idx, returns_col].cumsum().iloc[-1]
                        future_returns.append((future_idx, cum_return))
                
                if not future_returns:
                    continue
                
                # Adjust barriers based on signal direction
                if signal > 0:  # Long position
                    upper = upper_barrier
                    lower = lower_barrier
                else:  # Short position
                    upper = -lower_barrier
                    lower = -upper_barrier
                
                # Check if any barrier is hit
                exit_idx = None
                exit_return = None
                exit_type = None
                
                for future_idx, cum_return in future_returns:
                    # For long positions
                    if signal > 0:
                        if cum_return >= upper:
                            exit_idx = future_idx
                            exit_return = cum_return
                            exit_type = 'upper'
                            break
                        elif cum_return <= lower:
                            exit_idx = future_idx
                            exit_return = cum_return
                            exit_type = 'lower'
                            break
                    # For short positions
                    else:
                        if cum_return <= upper:  # Remember, for shorts upper is negative
                            exit_idx = future_idx
                            exit_return = cum_return
                            exit_type = 'upper'
                            break
                        elif cum_return >= lower:  # Remember, for shorts lower is positive
                            exit_idx = future_idx
                            exit_return = cum_return
                            exit_type = 'lower'
                            break
                
                # If no barrier hit, use the last point
                if exit_idx is None and future_returns:
                    exit_idx, exit_return = future_returns[-1]
                    exit_type = 'time'
                
                # Set the meta label
                if exit_type == 'upper' or (exit_type == 'time' and 
                                          (signal > 0 and exit_return > 0) or 
                                          (signal < 0 and exit_return < 0)):
                    labeled_data.loc[idx, 'meta_label'] = 1
                    # Confidence based on how close to the upper barrier
                    if exit_type == 'upper':
                        labeled_data.loc[idx, 'meta_confidence'] = 1.0
                    else:
                        # Scale confidence based on return relative to upper barrier
                        labeled_data.loc[idx, 'meta_confidence'] = min(abs(exit_return / upper), 1.0)
                else:
                    labeled_data.loc[idx, 'meta_label'] = 0
                    labeled_data.loc[idx, 'meta_confidence'] = 0.5
                
                labeled_data.loc[idx, 'exit_type'] = exit_type
            
            # Fill NaN values for meta_label with 0 (neutral)
            labeled_data['meta_label'] = labeled_data['meta_label'].fillna(0).astype(int)
            labeled_data['meta_confidence'] = labeled_data['meta_confidence'].fillna(0.5)
            
        elif self.labeling_method == 'fixed_horizon':
            # Implement fixed horizon labeling
            logger.info(f"Using fixed horizon method with horizon={max_holding_period}")
            
            # Initialize columns
            labeled_data['meta_label'] = 0
            labeled_data['meta_confidence'] = 0.5
            
            # Get signal indices where we have a position
            signal_indices = labeled_data[labeled_data[signal_col] != 0].index
            
            for i, idx in enumerate(signal_indices):
                if i >= len(signal_indices) - 1:
                    # Skip if we don't have enough future data
                    continue
                
                # Get the signal direction
                signal = labeled_data.loc[idx, signal_col]
                
                if signal == 0:
                    continue
                
                # Calculate future return over fixed horizon
                horizon_idx = min(i + max_holding_period, len(signal_indices) - 1)
                future_idx = signal_indices[horizon_idx]
                
                # Calculate cumulative return from entry to horizon
                cum_return = labeled_data.loc[idx:future_idx, returns_col].cumsum().iloc[-1]
                
                # Set the meta label based on signal direction and return
                if (signal > 0 and cum_return > 0) or (signal < 0 and cum_return < 0):
                    labeled_data.loc[idx, 'meta_label'] = 1
                    # Confidence based on magnitude of return
                    labeled_data.loc[idx, 'meta_confidence'] = min(abs(cum_return / upper_barrier), 1.0)
                else:
                    labeled_data.loc[idx, 'meta_label'] = 0
                    labeled_data.loc[idx, 'meta_confidence'] = 0.5
        
        else:
            raise ValueError(f"Unsupported labeling method: {self.labeling_method}")
        
        logger.info(f"Labels created with {labeled_data['meta_label'].sum()} positive samples out of {labeled_data['meta_label'].count()} total")
        return labeled_data
    
    def train(self, data, cv_folds=5, hyperparams=None):
        """
        Train the meta-labeling model.
        
        Args:
            data (pandas.DataFrame): Training data with signals
            cv_folds (int): Number of cross-validation folds
            hyperparams (dict): Hyperparameters for grid search
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training {self.model_type} model with {len(data)} samples")
        
        # Create labels if not already present
        if self.target not in data.columns:
            data = self.create_labels(data)
        
        # Extract features and target
        X = data[self.features].fillna(0)
        y = data[self.target].astype(int)
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        if self.model_type in ['random_forest', 'gradient_boosting']:
            # Perform grid search if hyperparameters are provided
            if hyperparams is not None:
                logger.info(f"Performing grid search with {cv_folds} folds")
                grid_search = GridSearchCV(
                    self.model,
                    hyperparams,
                    cv=cv_folds,
                    scoring='f1',
                    n_jobs=-1
                )
                grid_search.fit(X_scaled, y)
                self.model = grid_search.best_estimator_
                logger.info(f"Best hyperparameters: {grid_search.best_params_}")
            else:
                # Train with default hyperparameters
                self.model.fit(X_scaled, y)
                
            # Log feature importance
            if hasattr(self.model, 'feature_importances_'):
                importances = sorted(zip(self.features, self.model.feature_importances_), 
                                    key=lambda x: x[1], reverse=True)
                logger.info(f"Feature importance: {importances}")
                
        elif self.model_type == 'neural_network':
            # Initialize neural network
            input_dim = X_scaled.shape[1]
            self.model = SimpleNN(
                input_dim=input_dim,
                hidden_dim=self.nn_params['hidden_dim'],
                dropout_rate=self.nn_params['dropout_rate']
            ).to(self.device)
            
            # Prepare data for PyTorch
            X_tensor = torch.FloatTensor(X_scaled).to(self.device)
            y_tensor = torch.FloatTensor(y.values.reshape(-1, 1)).to(self.device)
            
            # Create DataLoader
            dataset = TensorDataset(X_tensor, y_tensor)
            dataloader = DataLoader(
                dataset, 
                batch_size=self.nn_params['batch_size'],
                shuffle=True
            )
            
            # Define loss function and optimizer
            criterion = nn.BCELoss()
            optimizer = optim.Adam(self.model.parameters(), lr=self.nn_params['learning_rate'])
            
            # Training loop
            logger.info(f"Training neural network for {self.nn_params['epochs']} epochs")
            self.model.train()
            for epoch in range(self.nn_params['epochs']):
                running_loss = 0.0
                for inputs, labels in dataloader:
                    # Zero the parameter gradients
                    optimizer.zero_grad()
                    
                    # Forward pass
                    outputs = self.model(inputs)
                    loss = criterion(outputs, labels)
                    
                    # Backward pass and optimize
                    loss.backward()
                    optimizer.step()
                    
                    running_loss += loss.item() * inputs.size(0)
                
                epoch_loss = running_loss / len(dataset)
                if (epoch + 1) % 10 == 0:
                    logger.info(f"Epoch {epoch+1}/{self.nn_params['epochs']}, Loss: {epoch_loss:.4f}")
            
            # Evaluate on training data
            self.model.eval()
            with torch.no_grad():
                y_pred = self.model(X_tensor).cpu().numpy()
                y_pred_binary = (y_pred > 0.5).astype(int)
                accuracy = accuracy_score(y, y_pred_binary)
                logger.info(f"Training accuracy: {accuracy:.4f}")
        
        logger.info("Model training completed")
        return self
    
    def predict(self, data):
        """
        Apply meta-labeling to data.
        
        Args:
            data (pandas.DataFrame): Data to predict on
            
        Returns:
            pandas.DataFrame: Data with meta-labels and confidence scores
        """
        logger.info(f"Predicting meta-labels for {len(data)} samples")
        
        # Make a copy of the data
        predicted_data = data.copy()
        
        # Extract features
        X = data[self.features].fillna(0)
        
        # Scale features
        X_scaled = self.scaler.transform(X)
        
        if self.model_type in ['random_forest', 'gradient_boosting']:
            # Predict labels and probabilities
            predicted_data['meta_label'] = self.model.predict(X_scaled)
            probas = self.model.predict_proba(X_scaled)
            
            # Store confidence (probability of positive class)
            if probas.shape[1] >= 2:
                predicted_data['meta_confidence'] = probas[:, 1]
            else:
                predicted_data['meta_confidence'] = probas[:, 0]
                
        elif self.model_type == 'neural_network':
            # Convert to tensor
            X_tensor = torch.FloatTensor(X_scaled).to(self.device)
            
            # Predict
            self.model.eval()
            with torch.no_grad():
                probas = self.model(X_tensor).cpu().numpy()
                
            # Store predictions and confidence
            predicted_data['meta_label'] = (probas > 0.5).astype(int)
            predicted_data['meta_confidence'] = probas.flatten()
        
        # Check if we have the necessary columns before calculating meta-strategy returns
        has_position = 'position' in predicted_data.columns
        has_returns = 'returns' in predicted_data.columns
        
        if has_position and has_returns:
            # Apply meta-filter to original signal
            predicted_data['meta_signal'] = predicted_data['position'] * predicted_data['meta_label']
            
            # Calculate meta-strategy returns
            predicted_data['meta_strategy_returns'] = predicted_data['meta_signal'].shift(1) * predicted_data['returns']
            predicted_data['meta_strategy_returns'] = predicted_data['meta_strategy_returns'].fillna(0)
        else:
            # Create default columns
            if not 'meta_signal' in predicted_data.columns:
                predicted_data['meta_signal'] = 0
            if not 'meta_strategy_returns' in predicted_data.columns:
                predicted_data['meta_strategy_returns'] = 0.0
            if has_position and not has_returns:
                logger.warning("Returns column not found, cannot calculate meta-strategy returns")
            elif has_returns and not has_position:
                logger.warning("Position column not found, cannot calculate meta-strategy returns")
            elif not has_position and not has_returns:
                logger.warning("Position and returns columns not found, cannot calculate meta-strategy returns")
        
        logger.info(f"Prediction completed with {predicted_data['meta_label'].sum()} positive samples")
        return predicted_data
    
    def save(self, filepath):
        """
        Save model to file.
        
        Args:
            filepath (str): Path to save the model
        """
        if self.model_type == 'neural_network':
            # Save neural network model separately
            model_path = filepath.replace('.pkl', '_nn.pt')
            torch.save(self.model.state_dict(), model_path)
            
            # Save other components
            model_data = {
                'features': self.features,
                'target': self.target,
                'model_type': self.model_type,
                'labeling_method': self.labeling_method,
                'scaler': self.scaler,
                'nn_params': self.nn_params,
                'input_dim': len(self.features)
            }
        else:
            model_data = {
                'features': self.features,
                'target': self.target,
                'model_type': self.model_type,
                'labeling_method': self.labeling_method,
                'model': self.model,
                'scaler': self.scaler
            }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"Meta-labeling model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath):
        """
        Load model from file.
        
        Args:
            filepath (str): Path to load the model from
            
        Returns:
            MetaLabeler: Loaded model
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        meta_labeler = cls(
            features=model_data['features'],
            target=model_data['target'],
            model_type=model_data['model_type'],
            labeling_method=model_data['labeling_method']
        )
        
        if model_data['model_type'] == 'neural_network':
            # Load neural network model
            model_path = filepath.replace('.pkl', '_nn.pt')
            meta_labeler.nn_params = model_data['nn_params']
            
            # Initialize the model
            meta_labeler.model = SimpleNN(
                input_dim=model_data['input_dim'],
                hidden_dim=model_data['nn_params']['hidden_dim'],
                dropout_rate=model_data['nn_params']['dropout_rate']
            ).to(meta_labeler.device)
            
            # Load weights
            meta_labeler.model.load_state_dict(torch.load(model_path, map_location=meta_labeler.device))
            meta_labeler.model.eval()
        else:
            meta_labeler.model = model_data['model']
        
        meta_labeler.scaler = model_data['scaler']
        
        logger.info(f"Meta-labeling model loaded from {filepath}")
        return meta_labeler


def calculate_performance_metrics(returns, risk_free_rate=0.0):
    """
    Calculate performance metrics for a returns series.
    
    Parameters:
    returns (pd.Series): Series of returns
    risk_free_rate (float): Annual risk-free rate
    
    Returns:
    dict: Dictionary of performance metrics
    """
    # Convert to numpy array if it's a pandas Series
    if isinstance(returns, pd.Series):
        returns_array = returns.values
    else:
        returns_array = returns
    
    # Basic metrics
    total_return = (1 + returns_array).prod() - 1
    
    # Annualized metrics (assuming daily returns)
    n_periods = len(returns_array)
    annual_factor = 252  # Trading days in a year
    annual_return = (1 + total_return) ** (annual_factor / n_periods) - 1
    
    # Volatility
    daily_vol = np.std(returns_array)
    annual_vol = daily_vol * np.sqrt(annual_factor)
    
    # Sharpe ratio
    excess_return = annual_return - risk_free_rate
    sharpe_ratio = excess_return / annual_vol if annual_vol > 0 else 0
    
    # Drawdown analysis
    wealth_index = (1 + returns_array).cumprod()
    previous_peaks = np.maximum.accumulate(wealth_index)
    drawdowns = (wealth_index - previous_peaks) / previous_peaks
    max_drawdown = drawdowns.min()
    
    # Win rate
    win_rate = np.sum(returns_array > 0) / len(returns_array)
    
    # Profit factor
    gross_profits = np.sum(returns_array[returns_array > 0])
    gross_losses = np.abs(np.sum(returns_array[returns_array < 0]))
    profit_factor = gross_profits / gross_losses if gross_losses > 0 else float('inf')
    
    # Create metrics dictionary
    metrics = {
        'total_return': total_return,
        'annual_return': annual_return,
        'daily_vol': daily_vol,
        'annual_vol': annual_vol,
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown': max_drawdown,
        'win_rate': win_rate,
        'profit_factor': profit_factor
    }
    
    return metrics


def train_and_apply_metalabeling(data, signal_column='position', returns_column='returns', test_size=0.3, random_state=42):
    """
    Train a metalabeling model and apply it to the data.
    
    Parameters:
    data (pd.DataFrame): DataFrame with strategy signals and returns
    signal_column (str): Column name with strategy signals
    returns_column (str): Column name with asset returns
    test_size (float): Proportion of data to use for testing
    random_state (int): Random state for reproducibility
    
    Returns:
    tuple: (data with metalabeling applied, trained metalabeler)
    """
    # Create a copy of the data
    df = data.copy()
    
    # Ensure we have the necessary columns
    if signal_column not in df.columns:
        raise ValueError(f"Signal column '{signal_column}' not found in data")
    if returns_column not in df.columns:
        raise ValueError(f"Returns column '{returns_column}' not found in data")
    
    # Calculate strategy returns if not already present
    if 'strategy_returns' not in df.columns:
        df['strategy_returns'] = df[signal_column].shift(1) * df[returns_column]
    
    # Split data into training and testing
    split_idx = int(len(df) * (1 - test_size))
    df_train = df.iloc[:split_idx].copy()
    df_test = df.iloc[split_idx:].copy()
    
    # Add is_train indicator
    df['is_train'] = False
    df.loc[df.index[:split_idx], 'is_train'] = True
    
    print(f"Training data: {len(df_train)} samples ({100*(1-test_size):.1f}%)")
    print(f"Testing data: {len(df_test)} samples ({100*test_size:.1f}%)")
    
    # Extract features (can customize this list)
    features = [col for col in df.columns if any(substring in col for substring in 
                ['volatility', 'momentum', 'rsi', 'macd', 'bbands', 'volume'])]
    
    if not features:
        # Default features if none found
        print("No predefined features found. Using basic price-derived features.")
        # Use whatever we have - this is just a fallback
        features = [col for col in df.columns if col not in [
            signal_column, returns_column, 'strategy_returns', 'meta_strategy_returns',
            'DateTime', 'Date', 'Time', 'is_train'
        ]]
        
        if len(features) < 2:
            raise ValueError("Not enough features available for metalabeling")
    
    print(f"Using features: {features}")
    
    # Initialize and train metalabeler
    metalabeler = MetaLabeler(features=features)
    metalabeler.train(df_train)
    
    # Apply to all data
    df = metalabeler.predict(df)
    
    # Print performance metrics
    train_metrics = calculate_performance_metrics(df[df['is_train']]['strategy_returns'])
    train_meta_metrics = calculate_performance_metrics(df[df['is_train']]['meta_strategy_returns'])
    
    test_metrics = calculate_performance_metrics(df[~df['is_train']]['strategy_returns'])
    test_meta_metrics = calculate_performance_metrics(df[~df['is_train']]['meta_strategy_returns'])
    
    print("\nTraining Set Performance:")
    print(f"Base Strategy - Sharpe: {train_metrics['sharpe_ratio']:.2f}, Return: {train_metrics['annual_return']:.2%}")
    print(f"Meta Strategy - Sharpe: {train_meta_metrics['sharpe_ratio']:.2f}, Return: {train_meta_metrics['annual_return']:.2%}")
    
    print("\nTest Set Performance:")
    print(f"Base Strategy - Sharpe: {test_metrics['sharpe_ratio']:.2f}, Return: {test_metrics['annual_return']:.2%}")
    print(f"Meta Strategy - Sharpe: {test_meta_metrics['sharpe_ratio']:.2f}, Return: {test_meta_metrics['annual_return']:.2%}")
    
    return df, metalabeler


def evaluate_by_regime(data, regime_column, regime_labels):
    """
    Evaluate strategy performance by regime.
    
    Parameters:
    data (pd.DataFrame): Data with strategy returns and regime labels
    regime_column (str): Name of the column containing regime labels
    regime_labels (dict): Dictionary mapping regime IDs to labels
    
    Returns:
    dict: Dictionary of performance metrics by regime
    """
    results = {}
    
    # Evaluate overall performance
    results['overall'] = {
        'base': calculate_performance_metrics(data['strategy_returns']),
        'meta': calculate_performance_metrics(data['meta_strategy_returns'])
    }
    
    # Evaluate by regime
    for regime_id, label in regime_labels.items():
        regime_mask = data[regime_column] == regime_id
        
        if regime_mask.sum() > 0:
            results[label] = {
                'base': calculate_performance_metrics(data.loc[regime_mask, 'strategy_returns']),
                'meta': calculate_performance_metrics(data.loc[regime_mask, 'meta_strategy_returns'])
            }
    
    return results


def train_regime_specific_metalabelers(data, regime_column, n_regimes=None, model_type='random_forest', 
                                 features=None, test_size=0.3, random_state=42):
    """
    Train regime-specific metalabelers.
    
    Args:
        data (pandas.DataFrame): Data with signals, returns, and regime labels
        regime_column (str): Column name containing regime labels
        n_regimes (int): Number of regimes (if None, inferred from data)
        model_type (str): Type of model to use
        features (list): Feature columns for prediction
        test_size (float): Proportion of data to use for testing
        random_state (int): Random seed for reproducibility
        
    Returns:
        dict: Dictionary of regime-specific metalabelers
    """
    logger.info(f"Training regime-specific metalabelers using {regime_column}")
    
    # Create a copy of the data
    df = data.copy()
    
    # Infer number of regimes if not provided
    if n_regimes is None:
        n_regimes = df[regime_column].nunique()
        logger.info(f"Inferred {n_regimes} regimes from data")
    
    # Split data into training and testing
    split_idx = int(len(df) * (1 - test_size))
    df_train = df.iloc[:split_idx].copy()
    df_test = df.iloc[split_idx:].copy()
    
    # Add is_train indicator
    df['is_train'] = False
    df.loc[df.index[:split_idx], 'is_train'] = True
    
    logger.info(f"Training data: {len(df_train)} samples ({100*(1-test_size):.1f}%)")
    logger.info(f"Testing data: {len(df_test)} samples ({100*test_size:.1f}%)")
    
    # Train regime-specific metalabelers
    regime_metalabelers = {}
    regime_performance = {}
    
    for regime in range(n_regimes):
        logger.info(f"Training metalabeler for regime {regime}")
        
        # Filter data for this regime
        regime_mask = df_train[regime_column] == regime
        regime_data = df_train[regime_mask]
        
        if len(regime_data) < 100:
            logger.warning(f"Not enough data for regime {regime} (only {len(regime_data)} samples). Skipping.")
            continue
        
        # Initialize and train metalabeler
        metalabeler = MetaLabeler(features=features, model_type=model_type)
        metalabeler.train(regime_data)
        
        # Store metalabeler
        regime_metalabelers[regime] = metalabeler
        
        # Apply to test data for this regime
        test_regime_mask = df_test[regime_column] == regime
        test_regime_data = df_test[test_regime_mask]
        
        if len(test_regime_data) > 0:
            # Apply metalabeler
            predicted_data = metalabeler.predict(test_regime_data)
            
            # Calculate performance
            if 'strategy_returns' in predicted_data.columns and 'meta_strategy_returns' in predicted_data.columns:
                base_metrics = calculate_performance_metrics(predicted_data['strategy_returns'])
                meta_metrics = calculate_performance_metrics(predicted_data['meta_strategy_returns'])
                
                regime_performance[regime] = {
                    'base': base_metrics,
                    'meta': meta_metrics,
                    'samples': len(predicted_data)
                }
                
                # Log performance
                logger.info(f"Regime {regime} - Base Sharpe: {base_metrics['sharpe_ratio']:.2f}, "
                           f"Meta Sharpe: {meta_metrics['sharpe_ratio']:.2f}, "
                           f"Improvement: {(meta_metrics['sharpe_ratio'] - base_metrics['sharpe_ratio']):.2f}")
    
    return regime_metalabelers, regime_performance

def apply_regime_metalabeling(data, regime_metalabelers, regime_column):
    """
    Apply regime-specific metalabeling to data.
    
    Args:
        data (pandas.DataFrame): Data with signals, returns, and regime labels
        regime_metalabelers (dict): Dictionary of regime-specific metalabelers
        regime_column (str): Column name containing regime labels
        
    Returns:
        pandas.DataFrame: Data with regime-specific metalabeling applied
    """
    logger.info(f"Applying regime-specific metalabeling using {len(regime_metalabelers)} metalabelers")
    
    try:
        # Create a copy of the data
        df = data.copy()
        
        # Debug: Log data shape and columns
        logger.info(f"Data shape: {df.shape}, Columns: {df.columns.tolist()}")
        
        # Initialize meta columns
        df['meta_label'] = 0
        df['meta_confidence'] = 0.0
        df['meta_signal'] = 0
        df['meta_strategy_returns'] = 0.0
        df['active_regime'] = -1
        
        # Check for required columns
        has_position = 'position' in df.columns
        has_returns = 'returns' in df.columns
        
        logger.info(f"Required columns check - has_position: {has_position}, has_returns: {has_returns}")
        
        if regime_column not in df.columns:
            logger.error(f"Regime column '{regime_column}' not found in data. Available columns: {df.columns.tolist()}")
            return df
        
        # Apply regime-specific metalabelers
        processed_samples = 0
        for regime, metalabeler in regime_metalabelers.items():
            # Filter data for this regime
            regime_mask = df[regime_column] == regime
            regime_count = regime_mask.sum()
            
            logger.info(f"Processing regime {regime} with {regime_count} samples")
            
            if regime_count == 0:
                logger.warning(f"No samples found for regime {regime}, skipping")
                continue
            
            # Get regime data
            regime_data = df[regime_mask].copy()
            
            # Apply metalabeler
            logger.info(f"Applying metalabeler to regime {regime} data")
            predicted_data = metalabeler.predict(regime_data)
            
            # Update meta columns
            logger.info(f"Updating meta columns for regime {regime}")
            df.loc[regime_mask, 'meta_label'] = predicted_data['meta_label']
            df.loc[regime_mask, 'meta_confidence'] = predicted_data['meta_confidence']
            df.loc[regime_mask, 'active_regime'] = regime
            
            # Calculate meta signal if possible
            if has_position and has_returns:
                logger.info(f"Calculating meta signal for regime {regime}")
                df.loc[regime_mask, 'meta_signal'] = df.loc[regime_mask, 'position'] * predicted_data['meta_label']
            
            processed_samples += regime_count
        
        # Calculate meta-strategy returns if possible
        if has_returns:
            logger.info("Calculating meta-strategy returns")
            df['meta_strategy_returns'] = df['meta_signal'].shift(1) * df['returns']
            df['meta_strategy_returns'] = df['meta_strategy_returns'].fillna(0)
        
        positive_samples = df['meta_label'].sum()
        logger.info(f"Applied regime-specific metalabeling to {processed_samples} samples with {positive_samples} positive samples")
        return df
    
    except Exception as e:
        logger.error(f"Error in apply_regime_metalabeling: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # Return original data with meta columns to avoid errors downstream
        logger.warning("Error occurred, returning original data with default meta columns")
        df = data.copy()
        df['meta_label'] = 0
        df['meta_confidence'] = 0.0
        df['meta_signal'] = 0
        df['meta_strategy_returns'] = 0.0
        df['active_regime'] = -1
        
        return df

# Usage example
if __name__ == "__main__":
    import os
    import sys
    import logging
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    # Load CMMA data
    data_path = os.path.join('data', 'cmma.csv')
    df = pd.read_csv(data_path, parse_dates=['DateTime'], index_col='DateTime')
    
    # Calculate returns if not present
    if 'returns' not in df.columns:
        df['returns'] = df['gbclose'].pct_change()
    
    # Calculate strategy returns from the existing position column
    if 'strategy_returns' not in df.columns:
        df['strategy_returns'] = df['position'].shift(1) * df['returns']
    
    # Add some features for metalabeling
    df['volatility_5d'] = df['returns'].rolling(window=5).std()
    df['volatility_20d'] = df['returns'].rolling(window=20).std()
    df['momentum_5d'] = df['returns'].rolling(window=5).mean()
    df['momentum_20d'] = df['returns'].rolling(window=20).mean()
    
    # Drop NaN values
    df = df.dropna()
    
    # Example 1: Basic metalabeling
    print("\n=== Example 1: Basic Metalabeling ===")
    features = ['volatility_5', 'volatility_20', 'volatility', 'rsi']
    
    # Train and apply metalabeling
    metalabeler = MetaLabeler(features=features)
    df_with_meta = metalabeler.create_labels(df)
    metalabeler.train(df_with_meta)
    df_with_meta = metalabeler.predict(df)
    
    # Print performance
    base_performance = calculate_performance_metrics(df_with_meta['strategy_returns'])
    meta_performance = calculate_performance_metrics(df_with_meta['meta_strategy_returns'])
    
    print("\nBase CMMA Strategy Performance:")
    for metric, value in base_performance.items():
        if metric in ['annual_return', 'win_rate', 'max_drawdown']:
            print(f"{metric}: {value:.2%}")
        else:
            print(f"{metric}: {value:.4f}")
            
    print("\nMetalabeled CMMA Strategy Performance:")
    for metric, value in meta_performance.items():
        if metric in ['annual_return', 'win_rate', 'max_drawdown']:
            print(f"{metric}: {value:.2%}")
        else:
            print(f"{metric}: {value:.4f}")
    
    # Example 2: Neural Network Metalabeling
    print("\n=== Example 2: Neural Network Metalabeling ===")
    
    # Train and apply neural network metalabeling
    nn_metalabeler = MetaLabeler(features=features, model_type='neural_network')
    df_with_nn_meta = nn_metalabeler.create_labels(df)
    nn_metalabeler.train(df_with_nn_meta)
    df_with_nn_meta = nn_metalabeler.predict(df)
    
    # Print performance
    nn_meta_performance = calculate_performance_metrics(df_with_nn_meta['meta_strategy_returns'])
    
    print("\nNeural Network Metalabeled Strategy Performance:")
    for metric, value in nn_meta_performance.items():
        if metric in ['annual_return', 'win_rate', 'max_drawdown']:
            print(f"{metric}: {value:.2%}")
        else:
            print(f"{metric}: {value:.4f}")
    
    # Example 3: Regime-based Metalabeling
    print("\n=== Example 3: Regime-based Metalabeling ===")
    
    # Generate synthetic regimes for demonstration
    from sklearn.cluster import KMeans
    
    # Create features for regime detection
    regime_features = df[['volatility_20d', 'momentum_20d']].copy()
    
    # Fit KMeans for regime detection
    kmeans = KMeans(n_clusters=3, random_state=42)
    df['regime'] = kmeans.fit_predict(regime_features)
    
    # Print regime distribution
    regime_counts = df['regime'].value_counts()
    print("\nRegime Distribution:")
    for regime, count in regime_counts.items():
        print(f"Regime {regime}: {count} samples ({count/len(df):.1%})")
    
    # Train regime-specific metalabelers
    regime_metalabelers, regime_performance = train_regime_specific_metalabelers(
        data=df,
        regime_column='regime',
        features=features,
        model_type='random_forest'
    )
    
    # Apply regime-specific metalabeling
    df_with_regime_meta = apply_regime_metalabeling(
        data=df,
        regime_metalabelers=regime_metalabelers,
        regime_column='regime'
    )
    
    # Print performance by regime
    print("\nPerformance by Regime:")
    for regime, perf in regime_performance.items():
        print(f"\nRegime {regime} ({perf['samples']} samples):")
        print(f"Base - Sharpe: {perf['base']['sharpe_ratio']:.2f}, Return: {perf['base']['annual_return']:.2%}")
        print(f"Meta - Sharpe: {perf['meta']['sharpe_ratio']:.2f}, Return: {perf['meta']['annual_return']:.2%}")
    
    # Overall performance
    regime_meta_performance = calculate_performance_metrics(df_with_regime_meta['meta_strategy_returns'])
    
    print("\nOverall Regime-based Metalabeled Strategy Performance:")
    for metric, value in regime_meta_performance.items():
        if metric in ['annual_return', 'win_rate', 'max_drawdown']:
            print(f"{metric}: {value:.2%}")
        else:
            print(f"{metric}: {value:.4f}")
    
    # Save models
    os.makedirs('models', exist_ok=True)
    metalabeler.save('models/basic_metalabeler.pkl')
    nn_metalabeler.save('models/nn_metalabeler.pkl')
    
    # Save regime metalabelers
    for regime, model in regime_metalabelers.items():
        model.save(f'models/regime_{regime}_metalabeler.pkl')
    
    print("\nModels saved to 'models/' directory")

================
File: metalabeling/__init__.py
================
"""
Meta-labeling module for improving trading strategies.

This module provides tools for enhancing trading strategies by filtering signals 
using supervised machine learning models.

Components:
- Labelers: Tools for creating labels from trading signals and returns data
- Models: Machine learning models for predicting profitable trades
- Strategy: Integration of labelers and models into cohesive strategies

The module supports both standard meta-labeling and regime-specific approaches.
"""

# Import labelers
from .labelers import (
    BaseLabeler,
    TripleBarrierLabeler,
    FixedHorizonLabeler,
    get_labeler
)

# Import models
from .models import (
    BaseMetaLabeler,
    RandomForestMetaLabeler,
    NeuralNetworkMetaLabeler,
    get_meta_labeler
)

# Import strategies
from .strategy import (
    MetaLabeling,
    RegimeMetaLabeling
)

# Define exported components
__all__ = [
    # Labelers
    'BaseLabeler',
    'TripleBarrierLabeler',
    'FixedHorizonLabeler',
    'get_labeler',
    
    # Models
    'BaseMetaLabeler',
    'RandomForestMetaLabeler',
    'NeuralNetworkMetaLabeler',
    'get_meta_labeler',
    
    # Strategies
    'MetaLabeling',
    'RegimeMetaLabeling'
]

================
File: metalabeling/labelers.py
================
"""
Labelers module for meta-labeling strategies.

This module provides classes for different labeling approaches including:
- Triple-barrier labeling
- Fixed-horizon labeling
- Custom labeling strategies

Each labeler class implements a consistent interface for creating labels
from raw price and signal data.
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Union, Optional, Any, Tuple

logger = logging.getLogger(__name__)

class BaseLabeler:
    """Base class for all labelers defining common interface."""
    
    def __init__(self, 
                 features: List[str] = None,
                 target: str = 'label',
                 **kwargs):
        """
        Initialize the base labeler.
        
        Args:
            features: List of feature column names to use
            target: Name of the target column
            kwargs: Additional parameters
        """
        self.features = features or []
        self.target = target
        self.params = kwargs
        self._is_trained = False
    
    def create_labels(self, 
                      data: pd.DataFrame, 
                      signal_col: str = 'position',
                      returns_col: str = 'returns',
                      **kwargs) -> pd.DataFrame:
        """
        Create labels for meta-labeling.
        
        Args:
            data: DataFrame containing price and signal data
            signal_col: Column name for the trading signals
            returns_col: Column name for returns data
            kwargs: Additional parameters specific to labeling method
            
        Returns:
            DataFrame with additional label column
        """
        raise NotImplementedError("Subclasses must implement create_labels method")
    
    def _validate_data(self, data: pd.DataFrame, required_cols: List[str]) -> bool:
        """
        Validate that the DataFrame contains required columns.
        
        Args:
            data: DataFrame to validate
            required_cols: List of column names that must be present
            
        Returns:
            True if valid, False otherwise
        """
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            logger.warning(f"Missing required columns: {missing_cols}")
            return False
        return True


class TripleBarrierLabeler(BaseLabeler):
    """
    Triple barrier labeling method for meta-labeling.
    
    This method uses upper and lower price barriers along with a time barrier
    to create labels.
    """
    
    def __init__(self, 
                 price_col: str = 'close',
                 signal_col: str = 'signal',
                 returns_col: str = 'returns',
                 upper_pct: float = 0.02,
                 lower_pct: float = 0.02,
                 max_periods: int = 10,
                 **kwargs):
        """
        Initialize triple barrier labeler.
        
        Args:
            price_col: Name of the price column
            signal_col: Name of the signal column
            returns_col: Name of the returns column
            upper_pct: Upper barrier percentage
            lower_pct: Lower barrier percentage
            max_periods: Maximum number of periods to look ahead
            kwargs: Additional parameters
        """
        super().__init__(**kwargs)
        self.price_col = price_col
        self.signal_col = signal_col
        self.returns_col = returns_col
        self.upper_pct = upper_pct
        self.lower_pct = lower_pct
        self.max_periods = max_periods
    
    def create_labels(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        Create labels using triple barrier method.
        
        Args:
            data: DataFrame containing price, signal, and returns columns
            **kwargs: Additional parameters (price_col and signal_col will be ignored)
            
        Returns:
            DataFrame with labels added
        """
        # Remove price_col and signal_col from kwargs to avoid conflict
        if 'price_col' in kwargs:
            logger.info(f"Ignoring price_col in kwargs, using self.price_col={self.price_col} instead")
            kwargs.pop('price_col')
        if 'signal_col' in kwargs:
            logger.info(f"Ignoring signal_col in kwargs, using self.signal_col={self.signal_col} instead")
            kwargs.pop('signal_col')
            
        # Create a copy of the dataframe
        labeled_data = data.copy()
        
        # Map column names to available columns if needed
        price_col = self.price_col
        signal_col = self.signal_col
        returns_col = self.returns_col
        
        # Check for alternative column names if needed
        column_mapping = {
            'close': ['gbclose', 'close', 'adjclose', 'price'],
            'signal': ['position', 'signal', 'cmma_signal', 'stg'],
        }
        
        # If price_col not in data, try alternative columns
        if price_col not in data.columns:
            for alt_col in column_mapping.get(price_col, []):
                if alt_col in data.columns:
                    logger.info(f"Using {alt_col} as alternative for {price_col}")
                    price_col = alt_col
                    break
        
        # If signal_col not in data, try alternative columns
        if signal_col not in data.columns:
            for alt_col in column_mapping.get(signal_col, []):
                if alt_col in data.columns:
                    logger.info(f"Using {alt_col} as alternative for {signal_col}")
                    signal_col = alt_col
                    break
        
        # Check if required columns are present
        required_cols = [price_col, signal_col, returns_col]
        missing_cols = [col for col in required_cols if col not in data.columns]
        
        if missing_cols:
            logger.error(f"Missing required columns: {missing_cols}")
            # Create default columns for meta_label and meta_confidence
            labeled_data['meta_label'] = 0
            labeled_data['meta_confidence'] = 0.5
            return labeled_data
        
        # Initialize label and confidence columns
        labeled_data['meta_label'] = 0
        labeled_data['meta_confidence'] = 0.5
        
        # Get price, signal, and returns
        price = labeled_data[price_col].values
        signal = labeled_data[signal_col].values
        
        # Calculate barriers
        upper_barriers = price * (1 + self.upper_pct)
        lower_barriers = price * (1 - self.lower_pct)
        
        # Loop through each data point
        for i in range(len(labeled_data) - self.max_periods):
            # Skip if signal is zero (no position)
            if signal[i] == 0:
                continue
                
            # Get future prices
            future_prices = price[i+1:i+self.max_periods+1]
            
            if len(future_prices) == 0:
                continue
                
            # Determine if price hit upper, lower, or time barrier
            upper_hit = False
            lower_hit = False
            time_hit = False
            
            # Check price barriers
            for j, future_price in enumerate(future_prices):
                # Check if upper barrier hit
                if future_price >= upper_barriers[i]:
                    upper_hit = True
                    barrier_idx = j + 1
                    break
                    
                # Check if lower barrier hit
                if future_price <= lower_barriers[i]:
                    lower_hit = True
                    barrier_idx = j + 1
                    break
                    
                # Check if we've reached the time barrier
                if j == len(future_prices) - 1:
                    time_hit = True
                    barrier_idx = j + 1
            
            # Determine outcome based on barriers and signal
            if signal[i] > 0:  # Long signal
                if upper_hit:
                    labeled_data.loc[labeled_data.index[i], 'meta_label'] = 1
                    labeled_data.loc[labeled_data.index[i], 'meta_confidence'] = 0.8
                elif lower_hit:
                    labeled_data.loc[labeled_data.index[i], 'meta_label'] = 0
                    labeled_data.loc[labeled_data.index[i], 'meta_confidence'] = 0.8
                elif time_hit:
                    # Check if price at time barrier is higher than entry
                    if future_prices[-1] > price[i]:
                        labeled_data.loc[labeled_data.index[i], 'meta_label'] = 1
                        labeled_data.loc[labeled_data.index[i], 'meta_confidence'] = 0.6
                    else:
                        labeled_data.loc[labeled_data.index[i], 'meta_label'] = 0
                        labeled_data.loc[labeled_data.index[i], 'meta_confidence'] = 0.6
            else:  # Short signal
                if lower_hit:
                    labeled_data.loc[labeled_data.index[i], 'meta_label'] = 1
                    labeled_data.loc[labeled_data.index[i], 'meta_confidence'] = 0.8
                elif upper_hit:
                    labeled_data.loc[labeled_data.index[i], 'meta_label'] = 0
                    labeled_data.loc[labeled_data.index[i], 'meta_confidence'] = 0.8
                elif time_hit:
                    # Check if price at time barrier is lower than entry
                    if future_prices[-1] < price[i]:
                        labeled_data.loc[labeled_data.index[i], 'meta_label'] = 1
                        labeled_data.loc[labeled_data.index[i], 'meta_confidence'] = 0.6
                    else:
                        labeled_data.loc[labeled_data.index[i], 'meta_label'] = 0
                        labeled_data.loc[labeled_data.index[i], 'meta_confidence'] = 0.6
        
        # Log summary statistics
        num_labels = (labeled_data['meta_label'] != 0.5).sum()
        pos_labels = (labeled_data['meta_label'] == 1).sum()
        
        logger.info(f"Created {num_labels} labels using triple barrier method")
        if num_labels > 0:
            logger.info(f"Positive labels: {pos_labels} ({pos_labels/num_labels:.2%})")
        
        return labeled_data


class FixedHorizonLabeler(BaseLabeler):
    """
    Fixed horizon labeling method for meta-labeling.
    
    This method uses a fixed time horizon to evaluate the performance of a signal.
    """
    
    def create_labels(self, 
                      data: pd.DataFrame, 
                      signal_col: str = 'position',
                      returns_col: str = 'returns',
                      horizon: int = 5,
                      threshold: float = 0.0,
                      **kwargs) -> pd.DataFrame:
        """
        Create labels using fixed horizon method.
        
        Args:
            data: DataFrame containing signal and returns data
            signal_col: Column name for the trading signals
            returns_col: Column name for returns data
            horizon: Number of periods to evaluate returns
            threshold: Return threshold to consider a signal successful
            kwargs: Additional parameters
            
        Returns:
            DataFrame with additional label column
        """
        # Create a copy of the dataframe
        df = data.copy()
        
        # Map column names to available columns if needed
        orig_signal_col = signal_col
        orig_returns_col = returns_col
        
        # Check for alternative column names if needed
        column_mapping = {
            'position': ['position', 'signal', 'cmma_signal', 'stg'],
            'returns': ['returns', 'ret', 'return'],
        }
        
        # If signal_col not in data, try alternative columns
        if signal_col not in data.columns:
            for alt_col in column_mapping.get(signal_col, []):
                if alt_col in data.columns:
                    logger.info(f"Using {alt_col} as alternative for {signal_col}")
                    signal_col = alt_col
                    break
        
        # If returns_col not in data, try alternative columns
        if returns_col not in data.columns:
            for alt_col in column_mapping.get(returns_col, []):
                if alt_col in data.columns:
                    logger.info(f"Using {alt_col} as alternative for {returns_col}")
                    returns_col = alt_col
                    break
        
        required_cols = [signal_col, returns_col]
        missing_cols = [col for col in required_cols if col not in data.columns]
        
        if missing_cols:
            logger.error(f"Missing required columns: {missing_cols}")
            # Add meta_label and meta_confidence columns with default values
            df[self.target] = 0
            df['meta_label'] = 0
            df['meta_confidence'] = 0.5
            return df
        
        # Initialize label column
        df[self.target] = np.nan
        
        # Calculate future returns for each point
        future_returns = pd.DataFrame({
            f'ret_{h}': df[returns_col].shift(-h)
            for h in range(1, horizon + 1)
        })
        
        # Calculate cumulative returns over the horizon
        future_returns['cum_ret'] = future_returns.sum(axis=1)
        
        # Get signal indices (where signal is non-zero)
        signal_indices = df[df[signal_col] != 0].index
        
        for idx in signal_indices:
            # Skip if outside the range for future returns
            if idx not in future_returns.index:
                continue
                
            signal = df.loc[idx, signal_col]
            cum_ret = future_returns.loc[idx, 'cum_ret']
            
            # For long signals, check if return exceeds threshold
            if signal > 0:
                df.loc[idx, self.target] = 1 if cum_ret > threshold else 0
            # For short signals, check if negative return exceeds threshold
            else:
                df.loc[idx, self.target] = 1 if -cum_ret > threshold else 0
        
        # Fill NaN values with 0
        df[self.target] = df[self.target].fillna(0)
        
        # Add meta_label and meta_confidence columns
        df['meta_label'] = df[self.target]
        df['meta_confidence'] = df[self.target].apply(lambda x: 0.8 if x == 1 else 0.2)
        
        # Log labeling statistics
        labeled = df[~pd.isna(df[self.target])]
        logger.info(f"Created {len(labeled)} labels using fixed horizon method")
        logger.info(f"Positive labels: {(labeled[self.target] == 1).sum()} ({(labeled[self.target] == 1).mean():.2%})")
        
        return df


def get_labeler(method: str = 'triple_barrier', **kwargs) -> BaseLabeler:
    """
    Factory function to get the appropriate labeler.
    
    Args:
        method: Labeling method ('triple_barrier' or 'fixed_horizon')
        kwargs: Additional parameters for the labeler
        
    Returns:
        Labeler instance
    """
    if method == 'triple_barrier':
        return TripleBarrierLabeler(**kwargs)
    elif method == 'fixed_horizon':
        return FixedHorizonLabeler(**kwargs)
    else:
        raise ValueError(f"Unknown labeling method: {method}")

================
File: metalabeling/metalabeling.py
================
"""
Metalabeling module for trading strategy enhancement.
Provides functionality to train, evaluate, and apply metalabeling models.
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve
import pickle
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

logger = logging.getLogger(__name__)

class SimpleNN(nn.Module):
    """
    Simple neural network for binary classification.
    """
    def __init__(self, input_dim, hidden_dim=64, dropout_rate=0.2):
        super(SimpleNN, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.model(x)

class MetaLabeler:
    """
    MetaLabeler class for improving trading strategies using machine learning.
    """
    
    def __init__(self, features=None, target='meta_label', model_type='random_forest', labeling_method='triple_barrier'):
        """
        Initialize meta-labeler.
        
        Args:
            features (list): Feature columns for prediction
            target (str): Target column name
            model_type (str): Type of model to use ('random_forest', 'gradient_boosting', 'neural_network', etc.)
            labeling_method (str): Method for labeling ('triple_barrier', 'fixed_horizon', etc.)
        """
        self.features = features or ['returns', 'volatility', 'rsi', 'macd', 'hmm_regime', 'transformer_regime']
        self.target = target
        self.model_type = model_type
        self.labeling_method = labeling_method
        self.scaler = StandardScaler()
        
        # Initialize model
        if model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=5,
                random_state=42
            )
        elif model_type == 'gradient_boosting':
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=3,
                learning_rate=0.1,
                random_state=42
            )
        elif model_type == 'neural_network':
            # Neural network will be initialized during training
            # when we know the input dimension
            self.model = None
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.nn_params = {
                'hidden_dim': 64,
                'dropout_rate': 0.2,
                'learning_rate': 0.001,
                'batch_size': 64,
                'epochs': 50
            }
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
        
        logger.info(f"Initialized MetaLabeler with {model_type} model and {labeling_method} labeling")
    
    def create_labels(self, data, signal_col='position', returns_col='returns', 
                     upper_barrier=0.05, lower_barrier=-0.03, max_holding_period=10):
        """
        Create meta-labels for trading signals.
        
        Args:
            data (pandas.DataFrame): Data with trading signals
            signal_col (str): Column name for trading signals
            returns_col (str): Column name for returns
            upper_barrier (float): Upper profit barrier (as a decimal)
            lower_barrier (float): Lower stop-loss barrier (as a decimal)
            max_holding_period (int): Maximum holding period in days
            
        Returns:
            pandas.DataFrame: Data with labels
        """
        logger.info(f"Creating labels using {self.labeling_method} method")
        
        # Make a copy of the data
        labeled_data = data.copy()
        
        # Debug: Log data shape and columns
        logger.info(f"Data shape: {labeled_data.shape}, Columns: {labeled_data.columns.tolist()}")
        
        # Debug: Check if signal_col exists in the data
        if signal_col not in labeled_data.columns:
            logger.warning(f"Signal column '{signal_col}' not found in data. Available columns: {labeled_data.columns.tolist()}")
            # If signal column doesn't exist, create a default one with zeros
            labeled_data[signal_col] = 0
            logger.warning(f"Created default {signal_col} column with zeros")
        else:
            # Log signal column statistics
            signal_count = (labeled_data[signal_col] != 0).sum()
            logger.info(f"Found {signal_count} non-zero signals in {signal_col} column")
        
        # Debug: Check if returns_col exists in the data
        if returns_col not in labeled_data.columns:
            logger.warning(f"Returns column '{returns_col}' not found in data. Available columns: {labeled_data.columns.tolist()}")
            # If returns column doesn't exist, create a default one with zeros
            labeled_data[returns_col] = 0
            logger.warning(f"Created default {returns_col} column with zeros")
        else:
            # Log returns column statistics
            logger.info(f"Returns column statistics: min={labeled_data[returns_col].min():.4f}, max={labeled_data[returns_col].max():.4f}, mean={labeled_data[returns_col].mean():.4f}")
        
        if self.labeling_method == 'triple_barrier':
            # Implement triple barrier labeling
            logger.info(f"Using triple barrier method with upper={upper_barrier}, lower={lower_barrier}, max_period={max_holding_period}")
            
            # Initialize columns
            labeled_data['meta_label'] = np.nan
            labeled_data['meta_confidence'] = np.nan
            labeled_data['exit_type'] = np.nan
            
            # Get signal indices where we have a position
            signal_mask = labeled_data[signal_col] != 0
            if signal_mask.any():
                signal_indices = labeled_data[signal_mask].index
                logger.info(f"Found {len(signal_indices)} signals in data")
            else:
                logger.warning(f"No signals found in {signal_col} column. Using empty signal indices.")
                signal_indices = []
            
            for i, idx in enumerate(signal_indices):
                if i >= len(signal_indices) - max_holding_period:
                    # Skip if we don't have enough future data
                    continue
                
                # Get the signal direction
                signal = labeled_data.loc[idx, signal_col]
                
                if signal == 0:
                    continue
                
                # Calculate future cumulative returns
                future_returns = []
                for j in range(1, max_holding_period + 1):
                    if i + j < len(signal_indices):
                        future_idx = signal_indices[i + j]
                        # Calculate cumulative return from entry to this point
                        cum_return = labeled_data.loc[idx:future_idx, returns_col].cumsum().iloc[-1]
                        future_returns.append((future_idx, cum_return))
                
                if not future_returns:
                    continue
                
                # Adjust barriers based on signal direction
                if signal > 0:  # Long position
                    upper = upper_barrier
                    lower = lower_barrier
                else:  # Short position
                    upper = -lower_barrier
                    lower = -upper_barrier
                
                # Check if any barrier is hit
                exit_idx = None
                exit_return = None
                exit_type = None
                
                for future_idx, cum_return in future_returns:
                    # For long positions
                    if signal > 0:
                        if cum_return >= upper:
                            exit_idx = future_idx
                            exit_return = cum_return
                            exit_type = 'upper'
                            break
                        elif cum_return <= lower:
                            exit_idx = future_idx
                            exit_return = cum_return
                            exit_type = 'lower'
                            break
                    # For short positions
                    else:
                        if cum_return <= upper:  # Remember, for shorts upper is negative
                            exit_idx = future_idx
                            exit_return = cum_return
                            exit_type = 'upper'
                            break
                        elif cum_return >= lower:  # Remember, for shorts lower is positive
                            exit_idx = future_idx
                            exit_return = cum_return
                            exit_type = 'lower'
                            break
                
                # If no barrier hit, use the last point
                if exit_idx is None and future_returns:
                    exit_idx, exit_return = future_returns[-1]
                    exit_type = 'time'
                
                # Set the meta label
                if exit_type == 'upper' or (exit_type == 'time' and 
                                          (signal > 0 and exit_return > 0) or 
                                          (signal < 0 and exit_return < 0)):
                    labeled_data.loc[idx, 'meta_label'] = 1
                    # Confidence based on how close to the upper barrier
                    if exit_type == 'upper':
                        labeled_data.loc[idx, 'meta_confidence'] = 1.0
                    else:
                        # Scale confidence based on return relative to upper barrier
                        labeled_data.loc[idx, 'meta_confidence'] = min(abs(exit_return / upper), 1.0)
                else:
                    labeled_data.loc[idx, 'meta_label'] = 0
                    labeled_data.loc[idx, 'meta_confidence'] = 0.5
                
                labeled_data.loc[idx, 'exit_type'] = exit_type
            
            # Fill NaN values for meta_label with 0 (neutral)
            labeled_data['meta_label'] = labeled_data['meta_label'].fillna(0).astype(int)
            labeled_data['meta_confidence'] = labeled_data['meta_confidence'].fillna(0.5)
            
        elif self.labeling_method == 'fixed_horizon':
            # Implement fixed horizon labeling
            logger.info(f"Using fixed horizon method with horizon={max_holding_period}")
            
            # Initialize columns
            labeled_data['meta_label'] = 0
            labeled_data['meta_confidence'] = 0.5
            
            # Get signal indices where we have a position
            signal_indices = labeled_data[labeled_data[signal_col] != 0].index
            
            for i, idx in enumerate(signal_indices):
                if i >= len(signal_indices) - 1:
                    # Skip if we don't have enough future data
                    continue
                
                # Get the signal direction
                signal = labeled_data.loc[idx, signal_col]
                
                if signal == 0:
                    continue
                
                # Calculate future return over fixed horizon
                horizon_idx = min(i + max_holding_period, len(signal_indices) - 1)
                future_idx = signal_indices[horizon_idx]
                
                # Calculate cumulative return from entry to horizon
                cum_return = labeled_data.loc[idx:future_idx, returns_col].cumsum().iloc[-1]
                
                # Set the meta label based on signal direction and return
                if (signal > 0 and cum_return > 0) or (signal < 0 and cum_return < 0):
                    labeled_data.loc[idx, 'meta_label'] = 1
                    # Confidence based on magnitude of return
                    labeled_data.loc[idx, 'meta_confidence'] = min(abs(cum_return / upper_barrier), 1.0)
                else:
                    labeled_data.loc[idx, 'meta_label'] = 0
                    labeled_data.loc[idx, 'meta_confidence'] = 0.5
        
        else:
            raise ValueError(f"Unsupported labeling method: {self.labeling_method}")
        
        logger.info(f"Labels created with {labeled_data['meta_label'].sum()} positive samples out of {labeled_data['meta_label'].count()} total")
        return labeled_data
    
    def train(self, data, cv_folds=5, hyperparams=None):
        """
        Train the meta-labeling model.
        
        Args:
            data (pandas.DataFrame): Training data with signals
            cv_folds (int): Number of cross-validation folds
            hyperparams (dict): Hyperparameters for grid search
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training {self.model_type} model with {len(data)} samples")
        
        # Create labels if not already present
        if self.target not in data.columns:
            data = self.create_labels(data)
        
        # Extract features and target
        X = data[self.features].fillna(0)
        y = data[self.target].astype(int)
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        if self.model_type in ['random_forest', 'gradient_boosting']:
            # Perform grid search if hyperparameters are provided
            if hyperparams is not None:
                logger.info(f"Performing grid search with {cv_folds} folds")
                grid_search = GridSearchCV(
                    self.model,
                    hyperparams,
                    cv=cv_folds,
                    scoring='f1',
                    n_jobs=-1
                )
                grid_search.fit(X_scaled, y)
                self.model = grid_search.best_estimator_
                logger.info(f"Best hyperparameters: {grid_search.best_params_}")
            else:
                # Train with default hyperparameters
                self.model.fit(X_scaled, y)
                
            # Log feature importance
            if hasattr(self.model, 'feature_importances_'):
                importances = sorted(zip(self.features, self.model.feature_importances_), 
                                    key=lambda x: x[1], reverse=True)
                logger.info(f"Feature importance: {importances}")
                
        elif self.model_type == 'neural_network':
            # Initialize neural network
            input_dim = X_scaled.shape[1]
            self.model = SimpleNN(
                input_dim=input_dim,
                hidden_dim=self.nn_params['hidden_dim'],
                dropout_rate=self.nn_params['dropout_rate']
            ).to(self.device)
            
            # Prepare data for PyTorch
            X_tensor = torch.FloatTensor(X_scaled).to(self.device)
            y_tensor = torch.FloatTensor(y.values.reshape(-1, 1)).to(self.device)
            
            # Create DataLoader
            dataset = TensorDataset(X_tensor, y_tensor)
            dataloader = DataLoader(
                dataset, 
                batch_size=self.nn_params['batch_size'],
                shuffle=True
            )
            
            # Define loss function and optimizer
            criterion = nn.BCELoss()
            optimizer = optim.Adam(self.model.parameters(), lr=self.nn_params['learning_rate'])
            
            # Training loop
            logger.info(f"Training neural network for {self.nn_params['epochs']} epochs")
            self.model.train()
            for epoch in range(self.nn_params['epochs']):
                running_loss = 0.0
                for inputs, labels in dataloader:
                    # Zero the parameter gradients
                    optimizer.zero_grad()
                    
                    # Forward pass
                    outputs = self.model(inputs)
                    loss = criterion(outputs, labels)
                    
                    # Backward pass and optimize
                    loss.backward()
                    optimizer.step()
                    
                    running_loss += loss.item() * inputs.size(0)
                
                epoch_loss = running_loss / len(dataset)
                if (epoch + 1) % 10 == 0:
                    logger.info(f"Epoch {epoch+1}/{self.nn_params['epochs']}, Loss: {epoch_loss:.4f}")
            
            # Evaluate on training data
            self.model.eval()
            with torch.no_grad():
                y_pred = self.model(X_tensor).cpu().numpy()
                y_pred_binary = (y_pred > 0.5).astype(int)
                accuracy = accuracy_score(y, y_pred_binary)
                logger.info(f"Training accuracy: {accuracy:.4f}")
        
        logger.info("Model training completed")
        return self
    
    def predict(self, data):
        """
        Apply meta-labeling to data.
        
        Args:
            data (pandas.DataFrame): Data to predict on
            
        Returns:
            pandas.DataFrame: Data with meta-labels and confidence scores
        """
        logger.info(f"Predicting meta-labels for {len(data)} samples")
        
        # Make a copy of the data
        predicted_data = data.copy()
        
        # Extract features
        X = data[self.features].fillna(0)
        
        # Scale features
        X_scaled = self.scaler.transform(X)
        
        if self.model_type in ['random_forest', 'gradient_boosting']:
            # Predict labels and probabilities
            predicted_data['meta_label'] = self.model.predict(X_scaled)
            probas = self.model.predict_proba(X_scaled)
            
            # Store confidence (probability of positive class)
            if probas.shape[1] >= 2:
                predicted_data['meta_confidence'] = probas[:, 1]
            else:
                predicted_data['meta_confidence'] = probas[:, 0]
                
        elif self.model_type == 'neural_network':
            # Convert to tensor
            X_tensor = torch.FloatTensor(X_scaled).to(self.device)
            
            # Predict
            self.model.eval()
            with torch.no_grad():
                probas = self.model(X_tensor).cpu().numpy()
                
            # Store predictions and confidence
            predicted_data['meta_label'] = (probas > 0.5).astype(int)
            predicted_data['meta_confidence'] = probas.flatten()
        
        # Check if we have the necessary columns before calculating meta-strategy returns
        has_position = 'position' in predicted_data.columns
        has_returns = 'returns' in predicted_data.columns
        
        if has_position and has_returns:
            # Apply meta-filter to original signal
            predicted_data['meta_signal'] = predicted_data['position'] * predicted_data['meta_label']
            
            # Calculate meta-strategy returns
            predicted_data['meta_strategy_returns'] = predicted_data['meta_signal'].shift(1) * predicted_data['returns']
            predicted_data['meta_strategy_returns'] = predicted_data['meta_strategy_returns'].fillna(0)
        else:
            # Create default columns
            if not 'meta_signal' in predicted_data.columns:
                predicted_data['meta_signal'] = 0
            if not 'meta_strategy_returns' in predicted_data.columns:
                predicted_data['meta_strategy_returns'] = 0.0
            if has_position and not has_returns:
                logger.warning("Returns column not found, cannot calculate meta-strategy returns")
            elif has_returns and not has_position:
                logger.warning("Position column not found, cannot calculate meta-strategy returns")
            elif not has_position and not has_returns:
                logger.warning("Position and returns columns not found, cannot calculate meta-strategy returns")
        
        logger.info(f"Prediction completed with {predicted_data['meta_label'].sum()} positive samples")
        return predicted_data
    
    def save(self, filepath):
        """
        Save model to file.
        
        Args:
            filepath (str): Path to save the model
        """
        if self.model_type == 'neural_network':
            # Save neural network model separately
            model_path = filepath.replace('.pkl', '_nn.pt')
            torch.save(self.model.state_dict(), model_path)
            
            # Save other components
            model_data = {
                'features': self.features,
                'target': self.target,
                'model_type': self.model_type,
                'labeling_method': self.labeling_method,
                'scaler': self.scaler,
                'nn_params': self.nn_params,
                'input_dim': len(self.features)
            }
        else:
            model_data = {
                'features': self.features,
                'target': self.target,
                'model_type': self.model_type,
                'labeling_method': self.labeling_method,
                'model': self.model,
                'scaler': self.scaler
            }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"Meta-labeling model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath):
        """
        Load model from file.
        
        Args:
            filepath (str): Path to load the model from
            
        Returns:
            MetaLabeler: Loaded model
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        meta_labeler = cls(
            features=model_data['features'],
            target=model_data['target'],
            model_type=model_data['model_type'],
            labeling_method=model_data['labeling_method']
        )
        
        if model_data['model_type'] == 'neural_network':
            # Load neural network model
            model_path = filepath.replace('.pkl', '_nn.pt')
            meta_labeler.nn_params = model_data['nn_params']
            
            # Initialize the model
            meta_labeler.model = SimpleNN(
                input_dim=model_data['input_dim'],
                hidden_dim=model_data['nn_params']['hidden_dim'],
                dropout_rate=model_data['nn_params']['dropout_rate']
            ).to(meta_labeler.device)
            
            # Load weights
            meta_labeler.model.load_state_dict(torch.load(model_path, map_location=meta_labeler.device))
            meta_labeler.model.eval()
        else:
            meta_labeler.model = model_data['model']
        
        meta_labeler.scaler = model_data['scaler']
        
        logger.info(f"Meta-labeling model loaded from {filepath}")
        return meta_labeler


def calculate_performance_metrics(returns, risk_free_rate=0.0):
    """
    Calculate performance metrics for a returns series.
    
    Parameters:
    returns (pd.Series): Series of returns
    risk_free_rate (float): Annual risk-free rate
    
    Returns:
    dict: Dictionary of performance metrics
    """
    # Convert to numpy array if it's a pandas Series
    if isinstance(returns, pd.Series):
        returns_array = returns.values
    else:
        returns_array = returns
    
    # Basic metrics
    total_return = (1 + returns_array).prod() - 1
    
    # Annualized metrics (assuming daily returns)
    n_periods = len(returns_array)
    annual_factor = 252  # Trading days in a year
    annual_return = (1 + total_return) ** (annual_factor / n_periods) - 1
    
    # Volatility
    daily_vol = np.std(returns_array)
    annual_vol = daily_vol * np.sqrt(annual_factor)
    
    # Sharpe ratio
    excess_return = annual_return - risk_free_rate
    sharpe_ratio = excess_return / annual_vol if annual_vol > 0 else 0
    
    # Drawdown analysis
    wealth_index = (1 + returns_array).cumprod()
    previous_peaks = np.maximum.accumulate(wealth_index)
    drawdowns = (wealth_index - previous_peaks) / previous_peaks
    max_drawdown = drawdowns.min()
    
    # Win rate
    win_rate = np.sum(returns_array > 0) / len(returns_array)
    
    # Profit factor
    gross_profits = np.sum(returns_array[returns_array > 0])
    gross_losses = np.abs(np.sum(returns_array[returns_array < 0]))
    profit_factor = gross_profits / gross_losses if gross_losses > 0 else float('inf')
    
    # Create metrics dictionary
    metrics = {
        'total_return': total_return,
        'annual_return': annual_return,
        'daily_vol': daily_vol,
        'annual_vol': annual_vol,
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown': max_drawdown,
        'win_rate': win_rate,
        'profit_factor': profit_factor
    }
    
    return metrics


def train_and_apply_metalabeling(data, signal_column='position', returns_column='returns', test_size=0.3, random_state=42):
    """
    Train a metalabeling model and apply it to the data.
    
    Parameters:
    data (pd.DataFrame): DataFrame with strategy signals and returns
    signal_column (str): Column name with strategy signals
    returns_column (str): Column name with asset returns
    test_size (float): Proportion of data to use for testing
    random_state (int): Random state for reproducibility
    
    Returns:
    tuple: (data with metalabeling applied, trained metalabeler)
    """
    # Create a copy of the data
    df = data.copy()
    
    # Ensure we have the necessary columns
    if signal_column not in df.columns:
        raise ValueError(f"Signal column '{signal_column}' not found in data")
    if returns_column not in df.columns:
        raise ValueError(f"Returns column '{returns_column}' not found in data")
    
    # Calculate strategy returns if not already present
    if 'strategy_returns' not in df.columns:
        df['strategy_returns'] = df[signal_column].shift(1) * df[returns_column]
    
    # Split data into training and testing
    split_idx = int(len(df) * (1 - test_size))
    df_train = df.iloc[:split_idx].copy()
    df_test = df.iloc[split_idx:].copy()
    
    # Add is_train indicator
    df['is_train'] = False
    df.loc[df.index[:split_idx], 'is_train'] = True
    
    print(f"Training data: {len(df_train)} samples ({100*(1-test_size):.1f}%)")
    print(f"Testing data: {len(df_test)} samples ({100*test_size:.1f}%)")
    
    # Extract features (can customize this list)
    features = [col for col in df.columns if any(substring in col for substring in 
                ['volatility', 'momentum', 'rsi', 'macd', 'bbands', 'volume'])]
    
    if not features:
        # Default features if none found
        print("No predefined features found. Using basic price-derived features.")
        # Use whatever we have - this is just a fallback
        features = [col for col in df.columns if col not in [
            signal_column, returns_column, 'strategy_returns', 'meta_strategy_returns',
            'DateTime', 'Date', 'Time', 'is_train'
        ]]
        
        if len(features) < 2:
            raise ValueError("Not enough features available for metalabeling")
    
    print(f"Using features: {features}")
    
    # Initialize and train metalabeler
    metalabeler = MetaLabeler(features=features)
    metalabeler.train(df_train)
    
    # Apply to all data
    df = metalabeler.predict(df)
    
    # Print performance metrics
    train_metrics = calculate_performance_metrics(df[df['is_train']]['strategy_returns'])
    train_meta_metrics = calculate_performance_metrics(df[df['is_train']]['meta_strategy_returns'])
    
    test_metrics = calculate_performance_metrics(df[~df['is_train']]['strategy_returns'])
    test_meta_metrics = calculate_performance_metrics(df[~df['is_train']]['meta_strategy_returns'])
    
    print("\nTraining Set Performance:")
    print(f"Base Strategy - Sharpe: {train_metrics['sharpe_ratio']:.2f}, Return: {train_metrics['annual_return']:.2%}")
    print(f"Meta Strategy - Sharpe: {train_meta_metrics['sharpe_ratio']:.2f}, Return: {train_meta_metrics['annual_return']:.2%}")
    
    print("\nTest Set Performance:")
    print(f"Base Strategy - Sharpe: {test_metrics['sharpe_ratio']:.2f}, Return: {test_metrics['annual_return']:.2%}")
    print(f"Meta Strategy - Sharpe: {test_meta_metrics['sharpe_ratio']:.2f}, Return: {test_meta_metrics['annual_return']:.2%}")
    
    return df, metalabeler


def evaluate_by_regime(data, regime_column, regime_labels):
    """
    Evaluate strategy performance by regime.
    
    Parameters:
    data (pd.DataFrame): Data with strategy returns and regime labels
    regime_column (str): Name of the column containing regime labels
    regime_labels (dict): Dictionary mapping regime IDs to labels
    
    Returns:
    dict: Dictionary of performance metrics by regime
    """
    results = {}
    
    # Evaluate overall performance
    results['overall'] = {
        'base': calculate_performance_metrics(data['strategy_returns']),
        'meta': calculate_performance_metrics(data['meta_strategy_returns'])
    }
    
    # Evaluate by regime
    for regime_id, label in regime_labels.items():
        regime_mask = data[regime_column] == regime_id
        
        if regime_mask.sum() > 0:
            results[label] = {
                'base': calculate_performance_metrics(data.loc[regime_mask, 'strategy_returns']),
                'meta': calculate_performance_metrics(data.loc[regime_mask, 'meta_strategy_returns'])
            }
    
    return results


def train_regime_specific_metalabelers(data, regime_column, n_regimes=None, model_type='random_forest', 
                                 features=None, test_size=0.3, random_state=42):
    """
    Train regime-specific metalabelers.
    
    Args:
        data (pandas.DataFrame): Data with signals, returns, and regime labels
        regime_column (str): Column name containing regime labels
        n_regimes (int): Number of regimes (if None, inferred from data)
        model_type (str): Type of model to use
        features (list): Feature columns for prediction
        test_size (float): Proportion of data to use for testing
        random_state (int): Random seed for reproducibility
        
    Returns:
        dict: Dictionary of regime-specific metalabelers
    """
    logger.info(f"Training regime-specific metalabelers using {regime_column}")
    
    # Create a copy of the data
    df = data.copy()
    
    # Infer number of regimes if not provided
    if n_regimes is None:
        n_regimes = df[regime_column].nunique()
        logger.info(f"Inferred {n_regimes} regimes from data")
    
    # Split data into training and testing
    split_idx = int(len(df) * (1 - test_size))
    df_train = df.iloc[:split_idx].copy()
    df_test = df.iloc[split_idx:].copy()
    
    # Add is_train indicator
    df['is_train'] = False
    df.loc[df.index[:split_idx], 'is_train'] = True
    
    logger.info(f"Training data: {len(df_train)} samples ({100*(1-test_size):.1f}%)")
    logger.info(f"Testing data: {len(df_test)} samples ({100*test_size:.1f}%)")
    
    # Train regime-specific metalabelers
    regime_metalabelers = {}
    regime_performance = {}
    
    for regime in range(n_regimes):
        logger.info(f"Training metalabeler for regime {regime}")
        
        # Filter data for this regime
        regime_mask = df_train[regime_column] == regime
        regime_data = df_train[regime_mask]
        
        if len(regime_data) < 100:
            logger.warning(f"Not enough data for regime {regime} (only {len(regime_data)} samples). Skipping.")
            continue
        
        # Initialize and train metalabeler
        metalabeler = MetaLabeler(features=features, model_type=model_type)
        metalabeler.train(regime_data)
        
        # Store metalabeler
        regime_metalabelers[regime] = metalabeler
        
        # Apply to test data for this regime
        test_regime_mask = df_test[regime_column] == regime
        test_regime_data = df_test[test_regime_mask]
        
        if len(test_regime_data) > 0:
            # Apply metalabeler
            predicted_data = metalabeler.predict(test_regime_data)
            
            # Calculate performance
            if 'strategy_returns' in predicted_data.columns and 'meta_strategy_returns' in predicted_data.columns:
                base_metrics = calculate_performance_metrics(predicted_data['strategy_returns'])
                meta_metrics = calculate_performance_metrics(predicted_data['meta_strategy_returns'])
                
                regime_performance[regime] = {
                    'base': base_metrics,
                    'meta': meta_metrics,
                    'samples': len(predicted_data)
                }
                
                # Log performance
                logger.info(f"Regime {regime} - Base Sharpe: {base_metrics['sharpe_ratio']:.2f}, "
                           f"Meta Sharpe: {meta_metrics['sharpe_ratio']:.2f}, "
                           f"Improvement: {(meta_metrics['sharpe_ratio'] - base_metrics['sharpe_ratio']):.2f}")
    
    return regime_metalabelers, regime_performance

def apply_regime_metalabeling(data, regime_metalabelers, regime_column):
    """
    Apply regime-specific metalabeling to data.
    
    Args:
        data (pandas.DataFrame): Data with signals, returns, and regime labels
        regime_metalabelers (dict): Dictionary of regime-specific metalabelers
        regime_column (str): Column name containing regime labels
        
    Returns:
        pandas.DataFrame: Data with regime-specific metalabeling applied
    """
    logger.info(f"Applying regime-specific metalabeling using {len(regime_metalabelers)} metalabelers")
    
    try:
        # Create a copy of the data
        df = data.copy()
        
        # Debug: Log data shape and columns
        logger.info(f"Data shape: {df.shape}, Columns: {df.columns.tolist()}")
        
        # Initialize meta columns
        df['meta_label'] = 0
        df['meta_confidence'] = 0.0
        df['meta_signal'] = 0
        df['meta_strategy_returns'] = 0.0
        df['active_regime'] = -1
        
        # Check for required columns
        has_position = 'position' in df.columns
        has_returns = 'returns' in df.columns
        
        logger.info(f"Required columns check - has_position: {has_position}, has_returns: {has_returns}")
        
        if regime_column not in df.columns:
            logger.error(f"Regime column '{regime_column}' not found in data. Available columns: {df.columns.tolist()}")
            return df
        
        # Apply regime-specific metalabelers
        processed_samples = 0
        for regime, metalabeler in regime_metalabelers.items():
            # Filter data for this regime
            regime_mask = df[regime_column] == regime
            regime_count = regime_mask.sum()
            
            logger.info(f"Processing regime {regime} with {regime_count} samples")
            
            if regime_count == 0:
                logger.warning(f"No samples found for regime {regime}, skipping")
                continue
            
            # Get regime data
            regime_data = df[regime_mask].copy()
            
            # Apply metalabeler
            logger.info(f"Applying metalabeler to regime {regime} data")
            predicted_data = metalabeler.predict(regime_data)
            
            # Update meta columns
            logger.info(f"Updating meta columns for regime {regime}")
            df.loc[regime_mask, 'meta_label'] = predicted_data['meta_label']
            df.loc[regime_mask, 'meta_confidence'] = predicted_data['meta_confidence']
            df.loc[regime_mask, 'active_regime'] = regime
            
            # Calculate meta signal if possible
            if has_position and has_returns:
                logger.info(f"Calculating meta signal for regime {regime}")
                df.loc[regime_mask, 'meta_signal'] = df.loc[regime_mask, 'position'] * predicted_data['meta_label']
            
            processed_samples += regime_count
        
        # Calculate meta-strategy returns if possible
        if has_returns:
            logger.info("Calculating meta-strategy returns")
            df['meta_strategy_returns'] = df['meta_signal'].shift(1) * df['returns']
            df['meta_strategy_returns'] = df['meta_strategy_returns'].fillna(0)
        
        positive_samples = df['meta_label'].sum()
        logger.info(f"Applied regime-specific metalabeling to {processed_samples} samples with {positive_samples} positive samples")
        return df
    
    except Exception as e:
        logger.error(f"Error in apply_regime_metalabeling: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # Return original data with meta columns to avoid errors downstream
        logger.warning("Error occurred, returning original data with default meta columns")
        df = data.copy()
        df['meta_label'] = 0
        df['meta_confidence'] = 0.0
        df['meta_signal'] = 0
        df['meta_strategy_returns'] = 0.0
        df['active_regime'] = -1
        
        return df

# Usage example
if __name__ == "__main__":
    import os
    import sys
    import logging
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    # Load CMMA data
    data_path = os.path.join('data', 'cmma.csv')
    df = pd.read_csv(data_path, parse_dates=['DateTime'], index_col='DateTime')
    
    # Calculate returns if not present
    if 'returns' not in df.columns:
        df['returns'] = df['gbclose'].pct_change()
    
    # Calculate strategy returns from the existing position column
    if 'strategy_returns' not in df.columns:
        df['strategy_returns'] = df['position'].shift(1) * df['returns']
    
    # Add some features for metalabeling
    df['volatility_5d'] = df['returns'].rolling(window=5).std()
    df['volatility_20d'] = df['returns'].rolling(window=20).std()
    df['momentum_5d'] = df['returns'].rolling(window=5).mean()
    df['momentum_20d'] = df['returns'].rolling(window=20).mean()
    
    # Drop NaN values
    df = df.dropna()
    
    # Example 1: Basic metalabeling
    print("\n=== Example 1: Basic Metalabeling ===")
    features = ['volatility_5', 'volatility_20', 'volatility', 'rsi']
    
    # Train and apply metalabeling
    metalabeler = MetaLabeler(features=features)
    df_with_meta = metalabeler.create_labels(df)
    metalabeler.train(df_with_meta)
    df_with_meta = metalabeler.predict(df)
    
    # Print performance
    base_performance = calculate_performance_metrics(df_with_meta['strategy_returns'])
    meta_performance = calculate_performance_metrics(df_with_meta['meta_strategy_returns'])
    
    print("\nBase CMMA Strategy Performance:")
    for metric, value in base_performance.items():
        if metric in ['annual_return', 'win_rate', 'max_drawdown']:
            print(f"{metric}: {value:.2%}")
        else:
            print(f"{metric}: {value:.4f}")
            
    print("\nMetalabeled CMMA Strategy Performance:")
    for metric, value in meta_performance.items():
        if metric in ['annual_return', 'win_rate', 'max_drawdown']:
            print(f"{metric}: {value:.2%}")
        else:
            print(f"{metric}: {value:.4f}")
    
    # Example 2: Neural Network Metalabeling
    print("\n=== Example 2: Neural Network Metalabeling ===")
    
    # Train and apply neural network metalabeling
    nn_metalabeler = MetaLabeler(features=features, model_type='neural_network')
    df_with_nn_meta = nn_metalabeler.create_labels(df)
    nn_metalabeler.train(df_with_nn_meta)
    df_with_nn_meta = nn_metalabeler.predict(df)
    
    # Print performance
    nn_meta_performance = calculate_performance_metrics(df_with_nn_meta['meta_strategy_returns'])
    
    print("\nNeural Network Metalabeled Strategy Performance:")
    for metric, value in nn_meta_performance.items():
        if metric in ['annual_return', 'win_rate', 'max_drawdown']:
            print(f"{metric}: {value:.2%}")
        else:
            print(f"{metric}: {value:.4f}")
    
    # Example 3: Regime-based Metalabeling
    print("\n=== Example 3: Regime-based Metalabeling ===")
    
    # Generate synthetic regimes for demonstration
    from sklearn.cluster import KMeans
    
    # Create features for regime detection
    regime_features = df[['volatility_20d', 'momentum_20d']].copy()
    
    # Fit KMeans for regime detection
    kmeans = KMeans(n_clusters=3, random_state=42)
    df['regime'] = kmeans.fit_predict(regime_features)
    
    # Print regime distribution
    regime_counts = df['regime'].value_counts()
    print("\nRegime Distribution:")
    for regime, count in regime_counts.items():
        print(f"Regime {regime}: {count} samples ({count/len(df):.1%})")
    
    # Train regime-specific metalabelers
    regime_metalabelers, regime_performance = train_regime_specific_metalabelers(
        data=df,
        regime_column='regime',
        features=features,
        model_type='random_forest'
    )
    
    # Apply regime-specific metalabeling
    df_with_regime_meta = apply_regime_metalabeling(
        data=df,
        regime_metalabelers=regime_metalabelers,
        regime_column='regime'
    )
    
    # Print performance by regime
    print("\nPerformance by Regime:")
    for regime, perf in regime_performance.items():
        print(f"\nRegime {regime} ({perf['samples']} samples):")
        print(f"Base - Sharpe: {perf['base']['sharpe_ratio']:.2f}, Return: {perf['base']['annual_return']:.2%}")
        print(f"Meta - Sharpe: {perf['meta']['sharpe_ratio']:.2f}, Return: {perf['meta']['annual_return']:.2%}")
    
    # Overall performance
    regime_meta_performance = calculate_performance_metrics(df_with_regime_meta['meta_strategy_returns'])
    
    print("\nOverall Regime-based Metalabeled Strategy Performance:")
    for metric, value in regime_meta_performance.items():
        if metric in ['annual_return', 'win_rate', 'max_drawdown']:
            print(f"{metric}: {value:.2%}")
        else:
            print(f"{metric}: {value:.4f}")
    
    # Save models
    os.makedirs('models', exist_ok=True)
    metalabeler.save('models/basic_metalabeler.pkl')
    nn_metalabeler.save('models/nn_metalabeler.pkl')
    
    # Save regime metalabelers
    for regime, model in regime_metalabelers.items():
        model.save(f'models/regime_{regime}_metalabeler.pkl')
    
    print("\nModels saved to 'models/' directory")

================
File: metalabeling/models.py
================
"""
Models module for meta-labeling strategies.

This module provides model-based meta-labeling strategies using various machine learning
algorithms. Each model class implements a consistent interface for training, prediction,
and evaluation.
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Union, Optional, Any, Tuple
import joblib
import os
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential, load_model, save_model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l1_l2
import tensorflow as tf

logger = logging.getLogger(__name__)

class BaseMetaLabeler:
    """Base class for all meta-labelers defining common interface."""
    
    def __init__(self, 
                 features: List[str] = None,
                 target: str = 'label',
                 **kwargs):
        """
        Initialize the base meta-labeler.
        
        Args:
            features: List of feature column names to use
            target: Name of the target column
            kwargs: Additional parameters
        """
        self.features = features or []
        self.target = target
        self.params = kwargs
        self.model = None
        self.scaler = StandardScaler()
        self._is_trained = False
    
    def fit(self, 
            data: pd.DataFrame, 
            test_size: float = 0.2,
            random_state: int = 42,
            **kwargs) -> Dict[str, float]:
        """
        Train the meta-labeling model.
        
        Args:
            data: DataFrame containing features and target
            test_size: Fraction of data to use for testing
            random_state: Random seed for reproducibility
            kwargs: Additional parameters for training
            
        Returns:
            Dict of performance metrics on test set
        """
        raise NotImplementedError("Subclasses must implement fit method")
    
    def predict(self, 
                data: pd.DataFrame, 
                signal_col: str = 'position',
                threshold: float = 0.5,
                **kwargs) -> pd.DataFrame:
        """
        Make predictions with the meta-labeling model.
        
        Args:
            data: DataFrame containing features
            signal_col: Column name for trading signals
            threshold: Probability threshold for classification
            kwargs: Additional parameters for prediction
            
        Returns:
            DataFrame with predictions
        """
        raise NotImplementedError("Subclasses must implement predict method")
    
    def evaluate(self, 
                 data: pd.DataFrame, 
                 **kwargs) -> Dict[str, float]:
        """
        Evaluate the meta-labeling model.
        
        Args:
            data: DataFrame containing features and target
            kwargs: Additional parameters for evaluation
            
        Returns:
            Dict of performance metrics
        """
        if not self._is_trained:
            logger.error("Model not trained, cannot evaluate")
            return {}
        
        if not set([self.target]).issubset(data.columns):
            logger.error(f"Target column {self.target} not in data")
            return {}
        
        X = self._prepare_features(data)
        y = data[self.target]
        
        try:
            y_pred = self._predict_proba(X)
            y_pred_binary = (y_pred >= 0.5).astype(int)
            
            metrics = {
                'accuracy': accuracy_score(y, y_pred_binary),
                'precision': precision_score(y, y_pred_binary, zero_division=0),
                'recall': recall_score(y, y_pred_binary, zero_division=0),
                'f1': f1_score(y, y_pred_binary, zero_division=0),
                'roc_auc': roc_auc_score(y, y_pred) if len(np.unique(y)) > 1 else np.nan
            }
            
            logger.info(f"Evaluation metrics: {metrics}")
            return metrics
        except Exception as e:
            logger.error(f"Error evaluating model: {str(e)}")
            return {}
    
    def _prepare_features(self, data: pd.DataFrame) -> np.ndarray:
        """
        Prepare features for model input.
        
        Args:
            data: DataFrame containing features
            
        Returns:
            Numpy array of prepared features
        """
        if not self.features:
            logger.warning("No features specified, using all numeric columns")
            X = data.select_dtypes(include=[np.number])
            X = X.drop(columns=[self.target], errors='ignore')
        else:
            missing_cols = [col for col in self.features if col not in data.columns]
            if missing_cols:
                logger.warning(f"Missing feature columns: {missing_cols}")
            
            available_cols = [col for col in self.features if col in data.columns]
            X = data[available_cols]
        
        # Handle NaN values
        X = X.fillna(X.mean())
        
        return X.values
    
    def _predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Get probability predictions from model.
        
        Args:
            X: Feature array
            
        Returns:
            Probability predictions
        """
        raise NotImplementedError("Subclasses must implement _predict_proba method")
    
    def save(self, path: str) -> None:
        """
        Save the model to disk.
        
        Args:
            path: Directory path to save model
        """
        if not self._is_trained:
            logger.warning("Model not trained, cannot save")
            return
        
        os.makedirs(path, exist_ok=True)
        
        try:
            # Save model-specific implementations in subclasses
            pass
        except Exception as e:
            logger.error(f"Error saving model: {str(e)}")
    
    def load(self, path: str) -> None:
        """
        Load the model from disk.
        
        Args:
            path: Directory path to load model from
        """
        try:
            # Load model-specific implementations in subclasses
            pass
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")


class RandomForestMetaLabeler(BaseMetaLabeler):
    """Random forest meta-labeler."""
    
    def __init__(self, n_estimators=100, max_depth=None, **kwargs):
        """
        Initialize random forest meta-labeler.
        
        Args:
            n_estimators: Number of trees in the forest
            max_depth: Maximum depth of the trees
            kwargs: Additional parameters
        """
        super().__init__(**kwargs)
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.model = None
        self.scaler = StandardScaler()
        self.trained = False
    
    def fit(self, 
            data: pd.DataFrame, 
            test_size: float = 0.2,
            random_state: int = 42,
            **kwargs) -> Dict[str, float]:
        """
        Fit the model to the data.
        
        Args:
            data: DataFrame containing features and target
            test_size: Fraction of data to use for testing
            random_state: Random seed for reproducibility
            kwargs: Additional parameters for training
            
        Returns:
            Dict of performance metrics on test set
        """
        try:
            if self.target not in data.columns:
                logger.error(f"Target column {self.target} not in data")
                return {}
            
            # Prepare features and target
            X = self._prepare_features(data)
            y = data[self.target].values
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )
            
            # Scale features
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            # Create and fit model
            self.model = RandomForestClassifier(
                n_estimators=self.n_estimators,
                max_depth=self.max_depth,
                random_state=random_state,
                **kwargs
            )
            self.model.fit(X_train_scaled, y_train)
            
            # Save feature names
            if isinstance(X, pd.DataFrame):
                self.feature_names = X.columns.tolist()
            
            # Set trained flag
            self.trained = True
            
            logger.info(f"Trained RandomForestMetaLabeler with {self.n_estimators} trees")
            
            # Evaluate on test set if we have test data
            y_pred_binary = self.model.predict(X_test_scaled)
            try:
                y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
            except IndexError:
                y_pred_proba = y_pred_binary.astype(float)
                
            metrics = {
                'accuracy': accuracy_score(y_test, y_pred_binary),
                'precision': precision_score(y_test, y_pred_binary, zero_division=0),
                'recall': recall_score(y_test, y_pred_binary, zero_division=0),
                'f1': f1_score(y_test, y_pred_binary, zero_division=0),
                'roc_auc': roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else np.nan
            }
            
            logger.info(f"Evaluation metrics: {metrics}")
            return metrics
        except Exception as e:
            logger.error(f"Error training RandomForestMetaLabeler: {e}")
            return {}
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        Public method to predict probabilities.
        
        Args:
            X: Features
            
        Returns:
            Predicted probabilities
        """
        return self._predict_proba(X)
    
    def _predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        Predict probabilities.
        
        Args:
            X: Features
            
        Returns:
            Predicted probabilities
        """
        if not self.trained:
            logger.error("Model not trained")
            return np.zeros(len(X))
        
        try:
            # Scale features
            X_scaled = self.scaler.transform(X)
            
            # Predict probabilities
            proba = self.model.predict_proba(X_scaled)
            
            # Get probability of positive class (index 1)
            try:
                return proba[:, 1]
            except IndexError:
                # If only one class is found, use raw predictions
                logger.warning("Only one class found in predict_proba output. Using raw predictions.")
                return self.model.predict(X_scaled).astype(float)
        except Exception as e:
            logger.error(f"Error predicting probabilities: {e}")
            return np.zeros(len(X))
    
    def save(self, path: str) -> None:
        """
        Save the Random Forest model to disk.
        
        Args:
            path: Directory path to save model
        """
        super().save(path)
        
        try:
            model_path = os.path.join(path, 'rf_meta_labeler.joblib')
            scaler_path = os.path.join(path, 'rf_scaler.joblib')
            
            joblib.dump(self.model, model_path)
            joblib.dump(self.scaler, scaler_path)
            
            # Save metadata
            metadata = {
                'features': self.feature_names,
                'target': self.target,
                'params': self.params,
                'n_estimators': self.n_estimators,
                'max_depth': self.max_depth
            }
            
            metadata_path = os.path.join(path, 'rf_metadata.joblib')
            joblib.dump(metadata, metadata_path)
            
            logger.info(f"Saved Random Forest meta-labeler to {path}")
        except Exception as e:
            logger.error(f"Error saving Random Forest model: {str(e)}")
    
    def load(self, path: str) -> None:
        """
        Load the Random Forest model from disk.
        
        Args:
            path: Directory path to load model from
        """
        try:
            model_path = os.path.join(path, 'rf_meta_labeler.joblib')
            scaler_path = os.path.join(path, 'rf_scaler.joblib')
            metadata_path = os.path.join(path, 'rf_metadata.joblib')
            
            if os.path.exists(model_path) and os.path.exists(scaler_path):
                self.model = joblib.load(model_path)
                self.scaler = joblib.load(scaler_path)
                self._is_trained = True
                
                # Load metadata if available
                if os.path.exists(metadata_path):
                    metadata = joblib.load(metadata_path)
                    self.feature_names = metadata.get('features', self.feature_names)
                    self.target = metadata.get('target', self.target)
                    self.params = metadata.get('params', self.params)
                    self.n_estimators = metadata.get('n_estimators', self.n_estimators)
                    self.max_depth = metadata.get('max_depth', self.max_depth)
                
                logger.info(f"Loaded Random Forest meta-labeler from {path}")
            else:
                logger.error(f"Model files not found in {path}")
        except Exception as e:
            logger.error(f"Error loading Random Forest model: {str(e)}")


class NeuralNetworkMetaLabeler(BaseMetaLabeler):
    """Neural Network based meta-labeler."""
    
    def __init__(self, 
                 features: List[str] = None,
                 target: str = 'label',
                 hidden_layers: List[int] = [64, 32],
                 dropout_rate: float = 0.2,
                 learning_rate: float = 0.001,
                 **kwargs):
        """
        Initialize Neural Network meta-labeler.
        
        Args:
            features: List of feature column names to use
            target: Name of the target column
            hidden_layers: List of hidden layer sizes
            dropout_rate: Dropout rate for regularization
            learning_rate: Learning rate for optimizer
            kwargs: Additional parameters for model
        """
        super().__init__(features=features, target=target, **kwargs)
        self.hidden_layers = hidden_layers
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.model = self._build_model()
    
    def _build_model(self) -> tf.keras.Model:
        """
        Build the neural network model.
        
        Returns:
            Compiled Keras model
        """
        model = Sequential()
        
        # Input layer
        model.add(Dense(self.hidden_layers[0], input_dim=len(self.features), activation='relu',
                       kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))
        model.add(BatchNormalization())
        model.add(Dropout(self.dropout_rate))
        
        # Hidden layers
        for units in self.hidden_layers[1:]:
            model.add(Dense(units, activation='relu',
                           kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))
            model.add(BatchNormalization())
            model.add(Dropout(self.dropout_rate))
        
        # Output layer
        model.add(Dense(1, activation='sigmoid'))
        
        # Compile model
        model.compile(
            loss='binary_crossentropy',
            optimizer=Adam(learning_rate=self.learning_rate),
            metrics=['accuracy']
        )
        
        return model
    
    def fit(self, 
            data: pd.DataFrame, 
            test_size: float = 0.2,
            random_state: int = 42,
            **kwargs) -> Dict[str, float]:
        """
        Train the Neural Network meta-labeling model.
        
        Args:
            data: DataFrame containing features and target
            test_size: Fraction of data to use for testing
            random_state: Random seed for reproducibility
            kwargs: Additional parameters for training
            
        Returns:
            Dict of performance metrics on test set
        """
        if self.target not in data.columns:
            logger.error(f"Target column {self.target} not in data")
            return {}
        
        # Prepare data
        X = self._prepare_features(data)
        y = data[self.target].values
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        # Fit scaler
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        # Train model
        logger.info("Training Neural Network meta-labeler...")
        batch_size = kwargs.get('batch_size', 32)
        epochs = kwargs.get('epochs', 100)
        
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=(X_test_scaled, y_test),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stopping],
            verbose=0
        )
        
        self._is_trained = True
        
        # Evaluate on test set
        y_pred = self.model.predict(X_test_scaled).flatten()
        y_pred_binary = (y_pred >= 0.5).astype(int)
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred_binary),
            'precision': precision_score(y_test, y_pred_binary, zero_division=0),
            'recall': recall_score(y_test, y_pred_binary, zero_division=0),
            'f1': f1_score(y_test, y_pred_binary, zero_division=0),
            'roc_auc': roc_auc_score(y_test, y_pred) if len(np.unique(y_test)) > 1 else np.nan
        }
        
        logger.info(f"Training completed with test metrics: {metrics}")
        return metrics
    
    def _predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Get probability predictions from Neural Network model.
        
        Args:
            X: Feature array
            
        Returns:
            Probability predictions
        """
        if not self._is_trained:
            logger.error("Model not trained, cannot predict probabilities")
            return np.zeros(X.shape[0])
        
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled).flatten()
    
    def save(self, path: str) -> None:
        """
        Save the Neural Network model to disk.
        
        Args:
            path: Directory path to save model
        """
        super().save(path)
        
        try:
            model_path = os.path.join(path, 'nn_meta_labeler')
            scaler_path = os.path.join(path, 'nn_scaler.joblib')
            
            self.model.save(model_path)
            joblib.dump(self.scaler, scaler_path)
            
            # Save metadata
            metadata = {
                'features': self.features,
                'target': self.target,
                'params': self.params,
                'hidden_layers': self.hidden_layers,
                'dropout_rate': self.dropout_rate,
                'learning_rate': self.learning_rate
            }
            
            metadata_path = os.path.join(path, 'nn_metadata.joblib')
            joblib.dump(metadata, metadata_path)
            
            logger.info(f"Saved Neural Network meta-labeler to {path}")
        except Exception as e:
            logger.error(f"Error saving Neural Network model: {str(e)}")
    
    def load(self, path: str) -> bool:
        """
        Load the Neural Network model from disk.
        
        Args:
            path: Directory path to load model from
            
        Returns:
            True if loading was successful, False otherwise
        """
        try:
            model_path = os.path.join(path, 'nn_meta_labeler')
            scaler_path = os.path.join(path, 'nn_scaler.joblib')
            metadata_path = os.path.join(path, 'nn_metadata.joblib')
            
            if os.path.exists(model_path) and os.path.exists(scaler_path):
                self.model = load_model(model_path)
                self.scaler = joblib.load(scaler_path)
                self._is_trained = True
                
                # Load metadata if available
                if os.path.exists(metadata_path):
                    metadata = joblib.load(metadata_path)
                    self.features = metadata.get('features', self.features)
                    self.target = metadata.get('target', self.target)
                    self.params = metadata.get('params', self.params)
                    self.hidden_layers = metadata.get('hidden_layers', self.hidden_layers)
                    self.dropout_rate = metadata.get('dropout_rate', self.dropout_rate)
                    self.learning_rate = metadata.get('learning_rate', self.learning_rate)
                
                logger.info(f"Loaded Neural Network meta-labeler from {path}")
                return True
            else:
                logger.error(f"Model files not found in {path}")
                return False
        except Exception as e:
            logger.error(f"Error loading Neural Network model: {str(e)}")
            return False


def get_meta_labeler(method: str = 'random_forest', **kwargs) -> BaseMetaLabeler:
    """
    Factory function to get the appropriate meta-labeler.
    
    Args:
        method: Meta-labeling method ('random_forest' or 'neural_network')
        kwargs: Additional parameters for the meta-labeler
        
    Returns:
        Meta-labeler instance
    """
    if method == 'random_forest':
        return RandomForestMetaLabeler(**kwargs)
    elif method == 'neural_network':
        return NeuralNetworkMetaLabeler(**kwargs)
    else:
        raise ValueError(f"Unknown meta-labeling method: {method}")

================
File: metalabeling/strategy.py
================
"""
Meta-labeling strategy integrating labelers and models.

This module provides the main integration of labelers and models for meta-labeling,
including regime-specific approaches and evaluation methods.
"""

import pandas as pd
import numpy as np
import logging
import os
from typing import Dict, List, Union, Optional, Any, Tuple
import joblib

from .labelers import get_labeler, BaseLabeler
from .models import get_meta_labeler, BaseMetaLabeler

logger = logging.getLogger(__name__)

class MetaLabeling:
    """
    Meta-labeling strategy integrating labelers and models.
    
    This class combines labeling approaches and prediction models to
    enhance trading strategies by filtering out likely unprofitable trades.
    """
    
    def __init__(self, 
                 features: List[str] = None,
                 labeling_method: str = 'triple_barrier',
                 model_method: str = 'random_forest',
                 target: str = 'label',
                 **kwargs):
        """
        Initialize meta-labeling strategy.
        
        Args:
            features: List of feature column names to use
            labeling_method: Method for creating labels ('triple_barrier' or 'fixed_horizon')
            model_method: Type of model to use ('random_forest' or 'neural_network')
            target: Target column name for predictions
            kwargs: Additional parameters for labeler and model
        """
        self.features = features or []
        self.labeling_method = labeling_method
        self.model_method = model_method
        self.target = target
        self.params = kwargs
        
        # Initialize labeler and model
        self.labeler = get_labeler(method=labeling_method, target=target, **kwargs)
        self.model = get_meta_labeler(method=model_method, features=features, target=target, **kwargs)
        
        # Track training status
        self._is_trained = False
        
        logger.info(f"Initialized meta-labeling strategy with {labeling_method} labeling and {model_method} model")
    
    def create_labels(self, 
                      data: pd.DataFrame, 
                      signal_col: str = 'position',
                      **kwargs) -> pd.DataFrame:
        """
        Create labels for meta-labeling.
        
        Args:
            data: DataFrame containing features and target
            signal_col: Column name for trading signals
            kwargs: Additional parameters for labeling
            
        Returns:
            DataFrame with labels
        """
        logger.info(f"Creating labels using {self.labeling_method} method")
        
        # Don't pass signal_col to create_labels if it's a TripleBarrierLabeler
        # since it's already configured with the signal_col during initialization
        return self.labeler.create_labels(data, **kwargs)
    
    def fit(self, 
            data: pd.DataFrame, 
            signal_col: str = 'position',
            test_size: float = 0.2,
            random_state: int = 42,
            **kwargs) -> Dict[str, float]:
        """
        Train the meta-labeling model.
        
        Args:
            data: DataFrame containing features 
            signal_col: Column name for trading signals
            test_size: Fraction of data to use for testing
            random_state: Random seed for reproducibility
            kwargs: Additional parameters for training
            
        Returns:
            Dict of performance metrics on test set
        """
        # Create labels if not already present
        if self.target not in data.columns:
            logger.info(f"Target column {self.target} not found, creating labels")
            data = self.create_labels(data, signal_col=signal_col, **kwargs)
        
        # Train model
        metrics = self.model.fit(data, test_size=test_size, random_state=random_state, **kwargs)
        self._is_trained = True
        
        return metrics
    
    def predict(self, 
                data: pd.DataFrame, 
                signal_col: str = 'position',
                threshold: float = 0.5,
                **kwargs) -> pd.DataFrame:
        """
        Apply meta-labeling to data.
        
        Args:
            data: DataFrame containing features
            signal_col: Column name for trading signals
            threshold: Probability threshold for binary predictions
            kwargs: Additional parameters for prediction
            
        Returns:
            DataFrame with meta-labeling predictions
        """
        # Create a copy to avoid modifying the original
        df = data.copy()
        
        # Check if model is trained
        if not self._is_trained:
            logger.error("Model not trained, cannot predict")
            # Return default values
            df['meta_position'] = df[signal_col] * 0.5 if signal_col in df.columns else 0.0
            df['label_prob'] = 0.5
            df['label_pred'] = 0
            return df
        
        # Ensure features are present
        missing_features = [f for f in self.features if f not in df.columns]
        if missing_features:
            logger.warning(f"Missing features: {missing_features}")
            # Add missing features with zeros
            for feature in missing_features:
                df[feature] = 0.0
        
        # Get predictions
        try:
            probabilities = self.model.predict_proba(df[self.features])
            
            # Store probabilities and binary predictions
            df['label_prob'] = probabilities
            df['label_pred'] = (probabilities >= threshold).astype(int)
            
            # Calculate meta position: original signal * probability
            if signal_col in df.columns:
                df['meta_position'] = df[signal_col] * df['label_prob']
            else:
                logger.warning(f"Signal column {signal_col} not found, using zeros")
                df['meta_position'] = 0.0
                
            return df
        except Exception as e:
            logger.error(f"Error in prediction: {str(e)}")
            # Return default values
            df['meta_position'] = df[signal_col] * 0.5 if signal_col in df.columns else 0.0
            df['label_prob'] = 0.5
            df['label_pred'] = 0
            return df
    
    def evaluate(self, 
                 data: pd.DataFrame, 
                 **kwargs) -> Dict[str, float]:
        """
        Evaluate the meta-labeling model.
        
        Args:
            data: DataFrame containing features and target
            kwargs: Additional parameters for evaluation
            
        Returns:
            Dict of performance metrics
        """
        if not self._is_trained:
            logger.warning("Model not trained, cannot evaluate")
            return {}
        
        return self.model.evaluate(data, **kwargs)
    
    def save(self, path: str) -> None:
        """
        Save the meta-labeling strategy to disk.
        
        Args:
            path: Directory path to save
        """
        if not self._is_trained:
            logger.warning("Model not trained, cannot save")
            return
        
        os.makedirs(path, exist_ok=True)
        
        try:
            # Save model and labeler
            self.model.save(os.path.join(path, 'model'))
            
            # Save metadata
            metadata = {
                'features': self.features,
                'labeling_method': self.labeling_method,
                'model_method': self.model_method,
                'target': self.target,
                'params': self.params,
                'is_trained': self._is_trained
            }
            
            metadata_path = os.path.join(path, 'meta_labeling_metadata.joblib')
            joblib.dump(metadata, metadata_path)
            
            logger.info(f"Saved meta-labeling strategy to {path}")
        except Exception as e:
            logger.error(f"Error saving meta-labeling strategy: {str(e)}")
    
    def load(self, path: str) -> None:
        """
        Load the meta-labeling strategy from disk.
        
        Args:
            path: Directory path to load from
        """
        try:
            metadata_path = os.path.join(path, 'meta_labeling_metadata.joblib')
            
            if os.path.exists(metadata_path):
                metadata = joblib.load(metadata_path)
                
                # Load metadata
                self.features = metadata.get('features', self.features)
                self.labeling_method = metadata.get('labeling_method', self.labeling_method)
                self.model_method = metadata.get('model_method', self.model_method)
                self.target = metadata.get('target', self.target)
                self.params = metadata.get('params', self.params)
                self._is_trained = metadata.get('is_trained', False)
                
                # Reinitialize labeler
                self.labeler = get_labeler(method=self.labeling_method, target=self.target, **self.params)
                
                # Load model
                self.model = get_meta_labeler(method=self.model_method, features=self.features, target=self.target, **self.params)
                self.model.load(os.path.join(path, 'model'))
                
                logger.info(f"Loaded meta-labeling strategy from {path}")
                return True
            else:
                logger.error(f"Metadata file not found in {path}")
                return False
        except Exception as e:
            logger.error(f"Error loading meta-labeling strategy: {str(e)}")
            return False


class RegimeMetaLabeling:
    """
    Regime-specific meta-labeling strategy.
    
    This class manages multiple meta-labeling models for different market regimes.
    """
    
    def __init__(self, 
                 features: List[str] = None,
                 labeling_method: str = 'triple_barrier',
                 model_method: str = 'random_forest',
                 target: str = 'label',
                 **kwargs):
        """
        Initialize regime-specific meta-labeling strategy.
        
        Args:
            features: List of feature column names to use
            labeling_method: Method for creating labels ('triple_barrier' or 'fixed_horizon')
            model_method: Type of model to use ('random_forest' or 'neural_network')
            target: Target column name for predictions
            kwargs: Additional parameters for labeler and model
        """
        self.features = features or []
        self.labeling_method = labeling_method
        self.model_method = model_method
        self.target = target
        self.params = kwargs
        
        # Dictionary to store regime-specific meta-labelers
        self.regime_meta_labelers = {}
        
        # Performance metrics for each regime
        self.regime_performance = {}
        
        logger.info(f"Initialized regime-specific meta-labeling with {labeling_method} labeling and {model_method} model")
    
    def fit(self, 
            data: pd.DataFrame, 
            regime_column: str,
            signal_col: str = 'position',
            test_size: float = 0.2,
            random_state: int = 42,
            **kwargs) -> Dict[str, Dict[str, float]]:
        """
        Train regime-specific meta-labeling models.
        
        Args:
            data: DataFrame containing features and regime labels
            regime_column: Column name containing regime labels
            signal_col: Column name for trading signals
            test_size: Fraction of data to use for testing
            random_state: Random seed for reproducibility
            kwargs: Additional parameters for training
            
        Returns:
            Dict of performance metrics by regime
        """
        logger.info(f"Training regime-specific meta-labelers using {regime_column}")
        
        # Verify regime column exists
        if regime_column not in data.columns:
            logger.error(f"Regime column {regime_column} not found in data")
            return {}
        
        # Create a copy of the data
        df = data.copy()
        
        # Get unique regimes
        regimes = df[regime_column].unique()
        logger.info(f"Found {len(regimes)} unique regimes: {regimes}")
        
        # Train a meta-labeler for each regime
        for regime in regimes:
            # Filter data for this regime
            regime_data = df[df[regime_column] == regime].copy()
            regime_count = len(regime_data)
            
            logger.info(f"Training meta-labeler for regime {regime} with {regime_count} samples")
            
            # Create a meta-labeler for this regime
            meta_labeler = MetaLabeling(
                features=self.features,
                labeling_method=self.labeling_method,
                model_method=self.model_method,
                target=self.target,
                **self.params
            )
            
            # Create labels if not already present
            if self.target not in regime_data.columns:
                logger.info(f"Target column {self.target} not found, creating labels")
                # Use gbclose as the price column
                regime_data = meta_labeler.create_labels(
                    regime_data, 
                    signal_col=signal_col, 
                    price_col='gbclose',
                    **kwargs
                )
            
            # Train the meta-labeler if we have enough data
            if len(regime_data) > 10:
                try:
                    # Train the model
                    performance = meta_labeler.fit(
                        regime_data, 
                        signal_col=signal_col,
                        test_size=test_size,
                        random_state=random_state,
                        **kwargs
                    )
                    
                    # Store the meta-labeler and performance
                    self.regime_meta_labelers[regime] = meta_labeler
                    self.regime_performance[regime] = performance
                    
                    logger.info(f"Trained meta-labeler for regime {regime} with metrics: {performance}")
                except Exception as e:
                    logger.error(f"Error training meta-labeler for regime {regime}: {str(e)}")
                    self.regime_performance[regime] = {}
            else:
                logger.warning(f"Not enough data for regime {regime}, skipping")
                self.regime_performance[regime] = {}
        
        logger.info(f"Trained {len(self.regime_meta_labelers)} regime-specific meta-labelers")
        return self.regime_performance
    
    def predict(self, 
                data: pd.DataFrame, 
                regime_column: str,
                signal_col: str = 'position',
                threshold: float = 0.5,
                **kwargs) -> pd.DataFrame:
        """
        Apply regime-specific meta-labeling to data.
        
        Args:
            data: DataFrame containing features and regime labels
            regime_column: Column name containing regime labels
            signal_col: Column name for trading signals
            threshold: Probability threshold for binary predictions
            kwargs: Additional parameters for prediction
            
        Returns:
            DataFrame with meta-labeling predictions
        """
        logger.info(f"Applying regime-specific meta-labeling using {regime_column}")
        
        # Verify regime column exists
        if regime_column not in data.columns:
            logger.error(f"Regime column {regime_column} not found in data")
            return data
        
        # Create a copy of the data
        df = data.copy()
        
        # Initialize prediction columns
        df['meta_position'] = 0.0
        df['label_prob'] = 0.5
        df['label_pred'] = 0
        df['active_regime'] = -1
        
        # Get unique regimes
        regimes = df[regime_column].unique()
        
        # Apply appropriate meta-labeler for each regime
        for regime in regimes:
            # Filter data for this regime
            regime_mask = df[regime_column] == regime
            regime_data = df[regime_mask].copy()
            
            logger.info(f"Processing regime {regime} with {len(regime_data)} samples")
            
            # Skip if no data for this regime
            if len(regime_data) == 0:
                continue
            
            # Apply meta-labeler if available for this regime
            if regime in self.regime_meta_labelers:
                try:
                    # Get meta-labeler for this regime
                    meta_labeler = self.regime_meta_labelers[regime]
                    
                    # Apply meta-labeling
                    regime_result = meta_labeler.predict(
                        regime_data, 
                        signal_col=signal_col,
                        threshold=threshold,
                        **kwargs
                    )
                    
                    # Update results for this regime
                    df.loc[regime_mask, 'meta_position'] = regime_result['meta_position']
                    df.loc[regime_mask, 'label_prob'] = regime_result['label_prob']
                    df.loc[regime_mask, 'label_pred'] = regime_result['label_pred']
                    df.loc[regime_mask, 'active_regime'] = regime
                except Exception as e:
                    logger.error(f"Error applying meta-labeler for regime {regime}: {str(e)}")
            else:
                logger.warning(f"No trained meta-labeler for regime {regime}")
                # Use default values for this regime
                df.loc[regime_mask, 'meta_position'] = df.loc[regime_mask, signal_col] * 0.5
                df.loc[regime_mask, 'label_prob'] = 0.5
                df.loc[regime_mask, 'label_pred'] = 0
                df.loc[regime_mask, 'active_regime'] = regime
        
        # Calculate meta-strategy returns
        logger.info("Calculating meta-strategy returns")
        if 'returns' in df.columns:
            df['meta_strategy_returns'] = df['meta_position'] * df['returns']
        
        logger.info(f"Applied regime-specific meta-labeling to {len(df)} samples")
        return df
    
    def save(self, path: str) -> None:
        """
        Save the regime-specific meta-labeling strategy to disk.
        
        Args:
            path: Directory path to save
        """
        os.makedirs(path, exist_ok=True)
        
        try:
            # Save each regime meta-labeler
            for regime, meta_labeler in self.regime_meta_labelers.items():
                regime_path = os.path.join(path, f'regime_{regime}')
                meta_labeler.save(regime_path)
            
            # Save metadata
            metadata = {
                'features': self.features,
                'labeling_method': self.labeling_method,
                'model_method': self.model_method,
                'target': self.target,
                'params': self.params,
                'regimes': list(self.regime_meta_labelers.keys()),
                'regime_performance': self.regime_performance
            }
            
            metadata_path = os.path.join(path, 'regime_meta_labeling_metadata.joblib')
            joblib.dump(metadata, metadata_path)
            
            logger.info(f"Saved regime-specific meta-labeling strategy to {path}")
        except Exception as e:
            logger.error(f"Error saving regime-specific meta-labeling strategy: {str(e)}")
    
    def load(self, path: str) -> None:
        """
        Load the regime-specific meta-labeling strategy from disk.
        
        Args:
            path: Directory path to load from
        """
        try:
            metadata_path = os.path.join(path, 'regime_meta_labeling_metadata.joblib')
            
            if os.path.exists(metadata_path):
                metadata = joblib.load(metadata_path)
                
                # Load metadata
                self.features = metadata.get('features', self.features)
                self.labeling_method = metadata.get('labeling_method', self.labeling_method)
                self.model_method = metadata.get('model_method', self.model_method)
                self.target = metadata.get('target', self.target)
                self.params = metadata.get('params', self.params)
                self.regime_performance = metadata.get('regime_performance', {})
                
                # Load regime meta-labelers
                regimes = metadata.get('regimes', [])
                for regime in regimes:
                    regime_path = os.path.join(path, f'regime_{regime}')
                    
                    # Create meta-labeler for this regime
                    meta_labeler = MetaLabeling(
                        features=self.features,
                        labeling_method=self.labeling_method,
                        model_method=self.model_method,
                        target=self.target,
                        **self.params
                    )
                    
                    # Load saved meta-labeler
                    if meta_labeler.load(regime_path):
                        self.regime_meta_labelers[regime] = meta_labeler
                    else:
                        logger.warning(f"Failed to load meta-labeler for regime {regime}")
                
                logger.info(f"Loaded regime-specific meta-labeling strategy from {path} with {len(self.regime_meta_labelers)} regimes")
                return True
            else:
                logger.error(f"Metadata file not found in {path}")
                return False
        except Exception as e:
            logger.error(f"Error loading regime-specific meta-labeling strategy: {str(e)}")
            return False

================
File: models/__init__.py
================
"""
Models package for Deep Learning Market Regime Detection and Position Sizing.
"""

# Import all model classes for easier access
from .base_rl import RLPositionSizer
from .dqn import DQNPositionSizer
from .ppo import PPOPositionSizer
from .sac import SACPositionSizer
from .dreamer import DreamerPositionSizer

__all__ = [
    'RLPositionSizer',
    'DQNPositionSizer',
    'PPOPositionSizer',
    'SACPositionSizer',
    'DreamerPositionSizer'
]

# Import models for easier access
from dl_metalabeling.models.hmm import HiddenMarketModel, load_hmm_model
from dl_metalabeling.models.transformer import (
    MarketRegimeTransformer,
    load_transformer_model,
    train_transformer_model
)

================
File: models/base_rl.py
================
"""
Base Reinforcement Learning Position Sizer model.
"""

import numpy as np
import pandas as pd
import pickle
import logging
from collections import defaultdict

logger = logging.getLogger(__name__)


class RLPositionSizer:
    """
    Base Q-learning model for dynamic position sizing.
    """
    
    def __init__(self, state_dim=8, action_dim=5, learning_rate=0.001, gamma=0.99, epsilon=0.1):
        """
        Initialize the RL position sizer.
        
        Args:
            state_dim (int): Dimensionality of the state space
            action_dim (int): Number of discrete actions (position sizes)
            learning_rate (float): Learning rate for Q-value updates
            gamma (float): Discount factor
            epsilon (float): Exploration rate
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        
        # Initialize Q-table
        self.q_table = defaultdict(lambda: np.zeros(action_dim))
        
        # Define position sizes
        self.position_sizes = np.linspace(-1.0, 1.0, action_dim)
        
        # Training stats
        self.training_rewards = []
        
        logger.info(f"Initialized base RL position sizer with {action_dim} discrete actions")
    
    def _discretize_state(self, observation):
        """
        Discretize continuous state for Q-table indexing.
        
        Args:
            observation (dict): Observation of the environment
            
        Returns:
            tuple: Discretized state
        """
        # Extract relevant features
        state_features = [
            observation.get('meta_confidence', 0),
            observation.get('hmm_regime', 0),
            observation.get('transformer_regime', 0),
            observation.get('returns', 0),
            observation.get('returns_5', 0),
            observation.get('returns_10', 0),
            observation.get('volatility_21', 0),
            observation.get('prev_position', 0)
        ]
        
        # Discretize each feature
        discretized = []
        for feature in state_features:
            if isinstance(feature, (int, np.integer)):
                discretized.append(int(feature))
            else:
                # Discretize continuous features into 5 bins
                bin_idx = min(max(int((feature + 3) / 6 * 5), 0), 4)
                discretized.append(bin_idx)
        
        return tuple(discretized[:self.state_dim])
    
    def _extract_features(self, data, idx):
        """
        Extract features from data at a specific index.
        
        Args:
            data (pandas.DataFrame): DataFrame with features
            idx (int): Index to extract features from
            
        Returns:
            dict: Dictionary of features
        """
        row = data.iloc[idx]
        
        observation = {
            'meta_confidence': row.get('meta_confidence', 0),
            'hmm_regime': row.get('hmm_regime', 0),
            'transformer_regime': row.get('transformer_regime', 0),
            'returns': row.get('returns', 0),
            'returns_5': row.get('returns_5', 0),
            'returns_10': row.get('returns_10', 0),
            'returns_20': row.get('returns_20', 0),
            'volatility_21': row.get('volatility_21', 0),
            'volatility_63': row.get('volatility_63', 0),
            'volatility_126': row.get('volatility_126', 0),
            'price_change': row.get('returns', 0),
            'prev_position': row.get('prev_position', 0)
        }
        
        return observation
    
    def _calculate_reward(self, action, price_change, prev_action=None):
        """
        Calculate reward for a given action.
        
        Args:
            action (int): Action index
            price_change (float): Price change percentage
            prev_action (int): Previous action index
            
        Returns:
            float: Reward
        """
        position_size = self.position_sizes[action]
        
        # Calculate return
        returns = position_size * price_change
        
        # Calculate transaction cost
        transaction_cost = 0
        if prev_action is not None:
            prev_position = self.position_sizes[prev_action]
            position_change = abs(position_size - prev_position)
            transaction_cost = position_change * 0.0001  # 1 pip for forex
        
        # Final reward is return minus transaction cost
        reward = returns - transaction_cost
        
        return reward
    
    def train(self, data, epochs=100, batch_size=64):
        """
        Train the model.
        
        Args:
            data (pandas.DataFrame): Training data
            epochs (int): Number of training epochs
            batch_size (int): Number of samples per batch
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training RL position sizer for {epochs} epochs with batch size {batch_size}")
        
        for epoch in range(epochs):
            total_reward = 0
            prev_action = None
            
            for idx in range(len(data) - 1):
                # Get current state
                observation = self._extract_features(data, idx)
                if prev_action is not None:
                    observation['prev_position'] = self.position_sizes[prev_action]
                
                state = self._discretize_state(observation)
                
                # Choose action using epsilon-greedy
                if np.random.random() < self.epsilon:
                    action = np.random.choice(self.action_dim)
                else:
                    action = np.argmax(self.q_table[state])
                
                # Calculate reward
                price_change = data.iloc[idx+1]['returns']
                reward = self._calculate_reward(action, price_change, prev_action)
                total_reward += reward
                
                # Get next state
                next_observation = self._extract_features(data, idx+1)
                next_observation['prev_position'] = self.position_sizes[action]
                next_state = self._discretize_state(next_observation)
                
                # Update Q-value
                best_next_action = np.argmax(self.q_table[next_state])
                td_target = reward + self.gamma * self.q_table[next_state][best_next_action]
                td_error = td_target - self.q_table[state][action]
                self.q_table[state][action] += self.learning_rate * td_error
                
                prev_action = action
            
            self.training_rewards.append(total_reward)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{epochs}, Reward: {total_reward:.4f}")
        
        logger.info(f"Training completed. Final reward: {self.training_rewards[-1]:.4f}")
        return self
    
    def predict(self, data):
        """
        Predict position sizes for the given data.
        
        Args:
            data (pandas.DataFrame): Data to predict for
            
        Returns:
            numpy.ndarray: Predicted position sizes
        """
        positions = np.zeros(len(data))
        prev_action = None
        
        for idx in range(len(data)):
            # Get current state
            observation = self._extract_features(data, idx)
            if prev_action is not None:
                observation['prev_position'] = self.position_sizes[prev_action]
            
            state = self._discretize_state(observation)
            
            # Choose best action
            action = np.argmax(self.q_table[state])
            positions[idx] = self.position_sizes[action]
            prev_action = action
        
        return positions
    
    def save(self, filepath):
        """
        Save the model to disk.
        
        Args:
            filepath (str): Path to save the model to
        """
        model_data = {
            'q_table': dict(self.q_table),
            'position_sizes': self.position_sizes,
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
            'learning_rate': self.learning_rate,
            'gamma': self.gamma,
            'epsilon': self.epsilon,
            'training_rewards': self.training_rewards
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"Model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath):
        """
        Load a model from disk.
        
        Args:
            filepath (str): Path to load the model from
            
        Returns:
            RLPositionSizer: Loaded model
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        model = cls(
            state_dim=model_data['state_dim'],
            action_dim=model_data['action_dim'],
            learning_rate=model_data['learning_rate'],
            gamma=model_data['gamma'],
            epsilon=model_data['epsilon']
        )
        
        # Load Q-table
        model.q_table = defaultdict(lambda: np.zeros(model.action_dim))
        for state, values in model_data['q_table'].items():
            model.q_table[state] = values
        
        model.position_sizes = model_data['position_sizes']
        model.training_rewards = model_data['training_rewards']
        
        logger.info(f"Model loaded from {filepath}")
        return model

================
File: models/dqn.py
================
"""
Deep Q-Network (DQN) Position Sizer model.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import logging
import os
import random
from collections import deque, namedtuple

logger = logging.getLogger(__name__)

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])


class ReplayBuffer:
    """Experience replay buffer for DQN."""
    
    def __init__(self, capacity):
        """
        Initialize replay buffer.
        
        Args:
            capacity (int): Maximum capacity of the buffer
        """
        self.buffer = deque(maxlen=capacity)
    
    def add(self, state, action, reward, next_state, done):
        """
        Add experience to buffer.
        
        Args:
            state (numpy.ndarray): Current state
            action (int): Action taken
            reward (float): Reward received
            next_state (numpy.ndarray): Next state
            done (bool): Whether episode is done
        """
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        """
        Sample a batch of experiences.
        
        Args:
            batch_size (int): Size of batch to sample
            
        Returns:
            tuple: Batch of (states, actions, rewards, next_states, dones)
        """
        experiences = random.sample(self.buffer, min(batch_size, len(self.buffer)))
        
        states = torch.FloatTensor([e.state for e in experiences])
        actions = torch.LongTensor([e.action for e in experiences])
        rewards = torch.FloatTensor([e.reward for e in experiences])
        next_states = torch.FloatTensor([e.next_state for e in experiences])
        dones = torch.FloatTensor([e.done for e in experiences])
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)


class QNetwork(nn.Module):
    """Q-Network for approximating Q-function in DQN."""
    
    def __init__(self, state_dim, action_dim, hidden_dims=None):
        """
        Initialize Q-Network.
        
        Args:
            state_dim (int): Dimension of state space
            action_dim (int): Dimension of action space
            hidden_dims (list): Dimensions of hidden layers
        """
        super(QNetwork, self).__init__()
        
        if hidden_dims is None:
            hidden_dims = [128, 64]
        
        # Build neural network
        layers = []
        layers.append(nn.Linear(state_dim, hidden_dims[0]))
        layers.append(nn.ReLU())
        
        for i in range(len(hidden_dims) - 1):
            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))
            layers.append(nn.ReLU())
        
        layers.append(nn.Linear(hidden_dims[-1], action_dim))
        
        self.model = nn.Sequential(*layers)
    
    def forward(self, state):
        """
        Forward pass through network.
        
        Args:
            state (torch.Tensor): State tensor
            
        Returns:
            torch.Tensor: Q-values for each action
        """
        return self.model(state)


class DQNPositionSizer:
    """
    Deep Q-Network (DQN) for dynamic position sizing.
    """
    
    def __init__(self, state_dim=10, action_dim=9, hidden_dims=None, learning_rate=0.0005,
                 gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,
                 buffer_size=10000):
        """
        Initialize DQN position sizer.
        
        Args:
            state_dim (int): Dimension of state space
            action_dim (int): Dimension of action space
            hidden_dims (list): Dimensions of hidden layers
            learning_rate (float): Learning rate for optimizer
            gamma (float): Discount factor
            epsilon_start (float): Starting value of epsilon
            epsilon_end (float): Minimum value of epsilon
            epsilon_decay (float): Decay rate of epsilon
            buffer_size (int): Size of replay buffer
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # DQN hyperparameters
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # Position sizes
        self.position_sizes = np.linspace(-1.0, 1.0, action_dim)
        
        # State normalization
        self.state_mean = np.zeros(state_dim)
        self.state_std = np.ones(state_dim)
        
        # Create Q-networks
        self.q_network = QNetwork(state_dim, action_dim, hidden_dims)
        self.target_network = QNetwork(state_dim, action_dim, hidden_dims)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # Create optimizer
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # Create replay buffer
        self.replay_buffer = ReplayBuffer(buffer_size)
        
        # Training stats
        self.losses = []
        self.rewards = []
        
        # Device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network.to(self.device)
        self.target_network.to(self.device)
        
        logger.info(f"Initialized DQN position sizer with {action_dim} actions on {self.device}")
    
    def _extract_state(self, data, idx):
        """
        Extract state from data at index.
        
        Args:
            data (pandas.DataFrame): Data
            idx (int): Index
            
        Returns:
            numpy.ndarray: State vector
        """
        row = data.iloc[idx]
        
        # Extract features
        state = np.array([
            row.get('meta_confidence', 0),
            row.get('hmm_regime', 0) / 2,  # Normalize to [0, 1]
            row.get('transformer_regime', 0) / 2,  # Normalize to [0, 1]
            row.get('returns', 0),
            row.get('returns_5', 0),
            row.get('returns_10', 0),
            row.get('returns_20', 0),
            row.get('volatility_21', 0),
            row.get('volatility_63', 0),
            row.get('volatility_126', 0),
            row.get('prev_position', 0)  # Will be updated during training/prediction
        ])
        
        return state[:self.state_dim]
    
    def _calculate_reward(self, action_idx, price_change, prev_action_idx=None):
        """
        Calculate reward for an action.
        
        Args:
            action_idx (int): Index of action taken
            price_change (float): Price change
            prev_action_idx (int): Index of previous action
            
        Returns:
            float: Reward
        """
        position_size = self.position_sizes[action_idx]
        
        # Calculate return
        returns = position_size * price_change
        
        # Calculate transaction cost
        transaction_cost = 0
        if prev_action_idx is not None:
            prev_position = self.position_sizes[prev_action_idx]
            position_change = abs(position_size - prev_position)
            transaction_cost = position_change * 0.0001  # 1 pip for forex
        
        # Final reward is return minus transaction cost
        reward = returns - transaction_cost
        
        return reward
    
    def _normalize_state(self, state):
        """
        Normalize state using stored mean and std.
        
        Args:
            state (numpy.ndarray): State vector
            
        Returns:
            numpy.ndarray: Normalized state
        """
        return (state - self.state_mean) / (self.state_std + 1e-8)
    
    def _select_action(self, state, training=True):
        """
        Select action using epsilon-greedy policy.
        
        Args:
            state (numpy.ndarray): State vector
            training (bool): Whether in training mode
            
        Returns:
            int: Selected action index
        """
        if training and np.random.random() < self.epsilon:
            return np.random.randint(self.action_dim)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()
    
    def _update_networks(self, batch_size):
        """
        Update Q-network from replay buffer.
        
        Args:
            batch_size (int): Batch size for update
            
        Returns:
            float: Loss value
        """
        if len(self.replay_buffer) < batch_size:
            return 0.0
        
        # Sample batch from replay buffer
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)
        
        # Compute Q-values
        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # Compute target Q-values
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)
        
        # Compute loss
        loss = nn.MSELoss()(q_values, target_q_values)
        
        # Update network
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, data, epochs=200, batch_size=64, target_update=10):
        """
        Train the DQN model.
        
        Args:
            data (pandas.DataFrame): Training data
            epochs (int): Number of epochs
            batch_size (int): Batch size for updates
            target_update (int): Update target network every n epochs
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training DQN position sizer for {epochs} epochs with batch size {batch_size}")
        
        # Calculate state statistics for normalization
        states = []
        for idx in range(len(data)):
            state = self._extract_state(data, idx)
            states.append(state)
        
        states = np.array(states)
        self.state_mean = states.mean(axis=0)
        self.state_std = states.std(axis=0)
        
        # Training loop
        for epoch in range(epochs):
            total_reward = 0
            total_loss = 0
            prev_action = None
            prev_state = None
            
            for idx in range(len(data) - 1):
                # Extract and normalize state
                state = self._extract_state(data, idx)
                if prev_action is not None:
                    state[-1] = self.position_sizes[prev_action]
                
                normalized_state = self._normalize_state(state)
                
                # Select action
                action = self._select_action(normalized_state, training=True)
                
                # Calculate reward
                price_change = data.iloc[idx+1]['returns']
                reward = self._calculate_reward(action, price_change, prev_action)
                total_reward += reward
                
                # Get next state
                next_state = self._extract_state(data, idx+1)
                next_state[-1] = self.position_sizes[action]
                normalized_next_state = self._normalize_state(next_state)
                
                # Store in replay buffer
                self.replay_buffer.add(
                    normalized_state,
                    action,
                    reward,
                    normalized_next_state,
                    False  # not done
                )
                
                # Update networks
                if len(self.replay_buffer) >= batch_size:
                    loss = self._update_networks(batch_size)
                    total_loss += loss
                
                prev_action = action
                prev_state = state
            
            # Update target network
            if (epoch + 1) % target_update == 0:
                self.target_network.load_state_dict(self.q_network.state_dict())
            
            # Decay epsilon
            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
            
            # Store metrics
            self.losses.append(total_loss)
            self.rewards.append(total_reward)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{epochs}, Reward: {total_reward:.4f}, Loss: {total_loss:.4f}, Epsilon: {self.epsilon:.4f}")
        
        logger.info(f"Training completed. Final reward: {self.rewards[-1]:.4f}")
        return self
    
    def predict(self, data):
        """
        Predict position sizes for data.
        
        Args:
            data (pandas.DataFrame): Data to predict for
            
        Returns:
            numpy.ndarray: Position sizes
        """
        positions = np.zeros(len(data))
        prev_action = None
        
        for idx in range(len(data)):
            # Extract and normalize state
            state = self._extract_state(data, idx)
            if prev_action is not None:
                state[-1] = self.position_sizes[prev_action]
            
            normalized_state = self._normalize_state(state)
            
            # Select best action
            action = self._select_action(normalized_state, training=False)
            positions[idx] = self.position_sizes[action]
            
            prev_action = action
        
        return positions
    
    def save(self, filepath):
        """
        Save model to files.
        
        Args:
            filepath (str): Base path to save model files
        """
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # Save model weights
        torch.save(self.q_network.state_dict(), f"{filepath}_model.pth")
        
        # Save config
        config = {
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
            'position_sizes': self.position_sizes,
            'state_mean': self.state_mean,
            'state_std': self.state_std,
            'gamma': self.gamma,
            'epsilon': self.epsilon,
            'epsilon_end': self.epsilon_end,
            'epsilon_decay': self.epsilon_decay,
            'losses': self.losses,
            'rewards': self.rewards
        }
        
        np.save(f"{filepath}_config.npy", config)
        
        logger.info(f"Model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath, hidden_dims=None):
        """
        Load model from files.
        
        Args:
            filepath (str): Base path to load model files
            hidden_dims (list): Hidden dimensions for network architecture
            
        Returns:
            DQNPositionSizer: Loaded model
        """
        # Load config
        config = np.load(f"{filepath}_config.npy", allow_pickle=True).item()
        
        # Create model
        model = cls(
            state_dim=config['state_dim'],
            action_dim=config['action_dim'],
            hidden_dims=hidden_dims,
            gamma=config['gamma'],
            epsilon_start=config['epsilon'],
            epsilon_end=config['epsilon_end'],
            epsilon_decay=config['epsilon_decay']
        )
        
        # Load state mean and std
        model.state_mean = config['state_mean']
        model.state_std = config['state_std']
        model.position_sizes = config['position_sizes']
        model.losses = config['losses']
        model.rewards = config['rewards']
        
        # Load model weights
        model.q_network.load_state_dict(torch.load(f"{filepath}_model.pth", map_location=model.device))
        model.target_network.load_state_dict(model.q_network.state_dict())
        
        logger.info(f"Model loaded from {filepath}")
        return model

================
File: models/dreamer.py
================
"""
Dreamer Position Sizer model.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import logging
import os
from collections import deque

logger = logging.getLogger(__name__)

class DreamerPositionSizer:
    """
    Dreamer model-based RL for dynamic position sizing.
    """
    
    def __init__(self, state_dim=10, action_dim=1, deter_dim=200, stoch_dim=30, 
                 hidden_dim=200, actor_lr=0.0001, critic_lr=0.0001, world_lr=0.0001,
                 gamma=0.99, lambda_gae=0.95, horizon=15):
        """
        Initialize Dreamer position sizer.
        
        Args:
            state_dim (int): Dimension of state space
            action_dim (int): Dimension of action space (1 for continuous)
            deter_dim (int): Dimension of deterministic state
            stoch_dim (int): Dimension of stochastic state
            hidden_dim (int): Dimension of hidden layers
            actor_lr (float): Learning rate for actor
            critic_lr (float): Learning rate for critic
            world_lr (float): Learning rate for world model
            gamma (float): Discount factor
            lambda_gae (float): GAE lambda parameter
            horizon (int): Planning horizon
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Model dimensions
        self.deter_dim = deter_dim
        self.stoch_dim = stoch_dim
        self.hidden_dim = hidden_dim
        
        # Hyperparameters
        self.gamma = gamma
        self.lambda_gae = lambda_gae
        self.horizon = horizon
        
        # Placeholder for models, optimizers, and buffers
        # These would be properly implemented in a full version
        
        # Performance tracking
        self.training_rewards = []
        
        logger.info(f"Initialized Dreamer position sizer with "
                   f"state dim {state_dim}, action dim {action_dim}, "
                   f"deterministic dim {deter_dim}, stochastic dim {stoch_dim}")
    
    def train(self, data, epochs=1000, batch_size=50, collect_steps=1000, train_steps=100):
        """
        Train the Dreamer model.
        
        Args:
            data (pandas.DataFrame): Training data
            epochs (int): Number of training epochs
            batch_size (int): Batch size for updates
            collect_steps (int): Steps to collect per epoch
            train_steps (int): Training iterations per epoch
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training Dreamer model with {epochs} epochs")
        
        # Placeholder for training logic
        # In a full implementation, this would:
        # 1. Collect real experience
        # 2. Train world model
        # 3. Generate imagined trajectories
        # 4. Update actor and critic
        
        # Simulate training by creating some random rewards
        for epoch in range(epochs):
            reward = np.random.normal(0, 1) * (epoch / epochs)
            self.training_rewards.append(reward)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{epochs}, Reward: {reward:.4f}")
        
        logger.info("Dreamer training completed")
        return self
    
    def predict(self, data):
        """
        Predict position sizes for data.
        
        Args:
            data (pandas.DataFrame): Data to predict for
            
        Returns:
            numpy.ndarray: Position sizes
        """
        # In a real implementation, this would use the trained policy
        # For now, we'll return random positions
        positions = np.zeros(len(data))
        
        # Simple random positions scaled by [-1, 1]
        return positions
    
    def save(self, filepath):
        """
        Save model to file.
        
        Args:
            filepath (str): Path to save model
        """
        logger.info(f"Saving Dreamer model to {filepath}")
        # Placeholder for save logic
    
    @classmethod
    def load(cls, filepath):
        """
        Load model from file.
        
        Args:
            filepath (str): Path to load model from
            
        Returns:
            DreamerPositionSizer: Loaded model
        """
        logger.info(f"Loading Dreamer model from {filepath}")
        # Placeholder for load logic
        return cls()

================
File: models/hmm.py
================
"""
Hidden Markov Model for market regime detection.
"""

import os
import numpy as np
import pandas as pd
import pickle
from hmmlearn import hmm

class HiddenMarketModel:
    """
    Hidden Markov Model for market regime detection.
    
    This class implements a wrapper around hmmlearn's GaussianHMM
    model for easier use in financial time series analysis.
    """
    
    def __init__(self, n_regimes=3, n_iter=1000, random_state=42, refit=False):
        """
        Initialize the Hidden Markov Model.
        
        Parameters:
        n_regimes (int): Number of regimes to model
        n_iter (int): Number of iterations for training
        random_state (int): Random seed for reproducibility
        refit (bool): Whether to refit the model if already trained
        """
        self.n_regimes = n_regimes
        self.n_iter = n_iter
        self.random_state = random_state
        self.refit = refit
        self.model = None
        self.is_trained = False
        self._setup_model()
        
        # Default regime labels
        self.regime_labels = ["Volatile", "Trending", "Mean-Reverting"]
        
    def _setup_model(self):
        """Set up the HMM model."""
        self.model = hmm.GaussianHMM(
            n_components=self.n_regimes,
            covariance_type="full",
            n_iter=self.n_iter,
            random_state=self.random_state
        )
        
    def _prepare_data(self, data):
        """
        Prepare data for HMM model.
        
        Parameters:
        data (pd.DataFrame or np.ndarray): Input data
        
        Returns:
        np.ndarray: Prepared data
        """
        # If DataFrame, extract returns
        if isinstance(data, pd.DataFrame):
            if 'returns' in data.columns:
                returns = data['returns'].values
            elif 'gbclose' in data.columns:
                returns = data['gbclose'].pct_change().values
            else:
                # Try to find a price column
                price_cols = [col for col in data.columns if any(x in col.lower() for x in ['close', 'price'])]
                if price_cols:
                    returns = data[price_cols[0]].pct_change().values
                else:
                    raise ValueError("No returns or price column found in data")
        else:
            returns = data
            
        # Remove NaN values
        returns = returns[~np.isnan(returns)]
        
        # Calculate features
        features = np.column_stack([
            returns,  # Raw returns
            np.abs(returns),  # Absolute returns (volatility)
            np.sign(returns),  # Sign of returns (direction)
            np.roll(returns, 1),  # Lagged returns (autocorrelation)
            np.roll(np.abs(returns), 1)  # Lagged absolute returns (volatility clustering)
        ])
        
        # Remove rows with NaN values (from rolling operations)
        features = features[~np.isnan(features).any(axis=1)]
        
        return features
    
    def fit(self, data):
        """
        Train the HMM model on the data.
        
        Parameters:
        data (pd.DataFrame or np.ndarray): Input data
        
        Returns:
        self: Trained model
        """
        # Check if model is already trained and refit is False
        if self.is_trained and not self.refit:
            print("Model already trained. Set refit=True to retrain.")
            return self
        
        # Prepare data
        features = self._prepare_data(data)
        
        # Train the model
        self.model.fit(features)
        
        # Mark as trained
        self.is_trained = True
        
        # Get regime characteristics
        self._analyze_regimes(features)
        
        return self
    
    def _analyze_regimes(self, features):
        """
        Analyze regime characteristics.
        
        Parameters:
        features (np.ndarray): Input features
        """
        # Get regime probabilities
        hidden_states = self.model.predict(features)
        
        # Analyze each regime
        for i in range(self.n_regimes):
            regime_data = features[hidden_states == i]
            
            # Calculate statistics for this regime
            avg_return = np.mean(regime_data[:, 0])
            volatility = np.std(regime_data[:, 0])
            frequency = len(regime_data) / len(features)
            
            print(f"Regime {i} ({self.regime_labels[i]}):")
            print(f"  Average Return: {avg_return:.6f}")
            print(f"  Volatility: {volatility:.6f}")
            print(f"  Frequency: {frequency:.2%}")
            print(f"  Days: {len(regime_data)}")
    
    def predict(self, data):
        """
        Predict regimes for new data.
        
        Parameters:
        data (pd.DataFrame or np.ndarray): Input data
        
        Returns:
        np.ndarray: Predicted regimes
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before prediction")
        
        # If input is a DataFrame, extract features and handle missing values
        if isinstance(data, pd.DataFrame):
            # Create a copy to avoid modifying the original
            df = data.copy()
            
            # Calculate returns if not present
            if 'returns' not in df.columns and 'gbclose' in df.columns:
                df['returns'] = df['gbclose'].pct_change()
            
            # Extract returns series and identify valid indices
            if 'returns' in df.columns:
                returns_series = df['returns']
                valid_indices = ~returns_series.isna()
                returns = returns_series.values
            else:
                # Try to find a price column
                price_cols = [col for col in df.columns if any(x in col.lower() for x in ['close', 'price'])]
                if price_cols:
                    returns_series = df[price_cols[0]].pct_change()
                    valid_indices = ~returns_series.isna()
                    returns = returns_series.values
                else:
                    raise ValueError("No returns or price column found in data")
            
            # Calculate features for rows with valid returns
            features_list = []
            regime_predictions = np.full(len(df), np.nan)
            
            # Use a sliding window approach for prediction
            window_size = 100  # Adjust as needed
            
            for i in range(len(df)):
                if valid_indices[i]:
                    # Get window of previous returns
                    start_idx = max(0, i - window_size)
                    window_returns = returns[start_idx:i+1]
                    
                    # Calculate features for this window
                    window_features = np.column_stack([
                        window_returns,
                        np.abs(window_returns),
                        np.sign(window_returns),
                        np.roll(window_returns, 1),
                        np.roll(np.abs(window_returns), 1)
                    ])
                    
                    # Remove NaN rows
                    window_features = window_features[~np.isnan(window_features).any(axis=1)]
                    
                    if len(window_features) > 0:
                        # Predict regime for this window
                        regime = self.model.predict(window_features)[-1]
                        regime_predictions[i] = regime
            
            # Fill NaN values with previous regime (forward fill)
            for i in range(1, len(regime_predictions)):
                if np.isnan(regime_predictions[i]):
                    regime_predictions[i] = regime_predictions[i-1]
            
            # Replace any remaining NaNs with the most common regime
            if np.isnan(regime_predictions[0]):
                most_common_regime = int(np.nanmedian(regime_predictions))
                regime_predictions = np.nan_to_num(regime_predictions, nan=most_common_regime)
            
            return regime_predictions.astype(int)
        else:
            # For numpy array input
            features = self._prepare_data(data)
            return self.model.predict(features)
    
    def get_regime_probs(self, data):
        """
        Get regime probabilities for each data point.
        
        Parameters:
        data (pd.DataFrame or np.ndarray): Input data
        
        Returns:
        np.ndarray: Regime probabilities
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before prediction")
        
        # Prepare data
        features = self._prepare_data(data)
        
        # Get regime probabilities
        return self.model.predict_proba(features)
    
    def save(self, filepath):
        """
        Save the trained model to a file.
        
        Parameters:
        filepath (str): Path to save the model
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before saving")
        
        model_data = {
            'model': self.model,
            'n_regimes': self.n_regimes,
            'n_iter': self.n_iter,
            'random_state': self.random_state,
            'is_trained': self.is_trained,
            'regime_labels': self.regime_labels
        }
        
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
            
        print(f"Model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath):
        """
        Load a trained model from a file.
        
        Parameters:
        filepath (str): Path to the saved model
        
        Returns:
        HiddenMarketModel: Loaded model
        """
        if not os.path.exists(filepath):
            print(f"Model file {filepath} not found")
            return None
            
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        # Create a new instance
        instance = cls(
            n_regimes=model_data['n_regimes'],
            n_iter=model_data['n_iter'],
            random_state=model_data['random_state']
        )
        
        # Update instance variables
        instance.model = model_data['model']
        instance.is_trained = model_data['is_trained']
        instance.regime_labels = model_data['regime_labels']
        
        return instance

def load_hmm_model(model_path='models/hmm_model.pkl'):
    """
    Load a trained HMM model from file.
    
    Parameters:
    model_path (str): Path to the saved model
    
    Returns:
    HiddenMarketModel: Loaded model or None if file not found
    """
    return HiddenMarketModel.load(model_path)

================
File: models/ppo.py
================
"""
Proximal Policy Optimization (PPO) Position Sizer model.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import logging
import os
from collections import deque

logger = logging.getLogger(__name__)

class PPOPositionSizer:
    """
    Proximal Policy Optimization (PPO) for dynamic position sizing.
    """
    
    def __init__(self, state_dim=10, action_dim=1, hidden_dims=None, 
                 actor_lr=0.0003, critic_lr=0.001, gamma=0.99, gae_lambda=0.95,
                 clip_param=0.2, value_coef=0.5, entropy_coef=0.01):
        """
        Initialize PPO position sizer.
        
        Args:
            state_dim (int): Dimension of state space
            action_dim (int): Dimension of action space (1 for continuous)
            hidden_dims (list): Dimensions of hidden layers
            actor_lr (float): Learning rate for actor
            critic_lr (float): Learning rate for critic
            gamma (float): Discount factor
            gae_lambda (float): GAE lambda parameter
            clip_param (float): PPO clipping parameter
            value_coef (float): Value loss coefficient
            entropy_coef (float): Entropy coefficient
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Hyperparameters
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_param = clip_param
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        
        # Placeholder for models, optimizers, and buffers
        # These would be properly implemented in a full version
        
        # Performance tracking
        self.training_rewards = []
        
        logger.info(f"Initialized PPO position sizer with state dim {state_dim} and action dim {action_dim}")
    
    def train(self, data, epochs=300, batch_size=64, num_updates=10):
        """
        Train the PPO model.
        
        Args:
            data (pandas.DataFrame): Training data
            epochs (int): Number of training epochs
            batch_size (int): Batch size for updates
            num_updates (int): Number of updates per batch
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training PPO model with {epochs} epochs")
        
        # Placeholder for training logic
        # In a full implementation, this would:
        # 1. Collect trajectories
        # 2. Compute advantages using GAE
        # 3. Update policy and value function multiple times
        
        # Simulate training by creating some random rewards
        for epoch in range(epochs):
            reward = np.random.normal(0, 1) * (epoch / epochs)
            self.training_rewards.append(reward)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{epochs}, Reward: {reward:.4f}")
        
        logger.info("PPO training completed")
        return self
    
    def predict(self, data):
        """
        Predict position sizes for data.
        
        Args:
            data (pandas.DataFrame): Data to predict for
            
        Returns:
            numpy.ndarray: Position sizes
        """
        # In a real implementation, this would use the trained policy
        # For now, we'll return random positions
        positions = np.zeros(len(data))
        
        # Simple random positions scaled by [-1, 1]
        return positions
    
    def save(self, filepath):
        """
        Save model to file.
        
        Args:
            filepath (str): Path to save model
        """
        logger.info(f"Saving PPO model to {filepath}")
        # Placeholder for save logic
    
    @classmethod
    def load(cls, filepath):
        """
        Load model from file.
        
        Args:
            filepath (str): Path to load model from
            
        Returns:
            PPOPositionSizer: Loaded model
        """
        logger.info(f"Loading PPO model from {filepath}")
        # Placeholder for load logic
        return cls()

================
File: models/sac.py
================
"""
Soft Actor-Critic (SAC) Position Sizer model.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import logging
import os
from collections import deque

logger = logging.getLogger(__name__)

class SACPositionSizer:
    """
    Soft Actor-Critic (SAC) for dynamic position sizing.
    """
    
    def __init__(self, state_dim=10, action_dim=1, hidden_dims=None, 
                 actor_lr=0.0003, critic_lr=0.0003, alpha_lr=0.0003,
                 gamma=0.99, tau=0.005, buffer_size=100000):
        """
        Initialize SAC position sizer.
        
        Args:
            state_dim (int): Dimension of state space
            action_dim (int): Dimension of action space (1 for continuous)
            hidden_dims (list): Dimensions of hidden layers
            actor_lr (float): Learning rate for actor
            critic_lr (float): Learning rate for critic
            alpha_lr (float): Learning rate for temperature parameter
            gamma (float): Discount factor
            tau (float): Soft update coefficient
            buffer_size (int): Size of replay buffer
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Hyperparameters
        self.gamma = gamma
        self.tau = tau
        
        # Placeholder for models, optimizers, and buffers
        # These would be properly implemented in a full version
        
        # Performance tracking
        self.training_rewards = []
        
        logger.info(f"Initialized SAC position sizer with state dim {state_dim} and action dim {action_dim}")
    
    def train(self, data, epochs=500, batch_size=256, initial_random_steps=10000):
        """
        Train the SAC model.
        
        Args:
            data (pandas.DataFrame): Training data
            epochs (int): Number of training epochs
            batch_size (int): Batch size for updates
            initial_random_steps (int): Number of random steps for exploration
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training SAC model with {epochs} epochs")
        
        # Placeholder for training logic
        # In a full implementation, this would:
        # 1. Collect experiences with exploration
        # 2. Update critics, actor, and temperature
        # 3. Perform soft target updates
        
        # Simulate training by creating some random rewards
        for epoch in range(epochs):
            reward = np.random.normal(0, 1) * (epoch / epochs)
            self.training_rewards.append(reward)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{epochs}, Reward: {reward:.4f}")
        
        logger.info("SAC training completed")
        return self
    
    def predict(self, data):
        """
        Predict position sizes for data.
        
        Args:
            data (pandas.DataFrame): Data to predict for
            
        Returns:
            numpy.ndarray: Position sizes
        """
        # In a real implementation, this would use the trained policy
        # For now, we'll return random positions
        positions = np.zeros(len(data))
        
        # Simple random positions scaled by [-1, 1]
        return positions
    
    def save(self, filepath):
        """
        Save model to file.
        
        Args:
            filepath (str): Path to save model
        """
        logger.info(f"Saving SAC model to {filepath}")
        # Placeholder for save logic
    
    @classmethod
    def load(cls, filepath):
        """
        Load model from file.
        
        Args:
            filepath (str): Path to load model from
            
        Returns:
            SACPositionSizer: Loaded model
        """
        logger.info(f"Loading SAC model from {filepath}")
        # Placeholder for load logic
        return cls()

================
File: models/transformer.py
================
"""
Transformer Encoder for market regime detection with self-supervised learning.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import os
import warnings
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import random
from tqdm import tqdm

# Suppress specific warnings
warnings.filterwarnings("ignore", category=pd.errors.SettingWithCopyWarning)

# Set random seeds for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class MarketRegimeTransformer(nn.Module):
    """Transformer Encoder model for market regime detection."""
    
    def __init__(self, input_dim, hidden_dim=64, num_layers=2, num_heads=4, dropout=0.1):
        """
        Initialize the Transformer Encoder model.
        
        Parameters:
        input_dim (int): Input feature dimension
        hidden_dim (int): Hidden layer dimension
        num_layers (int): Number of transformer encoder layers
        num_heads (int): Number of attention heads
        dropout (float): Dropout rate
        """
        super(MarketRegimeTransformer, self).__init__()
        
        # Embedding layer
        self.embedding = nn.Linear(input_dim, hidden_dim)
        
        # Positional encoding
        self.pos_encoder = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim*4,
            dropout=dropout,
            batch_first=True
        )
        
        # Transformer encoder
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer=self.pos_encoder,
            num_layers=num_layers
        )
        
        # Projection head (for self-supervised learning)
        self.projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
    
    def forward(self, x, return_features=False):
        """
        Forward pass through the network.
        
        Parameters:
        x (torch.Tensor): Input tensor of shape [batch_size, seq_len, input_dim]
        return_features (bool): Whether to return features for regime clustering
        
        Returns:
        torch.Tensor: Output tensor
        """
        # x shape: [batch_size, seq_len, input_dim]
        
        # Embedding
        x = self.embedding(x)
        
        # Transformer encoder
        features = self.transformer_encoder(x)
        
        # Global average pooling over sequence dimension
        features = torch.mean(features, dim=1)
        
        if return_features:
            return features
        
        # Projection head
        projection = self.projection(features)
        
        return projection


class NTXentLoss(nn.Module):
    """Normalized Temperature-scaled Cross Entropy Loss (NT-Xent)."""
    
    def __init__(self, temperature=0.5):
        """
        Initialize the loss function.
        
        Parameters:
        temperature (float): Temperature parameter
        """
        super(NTXentLoss, self).__init__()
        self.temperature = temperature
        self.criterion = nn.CrossEntropyLoss(reduction="sum")
        
    def forward(self, z_i, z_j):
        """
        Forward pass for contrastive loss calculation.
        
        Parameters:
        z_i (torch.Tensor): First set of embeddings [batch_size, projection_dim]
        z_j (torch.Tensor): Second set of embeddings [batch_size, projection_dim]
        
        Returns:
        torch.Tensor: Calculated loss value
        """
        # z_i, z_j are embeddings from two augmented versions of a sample
        # [batch_size, projection_dim]
        batch_size = z_i.shape[0]
        
        # Concatenate embeddings to create the full matrix
        representations = torch.cat([z_i, z_j], dim=0)
        
        # Normalize
        representations = nn.functional.normalize(representations, dim=1)
        
        # Similarity matrix
        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature
        
        # Remove diagonals (self-similarity)
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)
        similarity_matrix = similarity_matrix.masked_fill_(mask, -9e15)
        
        # Create labels for positive pairs
        # For each sample in z_i, the positive is the corresponding sample in z_j
        # Labels for z_i are [0, 1, 2, ..., batch_size-1] + batch_size
        # Labels for z_j are [0, 1, 2, ..., batch_size-1]
        labels_i = torch.arange(batch_size, device=device) + batch_size
        labels_j = torch.arange(batch_size, device=device)
        labels = torch.cat([labels_i, labels_j])
        
        loss = self.criterion(similarity_matrix, labels)
        
        return loss / (2 * batch_size)


class FinancialTimeSeriesDataset(Dataset):
    """Dataset for financial time series data with augmentation."""
    
    def __init__(self, data, seq_length=20, augmentation_strength=0.1):
        """
        Initialize the dataset.
        
        Parameters:
        data (np.array): Input data array
        seq_length (int): Sequence length for each sample
        augmentation_strength (float): Strength of augmentation noise
        """
        self.data = data
        self.seq_length = seq_length
        self.augmentation_strength = augmentation_strength
    
    def __len__(self):
        """Return the number of sequences in the dataset."""
        return len(self.data) - self.seq_length + 1
    
    def __getitem__(self, idx):
        """
        Get a sequence and its augmented version.
        
        Parameters:
        idx (int): Index of the sequence
        
        Returns:
        tuple: Original and augmented sequences
        """
        sequence = self.data[idx:idx+self.seq_length]
        augmented_sequence = self._augment(sequence.copy())
        
        return torch.FloatTensor(sequence), torch.FloatTensor(augmented_sequence)
    
    def _augment(self, x):
        """
        Augment a sequence by adding noise.
        
        Parameters:
        x (np.array): Input sequence
        
        Returns:
        np.array: Augmented sequence
        """
        # Simple augmentation: add Gaussian noise
        if self.augmentation_strength > 0:
            noise = np.random.normal(0, self.augmentation_strength, x.shape)
            x = x + noise
        return x


class TransformerMarketModel:
    """Class to handle training, evaluation, and prediction using the Transformer model."""
    
    def __init__(self, input_dim=7, hidden_dim=64, seq_length=20):
        """
        Initialize the model handler.
        
        Parameters:
        input_dim (int): Input feature dimension
        hidden_dim (int): Hidden layer dimension
        seq_length (int): Sequence length for each sample
        """
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.seq_length = seq_length
        self.model = MarketRegimeTransformer(input_dim=input_dim, hidden_dim=hidden_dim).to(device)
        self.kmeans = None
        self.regime_labels = None
        self.scaler = StandardScaler()
        self.is_trained = False
    
    def prepare_data(self, df):
        """
        Prepare data for the model.
        
        Parameters:
        df (pd.DataFrame): Input dataframe with market data
        
        Returns:
        np.array: Scaled feature matrix
        pd.DataFrame: Processed dataframe
        """
        # Define features
        features = [
            'returns',
            'volatility_5d',
            'volatility_20d',
            'momentum_5d',
            'momentum_20d',
            'deviation_20d',
            'deviation_50d'
        ]
        
        # Check if features exist, calculate if needed
        df_processed = df.copy()
        
        if 'returns' not in df_processed.columns and 'gbclose' in df_processed.columns:
            df_processed['returns'] = df_processed['gbclose'].pct_change()
            
        if 'volatility_5d' not in df_processed.columns and 'returns' in df_processed.columns:
            df_processed['volatility_5d'] = df_processed['returns'].rolling(window=5).std()
            
        if 'volatility_20d' not in df_processed.columns and 'returns' in df_processed.columns:
            df_processed['volatility_20d'] = df_processed['returns'].rolling(window=20).std()
            
        if 'momentum_5d' not in df_processed.columns and 'returns' in df_processed.columns:
            df_processed['momentum_5d'] = df_processed['returns'].rolling(window=5).mean()
            
        if 'momentum_20d' not in df_processed.columns and 'returns' in df_processed.columns:
            df_processed['momentum_20d'] = df_processed['returns'].rolling(window=20).mean()
            
        if 'deviation_20d' not in df_processed.columns and 'gbclose' in df_processed.columns:
            if 'ma_20d' not in df_processed.columns:
                df_processed['ma_20d'] = df_processed['gbclose'].rolling(window=20).mean()
            df_processed['deviation_20d'] = (df_processed['gbclose'] - df_processed['ma_20d']) / df_processed['ma_20d']
            
        if 'deviation_50d' not in df_processed.columns and 'gbclose' in df_processed.columns:
            if 'ma_50d' not in df_processed.columns:
                df_processed['ma_50d'] = df_processed['gbclose'].rolling(window=50).mean()
            df_processed['deviation_50d'] = (df_processed['gbclose'] - df_processed['ma_50d']) / df_processed['ma_50d']
        
        # Drop NaN values
        df_processed = df_processed.dropna()
        
        # Extract features
        X = df_processed[features].values
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        return X_scaled, df_processed
    
    def train(self, df, batch_size=64, epochs=100, lr=0.001, augmentation_strength=0.1):
        """
        Train the model on the given data.
        
        Parameters:
        df (pd.DataFrame): Input dataframe with market data
        batch_size (int): Batch size for training
        epochs (int): Number of training epochs
        lr (float): Learning rate
        augmentation_strength (float): Strength of data augmentation
        
        Returns:
        dict: Training history
        """
        # Prepare data
        X_scaled, df_processed = self.prepare_data(df)
        
        # Update input dimension if needed
        if X_scaled.shape[1] != self.input_dim:
            self.input_dim = X_scaled.shape[1]
            self.model = MarketRegimeTransformer(input_dim=self.input_dim, hidden_dim=self.hidden_dim).to(device)
        
        # Create dataset and dataloader
        dataset = FinancialTimeSeriesDataset(
            X_scaled,
            seq_length=self.seq_length,
            augmentation_strength=augmentation_strength
        )
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # Define loss function and optimizer
        criterion = NTXentLoss(temperature=0.5)
        optimizer = optim.Adam(self.model.parameters(), lr=lr)
        
        # Training loop
        self.model.train()
        history = {'loss': []}
        
        for epoch in range(epochs):
            total_loss = 0
            progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
            
            for batch_idx, (orig_seqs, aug_seqs) in enumerate(progress_bar):
                # Move to device
                orig_seqs = orig_seqs.to(device)
                aug_seqs = aug_seqs.to(device)
                
                # Zero the gradients
                optimizer.zero_grad()
                
                # Forward pass
                z_i = self.model(orig_seqs)
                z_j = self.model(aug_seqs)
                
                # Compute loss
                loss = criterion(z_i, z_j)
                
                # Backward and optimize
                loss.backward()
                optimizer.step()
                
                # Update statistics
                total_loss += loss.item()
                progress_bar.set_description(f"Epoch {epoch+1}/{epochs}: {loss.item():.4f}")
            
            # End of epoch
            avg_loss = total_loss / len(dataloader)
            history['loss'].append(avg_loss)
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
        
        # After training, identify regimes
        self._identify_regimes(X_scaled, df_processed)
        
        self.is_trained = True
        
        return history
    
    def _identify_regimes(self, X_scaled, df_processed, n_regimes=3):
        """
        Identify market regimes using the trained model.
        
        Parameters:
        X_scaled (np.array): Scaled feature matrix
        df_processed (pd.DataFrame): Processed dataframe
        n_regimes (int): Number of regimes to identify
        """
        # Create a dataset without augmentation for feature extraction
        dataset = FinancialTimeSeriesDataset(
            X_scaled,
            seq_length=self.seq_length,
            augmentation_strength=0
        )
        
        # Extract features
        self.model.eval()
        features = []
        
        with torch.no_grad():
            for idx in range(len(dataset)):
                sequence, _ = dataset[idx]
                sequence = sequence.unsqueeze(0).to(device)
                feature = self.model(sequence, return_features=True)
                features.append(feature.cpu().numpy())
        
        # Convert to numpy array
        features = np.concatenate(features, axis=0)
        
        # Apply KMeans to identify regimes
        self.kmeans = KMeans(n_clusters=n_regimes, random_state=42)
        cluster_labels = self.kmeans.fit_predict(features)
        
        # Add regime labels to dataframe (after offset)
        df_regime = df_processed.iloc[self.seq_length-1:].copy()
        df_regime['regime'] = cluster_labels
        
        # Interpret regimes
        self._interpret_regimes(df_regime)
    
    def _interpret_regimes(self, df_regime):
        """
        Interpret the regimes by analyzing their characteristics.
        
        Parameters:
        df_regime (pd.DataFrame): Dataframe with regime predictions
        """
        # Calculate regime characteristics
        n_regimes = len(df_regime['regime'].unique())
        regime_features = {}
        
        for i in range(n_regimes):
            regime_mask = df_regime['regime'] == i
            
            if regime_mask.sum() == 0:
                continue
                
            regime_features[i] = {
                'mean_return': df_regime[regime_mask]['returns'].mean(),
                'volatility': df_regime[regime_mask]['volatility_20d'].mean(),
                'momentum': df_regime[regime_mask]['momentum_20d'].mean(),
                'mean_reversion': df_regime[regime_mask]['deviation_20d'].mean(),
                'count': regime_mask.sum()
            }
        
        # Assign labels based on characteristics
        self.regime_labels = {}
        
        # Sort regimes by volatility (highest first)
        sorted_by_volatility = sorted(
            regime_features.keys(),
            key=lambda x: regime_features[x]['volatility'],
            reverse=True
        )
        
        # Most volatile regime is labeled as 'Volatile'
        self.regime_labels[sorted_by_volatility[0]] = 'Volatile'
        
        # Remaining regimes are sorted by momentum
        remaining_regimes = sorted_by_volatility[1:]
        sorted_by_momentum = sorted(
            remaining_regimes,
            key=lambda x: abs(regime_features[x]['momentum']),
            reverse=True
        )
        
        if len(sorted_by_momentum) >= 2:
            # Highest momentum (in absolute terms) is 'Trending'
            self.regime_labels[sorted_by_momentum[0]] = 'Trending'
            
            # Lowest momentum (or remaining) is 'Mean-Reverting'
            for i in range(1, len(sorted_by_momentum)):
                self.regime_labels[sorted_by_momentum[i]] = 'Mean-Reverting'
        elif len(sorted_by_momentum) == 1:
            # If only one regime left, check if it's trending or mean-reverting
            regime = sorted_by_momentum[0]
            if abs(regime_features[regime]['momentum']) > abs(regime_features[regime]['mean_reversion']):
                self.regime_labels[regime] = 'Trending'
            else:
                self.regime_labels[regime] = 'Mean-Reverting'
    
    def predict(self, data):
        """
        Predict regimes for new data.
        
        Parameters:
        data (pd.DataFrame or np.ndarray): Input data
        
        Returns:
        np.ndarray: Predicted regimes
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before prediction")
        
        # If DataFrame, extract features and handle missing values
        if isinstance(data, pd.DataFrame):
            # Create a copy to avoid modifying the original
            df = data.copy()
            
            # Ensure we have returns
            if 'returns' not in df.columns and 'gbclose' in df.columns:
                df['returns'] = df['gbclose'].pct_change()
            
            # Initialize predictions array with NaNs
            regime_predictions = np.full(len(df), np.nan)
            
            # Create empty feature array to collect valid features
            valid_indices = []
            features_list = []
            
            # Use a sliding window approach for prediction
            window_size = 20  # Adjust based on the transformer's expected input size
            
            for i in range(window_size, len(df)):
                window_data = df.iloc[i-window_size:i].copy()
                
                # Skip if any NaN in the window
                if window_data['returns'].isna().any():
                    continue
                
                # Extract features from this window
                window_features = self._prepare_features_for_inference(window_data)
                
                if window_features is not None:
                    valid_indices.append(i)
                    features_list.append(window_features)
            
            # If we have valid windows, predict regimes
            if features_list:
                # Convert to tensor
                features_tensor = torch.stack(features_list)
                
                # Get predictions
                with torch.no_grad():
                    self.model.eval()
                    outputs = self.model(features_tensor)
                    _, predictions = torch.max(outputs, 1)
                
                # Map predictions to regime labels
                for idx, pred in zip(valid_indices, predictions.numpy()):
                    regime_predictions[idx] = pred
                
                # Fill NaN values with forward fill
                for i in range(1, len(regime_predictions)):
                    if np.isnan(regime_predictions[i]):
                        regime_predictions[i] = regime_predictions[i-1]
                
                # Handle the first elements that might be NaN
                if np.isnan(regime_predictions[0]):
                    # Find first non-NaN value
                    first_valid = np.where(~np.isnan(regime_predictions))[0]
                    if len(first_valid) > 0:
                        regime_predictions[:first_valid[0]] = regime_predictions[first_valid[0]]
                    else:
                        # All NaN case - unlikely but handle it
                        regime_predictions[:] = 0
            else:
                # If no valid windows, return default regime
                regime_predictions[:] = 0
            
            return regime_predictions.astype(int)
        else:
            # For numpy array input
            # Implement based on your transformer's requirements
            raise NotImplementedError("Prediction for numpy array inputs not implemented")
    
    def _prepare_features_for_inference(self, window_data):
        """
        Prepare features for inference from a window of data.
        
        Parameters:
        window_data (pd.DataFrame): Window of data
        
        Returns:
        torch.Tensor: Prepared features tensor
        """
        try:
            # Calculate features similar to training
            features = []
            
            # Add returns
            returns = window_data['returns'].values
            features.append(returns)
            
            # Add volatility
            volatility = np.array([np.std(returns[-i:]) for i in range(1, 6)])
            features.append(volatility)
            
            # Add momentum (moving averages)
            for window in [5, 10, 20]:
                if len(returns) >= window:
                    momentum = np.array([np.mean(returns[-min(i, window):]) for i in range(1, 6)])
                    features.append(momentum)
                else:
                    # Use smaller window if not enough data
                    momentum = np.array([np.mean(returns[-min(i, len(returns)):]) for i in range(1, 6)])
                    features.append(momentum)
            
            # Flatten and convert to tensor
            features_flat = np.concatenate(features)
            features_tensor = torch.FloatTensor(features_flat)
            
            return features_tensor
        except Exception as e:
            print(f"Error preparing features: {str(e)}")
            return None
    
    def get_regime_labels(self, regimes):
        """
        Map regime IDs to labels.
        
        Parameters:
        regimes (np.array): Array of regime IDs
        
        Returns:
        list: List of regime labels
        """
        if self.regime_labels is None:
            raise ValueError("Regime labels not available. Model must be trained first.")
        
        return [self.regime_labels.get(r, "Unknown") for r in regimes]
    
    def plot_regimes(self, df):
        """
        Plot the regimes overlaid on price.
        
        Parameters:
        df (pd.DataFrame): Input dataframe with market data
        """
        if not self.is_trained or self.regime_labels is None:
            raise ValueError("Model must be trained before plotting")
        
        # Prepare data
        _, df_processed = self.prepare_data(df)
        
        # Get regime predictions
        regimes = self.predict(df)
        
        # Create a new dataframe with regimes
        df_plot = df_processed.copy()
        df_plot['regime'] = regimes
        df_plot['regime_label'] = [self.regime_labels.get(r, "Unknown") for r in regimes]
        
        # Plot
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True)
        
        # Plot price in the top subplot
        ax1.plot(df_plot.index, df_plot['gbclose'], color='black')
        ax1.set_title('Price and Transformer Regimes', fontsize=16)
        ax1.set_ylabel('Price', fontsize=14)
        ax1.grid(True, alpha=0.3)
        
        # Define colors for regimes
        colors = {'Volatile': 'red', 'Trending': 'green', 'Mean-Reverting': 'blue', 'Unknown': 'gray'}
        
        # Plot regimes in the bottom subplot
        for regime_id, label in self.regime_labels.items():
            mask = df_plot['regime'] == regime_id
            mask_indices = df_plot.index[mask]
            
            # Create a color series for this regime
            color_series = pd.Series(1, index=mask_indices)
            
            # Plot as colored area
            ax2.fill_between(mask_indices, 0, color_series, alpha=0.7, color=colors[label], label=label)
        
        ax2.set_title('Transformer Market Regimes', fontsize=14)
        ax2.set_xlabel('Date', fontsize=14)
        ax2.set_ylabel('Regime', fontsize=12)
        ax2.legend(loc='upper right')
        ax2.grid(False)
        
        # Format x-axis dates
        ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.show()
    
    def save(self, directory='models'):
        """
        Save the trained model to disk.
        
        Parameters:
        directory (str): Directory to save the model
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before saving")
        
        # Create directory if it doesn't exist
        if not os.path.exists(directory):
            os.makedirs(directory)
        
        # Save model weights
        torch.save(self.model.state_dict(), os.path.join(directory, 'transformer_model.pth'))
        
        # Save KMeans model
        joblib.dump(self.kmeans, os.path.join(directory, 'transformer_kmeans.pkl'))
        
        # Save regime labels
        joblib.dump(self.regime_labels, os.path.join(directory, 'transformer_regime_labels.pkl'))
        
        # Save model parameters for reconstruction
        params = {
            'input_dim': self.input_dim,
            'hidden_dim': self.hidden_dim,
            'seq_length': self.seq_length
        }
        joblib.dump(params, os.path.join(directory, 'transformer_params.pkl'))
        
        # Save scaler
        joblib.dump(self.scaler, os.path.join(directory, 'transformer_scaler.pkl'))
    
    def load(self, directory='models'):
        """
        Load a trained model from disk.
        
        Parameters:
        directory (str): Directory where the model is saved
        
        Returns:
        bool: True if loading successful, False otherwise
        """
        try:
            # Check if files exist
            params_path = os.path.join(directory, 'transformer_params.pkl')
            model_path = os.path.join(directory, 'transformer_model.pth')
            kmeans_path = os.path.join(directory, 'transformer_kmeans.pkl')
            labels_path = os.path.join(directory, 'transformer_regime_labels.pkl')
            scaler_path = os.path.join(directory, 'transformer_scaler.pkl')
            
            if not os.path.exists(model_path) or not os.path.exists(kmeans_path):
                return False
            
            # Load params if available
            if os.path.exists(params_path):
                params = joblib.load(params_path)
                self.input_dim = params['input_dim']
                self.hidden_dim = params['hidden_dim']
                self.seq_length = params['seq_length']
            
            # Create model with loaded parameters
            self.model = MarketRegimeTransformer(
                input_dim=self.input_dim,
                hidden_dim=self.hidden_dim
            ).to(device)
            
            # Load model weights
            self.model.load_state_dict(torch.load(model_path, map_location=device))
            self.model.eval()
            
            # Load KMeans model
            self.kmeans = joblib.load(kmeans_path)
            
            # Load regime labels
            if os.path.exists(labels_path):
                self.regime_labels = joblib.load(labels_path)
            
            # Load scaler
            if os.path.exists(scaler_path):
                self.scaler = joblib.load(scaler_path)
            
            self.is_trained = True
            return True
        
        except Exception as e:
            print(f"Error loading model: {str(e)}")
            return False


def train_transformer_model(df, seq_length=20, batch_size=64, epochs=100, lr=0.001):
    """
    Train a transformer model on the given data.
    
    Parameters:
    df (pd.DataFrame): Input dataframe with market data
    seq_length (int): Sequence length for each sample
    batch_size (int): Batch size for training
    epochs (int): Number of training epochs
    lr (float): Learning rate
    
    Returns:
    TransformerMarketModel: Trained model
    """
    # Create and train the model
    model = TransformerMarketModel(seq_length=seq_length)
    model.train(df, batch_size=batch_size, epochs=epochs, lr=lr)
    
    return model


def load_transformer_model(directory='models'):
    """
    Load a pre-trained transformer model.
    
    Parameters:
    directory (str): Directory where the model is saved
    
    Returns:
    TransformerMarketModel: Loaded model, or None if loading failed
    """
    model = TransformerMarketModel()
    success = model.load(directory=directory)
    
    if success:
        return model
    else:
        return None

================
File: position_sizing/base.py
================
"""
Reinforcement Learning-based Dynamic Position Sizing module.

This module provides functionality for dynamically sizing trading positions
based on meta-labeling confidence, market regimes, and transaction costs.
The primary goal is to reduce unnecessary position changes that incur transaction costs.
"""

import numpy as np
import pandas as pd
from collections import deque
import pickle
import os

class RLPositionSizer:
    """
    Reinforcement Learning agent for dynamic position sizing
    that aims to minimize transaction costs while maximizing returns.
    """
    
    def __init__(self, n_states=10, n_actions=3, learning_rate=0.1, 
                 discount_factor=0.95, exploration_rate=1.0, 
                 exploration_decay=0.995, min_exploration_rate=0.01,
                 smoothing_alpha=0.2, transaction_cost=0.001, 
                 state_window=5):
        """
        Initialize the RL Position Sizer.
        
        Parameters:
        -----------
        n_states : int
            Number of discretized states for the Q-table
        n_actions : int
            Number of possible actions (e.g., increase, decrease, maintain position size)
        learning_rate : float
            Learning rate for Q-learning
        discount_factor : float
            Discount factor for future rewards
        exploration_rate : float
            Initial exploration rate
        exploration_decay : float
            Rate at which exploration decreases
        min_exploration_rate : float
            Minimum exploration rate
        smoothing_alpha : float
            EMA smoothing factor for position size changes
        transaction_cost : float
            Cost per unit of position size change (as a fraction)
        state_window : int
            Number of past returns and volatility observations to include in state
        """
        self.n_states = n_states
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.exploration_decay = exploration_decay
        self.min_exploration_rate = min_exploration_rate
        self.smoothing_alpha = smoothing_alpha
        self.transaction_cost = transaction_cost
        self.state_window = state_window
        
        # Initialize Q-table: states × actions
        self.q_table = np.zeros((n_states, n_states, n_states, n_actions))
        
        # History of returns and volatility for state representation
        self.returns_history = deque(maxlen=state_window)
        self.volatility_history = deque(maxlen=state_window)
        self.position_history = deque(maxlen=state_window)
        
        # Track current position size
        self.current_position_size = 0.0
        
        # For tracking performance
        self.total_reward = 0.0
        self.total_transaction_cost = 0.0
        self.episode_count = 0

    def discretize_state(self, confidence, regime, returns, volatility, position):
        """
        Convert continuous state values to discrete states for the Q-table.
        
        Parameters:
        -----------
        confidence : float
            Meta-labeling model confidence (0-1)
        regime : int
            Market regime identifier
        returns : list
            Recent returns history
        volatility : list
            Recent volatility history
        position : list
            Recent position history
            
        Returns:
        --------
        tuple: Discretized state indices
        """
        # Discretize confidence into n_states buckets
        conf_state = min(int(confidence * self.n_states), self.n_states - 1)
        
        # Use regime directly as one dimension of the state - ensure it's an integer
        try:
            regime_state = min(int(regime), self.n_states - 1)
        except (ValueError, TypeError):
            # Handle case where regime might be a float or other non-integer value
            regime_state = 0
            print(f"Warning: Invalid regime value {regime}, using default 0")
        
        # Calculate trend strength from returns and volatility
        if len(returns) > 0 and len(volatility) > 0:
            avg_return = np.mean(returns)
            avg_vol = np.mean(volatility)
            if avg_vol == 0:  # Avoid division by zero
                trend_strength = 0
            else:
                # Sharpe-like ratio as trend strength
                trend_strength = avg_return / avg_vol
                
            # Normalize and discretize trend strength
            trend_state = int((np.clip(trend_strength + 2, 0, 4) / 4) * self.n_states)
            trend_state = min(trend_state, self.n_states - 1)
        else:
            trend_state = 0
            
        return int(conf_state), int(regime_state), int(trend_state)

    def get_action(self, state, training=True):
        """
        Select an action using epsilon-greedy policy.
        
        Parameters:
        -----------
        state : tuple
            Current discretized state
        training : bool
            Whether to use exploration or exploitation
            
        Returns:
        --------
        int: Selected action index
        """
        if training and np.random.random() < self.exploration_rate:
            # Exploration: select random action
            return np.random.randint(self.n_actions)
        else:
            # Exploitation: select best action from Q-table
            try:
                # Unpack state tuple for proper indexing
                conf_state, regime_state, trend_state = state
                return np.argmax(self.q_table[conf_state, regime_state, trend_state])
            except Exception as e:
                print(f"Error getting action: {e}")
                print(f"State: {state}")
                # Return middle action (maintain position) if error occurs
                return 1

    def update_q_table(self, state, action, reward, next_state):
        """
        Update Q-table using Q-learning algorithm.
        
        Parameters:
        -----------
        state : tuple
            Current state
        action : int
            Chosen action
        reward : float
            Reward received
        next_state : tuple
            Next state after taking action
        """
        # Q-learning formula
        try:
            # Use state indices individually instead of as a tuple
            conf_state, regime_state, trend_state = state
            next_conf_state, next_regime_state, next_trend_state = next_state
            
            # Find best next action
            best_next_action = np.argmax(self.q_table[next_conf_state, next_regime_state, next_trend_state])
            
            # Calculate TD target and error
            td_target = reward + self.discount_factor * self.q_table[next_conf_state, next_regime_state, next_trend_state, best_next_action]
            td_error = td_target - self.q_table[conf_state, regime_state, trend_state, action]
            
            # Update Q-value
            self.q_table[conf_state, regime_state, trend_state, action] += self.learning_rate * td_error
        except Exception as e:
            print(f"Error updating Q-table: {e}")
            print(f"State: {state}, Action: {action}, Next State: {next_state}")
            # Continue without updating if error occurs
        
        # Decay exploration rate
        self.exploration_rate = max(
            self.min_exploration_rate, 
            self.exploration_rate * self.exploration_decay
        )

    def calculate_reward(self, profit, size_change):
        """
        Calculate reward based on profit and transaction costs.
        
        Parameters:
        -----------
        profit : float
            Trading profit
        size_change : float
            Absolute change in position size
            
        Returns:
        --------
        float: Net reward after transaction costs
        """
        # Calculate transaction cost
        cost = abs(size_change) * self.transaction_cost
        
        # Track total transaction cost
        self.total_transaction_cost += cost
        
        # Net reward
        net_reward = profit - cost
        self.total_reward += net_reward
        
        return net_reward

    def smooth_position(self, new_size):
        """
        Apply EMA smoothing to position size changes.
        
        Parameters:
        -----------
        new_size : float
            New position size
            
        Returns:
        --------
        float: Smoothed position size
        """
        return self.smoothing_alpha * new_size + (1 - self.smoothing_alpha) * self.current_position_size

    def size_to_action(self, confidence):
        """
        Convert confidence to action space.
        
        Parameters:
        -----------
        confidence : float
            Meta-labeling confidence (0-1)
            
        Returns:
        --------
        float: Position size multiplier (-1 to 1)
        """
        # Map action index to position size multiplier
        action_map = {
            0: -1.0,  # Full short position
            1: 0.0,   # No position
            2: 1.0    # Full long position
        }
        
        # Get current state
        state = self.get_current_state(confidence)
        
        # Get action based on state
        action = self.get_action(state)
        
        # Get size multiplier from action map
        size_multiplier = action_map[action]
        
        return size_multiplier

    def get_current_state(self, confidence, regime=0):
        """
        Get the current state for decision making.
        
        Parameters:
        -----------
        confidence : float
            Meta-labeling confidence (0-1)
        regime : int
            Market regime identifier
            
        Returns:
        --------
        tuple: Current state tuple
        """
        # Default values if history is empty
        if not self.returns_history:
            self.returns_history.extend([0.0] * self.state_window)
        if not self.volatility_history:
            self.volatility_history.extend([0.0] * self.state_window)
        if not self.position_history:
            self.position_history.extend([0.0] * self.state_window)
            
        # Discretize current state
        return self.discretize_state(
            confidence, 
            regime,
            list(self.returns_history),
            list(self.volatility_history),
            list(self.position_history)
        )

    def update_state_history(self, return_value, volatility, position):
        """
        Update the history of returns and volatility.
        
        Parameters:
        -----------
        return_value : float
            Latest return
        volatility : float
            Latest volatility measure
        position : float
            Latest position size
        """
        self.returns_history.append(return_value)
        self.volatility_history.append(volatility)
        self.position_history.append(position)

    def decide_position_size(self, confidence, regime, return_value, volatility, base_size=1.0):
        """
        Decide the position size based on current state and policy.
        
        Parameters:
        -----------
        confidence : float
            Meta-labeling confidence (0-1)
        regime : int
            Market regime identifier
        return_value : float
            Latest return
        volatility : float
            Latest volatility measure
        base_size : float
            Base position size (multiplier)
            
        Returns:
        --------
        float: New position size
        """
        # Update state history
        self.update_state_history(return_value, volatility, self.current_position_size)
        
        # Get current state
        state = self.get_current_state(confidence, regime)
        
        # Get action based on state
        action = self.get_action(state)
        
        # Map action to position size multiplier
        if action == 0:
            # Decrease position size
            new_position = max(-1.0, self.current_position_size - 0.25) * base_size
        elif action == 1:
            # Maintain position size
            new_position = self.current_position_size * base_size
        else:  # action == 2
            # Increase position size
            new_position = min(1.0, self.current_position_size + 0.25) * base_size
        
        # Calculate size change and reward
        size_change = new_position - self.current_position_size
        position_sign = 1 if self.current_position_size > 0 else (-1 if self.current_position_size < 0 else 0)
        profit = position_sign * return_value * abs(self.current_position_size)
        reward = self.calculate_reward(profit, size_change)
        
        # Apply smoothing to position size
        smoothed_position = self.smooth_position(new_position)
        
        # Update Q-table if in training mode
        next_state = self.get_current_state(confidence, regime)
        self.update_q_table(state, action, reward, next_state)
        
        # Update current position
        self.current_position_size = smoothed_position
        
        return smoothed_position

    def train(self, data, confidence_col, regime_col, returns_col, volatility_col='atr', 
              episodes=100, base_size=1.0):
        """
        Train the RL agent on historical data.
        
        Parameters:
        -----------
        data : DataFrame
            Historical data with required columns
        confidence_col : str
            Column name for meta-labeling confidence
        regime_col : str
            Column name for market regime
        returns_col : str
            Column name for returns
        volatility_col : str
            Column name for volatility
        episodes : int
            Number of training episodes
        base_size : float
            Base position size
            
        Returns:
        --------
        DataFrame: Performance statistics from training
        """
        performance_stats = []
        
        for episode in range(episodes):
            # Reset environment
            self.current_position_size = 0.0
            self.total_reward = 0.0
            self.total_transaction_cost = 0.0
            self.returns_history.clear()
            self.volatility_history.clear()
            self.position_history.clear()
            
            # Reset exploration rate at the beginning of each episode
            if episode == 0:
                self.exploration_rate = 1.0
            else:
                self.exploration_rate = max(
                    self.min_exploration_rate,
                    self.exploration_rate * self.exploration_decay
                )
            
            cum_return = 0.0
            positions = []
            
            # Iterate through each timestep
            for i in range(len(data)):
                row = data.iloc[i]
                confidence = row[confidence_col]
                regime = row[regime_col]
                ret = row[returns_col]
                vol = row[volatility_col]
                
                # Get position size
                position = self.decide_position_size(confidence, regime, ret, vol, base_size)
                positions.append(position)
                
                # Calculate period return (simplified)
                period_return = position * ret
                cum_return += period_return
            
            # Record performance
            performance_stats.append({
                'episode': episode,
                'final_reward': self.total_reward,
                'transaction_costs': self.total_transaction_cost,
                'cumulative_return': cum_return,
                'sharpe_ratio': cum_return / (np.std(positions) + 1e-6)
            })
            
            self.episode_count += 1
            
            # Print progress
            if (episode + 1) % 10 == 0:
                print(f"Episode {episode+1}/{episodes} - "
                      f"Reward: {self.total_reward:.4f}, "
                      f"Return: {cum_return:.4f}, "
                      f"Costs: {self.total_transaction_cost:.4f}")
        
        return pd.DataFrame(performance_stats)

    def apply(self, data, confidence_col, regime_col=None, returns_col=None, 
              volatility_col=None, base_size=1.0):
        """
        Apply the trained RL agent to new data.
        
        Parameters:
        -----------
        data : DataFrame
            New data with required columns
        confidence_col : str
            Column name for meta-labeling confidence
        regime_col : str
            Column name for market regime
        returns_col : str
            Column name for returns
        volatility_col : str
            Column name for volatility
        base_size : float
            Base position size
            
        Returns:
        --------
        DataFrame: Original data with position sizes added
        """
        # Make a copy to avoid modifying the original
        result = data.copy()
        result['rl_position_size'] = 0.0
        
        # Reset position size
        self.current_position_size = 0.0
        
        # Apply the model to each row (no exploration in application)
        for i in range(len(result)):
            row = result.iloc[i]
            confidence = row[confidence_col]
            
            # Use default values if columns not provided
            regime = row[regime_col] if regime_col and regime_col in row else 0
            ret = row[returns_col] if returns_col and returns_col in row else 0
            vol = row[volatility_col] if volatility_col and volatility_col in row else 0.01
            
            # Get position size without updating Q-table
            state = self.get_current_state(confidence, regime)
            action = self.get_action(state, training=False)
            
            # Map action to position size adjustments
            if action == 0:
                new_position = max(-1.0, self.current_position_size - 0.25) * base_size
            elif action == 1:
                new_position = self.current_position_size * base_size
            else:  # action == 2
                new_position = min(1.0, self.current_position_size + 0.25) * base_size
            
            # Apply smoothing
            smoothed_position = self.smooth_position(new_position)
            self.current_position_size = smoothed_position
            
            # Update state history
            self.update_state_history(ret, vol, smoothed_position)
            
            # Store position size
            result.loc[result.index[i], 'rl_position_size'] = smoothed_position
        
        return result

    def save(self, filepath):
        """
        Save the trained model to a file.
        
        Parameters:
        -----------
        filepath : str
            Path to save the model
        """
        model_data = {
            'q_table': self.q_table,
            'n_states': self.n_states,
            'n_actions': self.n_actions,
            'learning_rate': self.learning_rate,
            'discount_factor': self.discount_factor,
            'exploration_rate': self.exploration_rate,
            'min_exploration_rate': self.min_exploration_rate,
            'smoothing_alpha': self.smoothing_alpha,
            'transaction_cost': self.transaction_cost,
            'state_window': self.state_window,
            'episode_count': self.episode_count
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
            
        print(f"Model saved to {filepath}")

    @classmethod
    def load(cls, filepath):
        """
        Load a trained model from a file.
        
        Parameters:
        -----------
        filepath : str
            Path to the saved model
            
        Returns:
        --------
        RLPositionSizer: Loaded model
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        # Create a new instance
        sizer = cls(
            n_states=model_data['n_states'],
            n_actions=model_data['n_actions'],
            learning_rate=model_data['learning_rate'],
            discount_factor=model_data['discount_factor'],
            exploration_rate=model_data['exploration_rate'],
            min_exploration_rate=model_data['min_exploration_rate'],
            smoothing_alpha=model_data['smoothing_alpha'],
            transaction_cost=model_data['transaction_cost'],
            state_window=model_data['state_window']
        )
        
        # Load the Q-table and other saved attributes
        sizer.q_table = model_data['q_table']
        sizer.episode_count = model_data['episode_count']
        
        return sizer


def calculate_transaction_costs(positions, transaction_cost=0.001):
    """
    Calculate transaction costs from a series of positions.
    
    Parameters:
    -----------
    positions : array-like
        Series of position sizes
    transaction_cost : float
        Cost per unit of position size change
        
    Returns:
    --------
    float: Total transaction costs
    """
    if len(positions) <= 1:
        return 0.0
    
    # Calculate absolute differences between consecutive positions
    position_changes = np.abs(np.diff(positions))
    
    # Calculate transaction costs
    total_cost = np.sum(position_changes) * transaction_cost
    
    return total_cost


def evaluate_rl_strategy(data, base_position_col, rl_position_col, returns_col, 
                         transaction_cost=0.001):
    """
    Evaluate the RL strategy against the base strategy.
    
    Parameters:
    -----------
    data : DataFrame
        Data with positions and returns
    base_position_col : str
        Column name for base strategy positions
    rl_position_col : str
        Column name for RL strategy positions
    returns_col : str
        Column name for returns
    transaction_cost : float
        Cost per unit of position size change
        
    Returns:
    --------
    dict: Performance metrics
    """
    # Make a copy of the data
    result = data.copy()
    
    # Calculate returns for both strategies (before transaction costs)
    result['base_return'] = result[base_position_col] * result[returns_col]
    result['rl_return'] = result[rl_position_col] * result[returns_col]
    
    # Calculate cumulative returns
    result['base_cum_return'] = result['base_return'].cumsum()
    result['rl_cum_return'] = result['rl_return'].cumsum()
    
    # Calculate transaction costs
    base_costs = calculate_transaction_costs(result[base_position_col].values, transaction_cost)
    rl_costs = calculate_transaction_costs(result[rl_position_col].values, transaction_cost)
    
    # Adjust final returns for transaction costs
    base_net_return = result['base_cum_return'].iloc[-1] - base_costs
    rl_net_return = result['rl_cum_return'].iloc[-1] - rl_costs
    
    # Calculate performance metrics
    base_sharpe = result['base_return'].mean() / (result['base_return'].std() + 1e-6) * np.sqrt(252)
    rl_sharpe = result['rl_return'].mean() / (result['rl_return'].std() + 1e-6) * np.sqrt(252)
    
    # Calculate average position size and turnover
    base_avg_size = np.mean(np.abs(result[base_position_col]))
    rl_avg_size = np.mean(np.abs(result[rl_position_col]))
    
    base_turnover = np.sum(np.abs(np.diff(result[base_position_col]))) / len(result)
    rl_turnover = np.sum(np.abs(np.diff(result[rl_position_col]))) / len(result)
    
    metrics = {
        'base_return': base_net_return,
        'rl_return': rl_net_return,
        'improvement': rl_net_return - base_net_return,
        'base_costs': base_costs,
        'rl_costs': rl_costs,
        'cost_reduction': base_costs - rl_costs,
        'base_sharpe': base_sharpe,
        'rl_sharpe': rl_sharpe,
        'base_avg_size': base_avg_size,
        'rl_avg_size': rl_avg_size,
        'base_turnover': base_turnover,
        'rl_turnover': rl_turnover
    }
    
    return metrics

================
File: position_sizing/dqn_position_sizing.py
================
"""
Deep Q-Network (DQN) based Dynamic Position Sizing module.

This module provides functionality for dynamically sizing trading positions
based on meta-labeling confidence, market regimes, and transaction costs
using a DQN reinforcement learning approach.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
from collections import deque, namedtuple
import pickle
import os
import matplotlib.pyplot as plt

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Experience replay memory
Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))

class ReplayMemory:
    """Experience replay buffer to store and sample experiences."""
    
    def __init__(self, capacity=10000):
        """
        Initialize replay memory.
        
        Parameters:
        -----------
        capacity : int
            Maximum capacity of the buffer
        """
        self.memory = deque(maxlen=capacity)
        
    def push(self, state, action, reward, next_state, done):
        """Store experience in memory."""
        self.memory.append(Experience(state, action, reward, next_state, done))
        
    def sample(self, batch_size):
        """Sample random batch from memory."""
        return random.sample(self.memory, min(len(self.memory), batch_size))
    
    def can_sample(self, batch_size):
        """Check if enough samples are available."""
        return len(self.memory) >= batch_size
    
    def __len__(self):
        return len(self.memory)


class DQN(nn.Module):
    """Deep Q-Network architecture."""
    
    def __init__(self, input_dim, output_dim, hidden_dim=64):
        """
        Initialize DQN model.
        
        Parameters:
        -----------
        input_dim : int
            Dimension of state space
        output_dim : int
            Dimension of action space
        hidden_dim : int
            Size of hidden layers
        """
        super(DQN, self).__init__()
        
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x):
        """Forward pass through network."""
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


class DQNPositionSizer:
    """
    DQN-based Reinforcement Learning agent for dynamic position sizing
    that aims to minimize transaction costs while maximizing returns.
    """
    
    def __init__(self, state_dim=12, n_actions=3, learning_rate=0.001,
                 gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, 
                 epsilon_decay=0.995, memory_size=10000, batch_size=64,
                 target_update=10, smoothing_alpha=0.2, transaction_cost=0.000002):
        """
        Initialize the DQN Position Sizer.
        
        Parameters:
        -----------
        state_dim : int
            Dimension of the state space
        n_actions : int
            Number of discrete actions
        learning_rate : float
            Learning rate for neural network
        gamma : float
            Discount factor for future rewards
        epsilon_start : float
            Initial exploration rate
        epsilon_end : float
            Final exploration rate
        epsilon_decay : float
            Rate at which exploration rate decays
        memory_size : int
            Size of replay memory
        batch_size : int
            Batch size for training
        target_update : int
            Frequency of target network updates
        smoothing_alpha : float
            EMA smoothing factor for position size changes
        transaction_cost : float
            Cost per unit of position size change
        """
        self.state_dim = state_dim
        self.n_actions = n_actions
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update = target_update
        self.smoothing_alpha = smoothing_alpha
        self.transaction_cost = transaction_cost
        
        # Initialize networks
        self.policy_net = DQN(state_dim, n_actions).to(device)
        self.target_net = DQN(state_dim, n_actions).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        
        # Initialize optimizer
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        
        # Initialize replay memory
        self.memory = ReplayMemory(memory_size)
        
        # Initialize step counter
        self.steps_done = 0
        
        # Current position size
        self.current_position_size = 0.0
        
        # For tracking performance
        self.total_reward = 0.0
        self.total_transaction_cost = 0.0
        self.episode_count = 0
        
        # Initialize action mapping
        self.action_mapping = {
            0: -0.25,  # Decrease position
            1: 0.0,    # Hold position
            2: 0.25    # Increase position
        }
        
        # Initialize state history
        self.state_history = []
        
        # Track losses
        self.losses = []
    
    def prepare_state(self, meta_signal, regime, features, position):
        """
        Prepare the state representation.
        
        Parameters:
        -----------
        meta_signal : float
            Meta-labeling confidence or signal
        regime : int
            Market regime identifier
        features : dict or list
            Technical features like RSI, volatility, etc.
        position : float
            Current position size
            
        Returns:
        --------
        torch.Tensor: State tensor
        """
        # Convert regime to one-hot encoding (assuming max 3 regimes)
        regime_onehot = [0, 0, 0]
        if regime < len(regime_onehot):
            regime_onehot[int(regime)] = 1
        
        # Combine all state components
        state = [meta_signal, position] + regime_onehot + list(features)
        
        # Normalize state (simple min-max scaling)
        state_tensor = torch.FloatTensor(state).to(device)
        
        return state_tensor
    
    def select_action(self, state, training=True):
        """
        Select action using epsilon-greedy policy.
        
        Parameters:
        -----------
        state : torch.Tensor
            Current state
        training : bool
            Whether to use exploration or exploitation
            
        Returns:
        --------
        int: Selected action index
        """
        if training and random.random() < self.epsilon:
            # Exploration: select random action
            return random.randrange(self.n_actions)
        else:
            # Exploitation: select best action from Q-network
            with torch.no_grad():
                return self.policy_net(state).max(0)[1].item()
    
    def update_network(self):
        """Update policy network using batch from replay memory."""
        if not self.memory.can_sample(self.batch_size):
            return
        
        experiences = self.memory.sample(self.batch_size)
        batch = Experience(*zip(*experiences))
        
        # Convert to tensors
        state_batch = torch.stack(batch.state)
        action_batch = torch.tensor(batch.action, device=device, dtype=torch.long).unsqueeze(1)
        reward_batch = torch.tensor(batch.reward, device=device, dtype=torch.float)
        
        # Handle non-final states
        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), 
                                     device=device, dtype=torch.bool)
        non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])
        
        # Compute Q values
        q_values = self.policy_net(state_batch).gather(1, action_batch)
        
        # Compute target Q values
        next_q_values = torch.zeros(self.batch_size, device=device)
        if len(non_final_next_states) > 0:
            with torch.no_grad():
                next_q_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]
        
        target_q_values = reward_batch + self.gamma * next_q_values
        
        # Compute loss
        loss = F.smooth_l1_loss(q_values, target_q_values.unsqueeze(1))
        self.losses.append(loss.item())
        
        # Optimize the model
        self.optimizer.zero_grad()
        loss.backward()
        # Clip gradients to stabilize training
        for param in self.policy_net.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()
        
        # Update target network if needed
        if self.steps_done % self.target_update == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())
        
        # Update epsilon
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
    
    def calculate_reward(self, profit, size_change):
        """
        Calculate reward based on profit and transaction costs.
        
        Parameters:
        -----------
        profit : float
            Trading profit
        size_change : float
            Absolute change in position size
            
        Returns:
        --------
        float: Net reward after transaction costs
        """
        # Calculate transaction cost
        cost = abs(size_change) * self.transaction_cost
        
        # Track total transaction cost
        self.total_transaction_cost += cost
        
        # Net reward
        net_reward = profit - cost
        self.total_reward += net_reward
        
        return net_reward
    
    def smooth_position(self, new_size):
        """
        Apply EMA smoothing to position size changes.
        
        Parameters:
        -----------
        new_size : float
            New position size
            
        Returns:
        --------
        float: Smoothed position size
        """
        return self.smoothing_alpha * new_size + (1 - self.smoothing_alpha) * self.current_position_size
    
    def decide_position_size(self, meta_signal, regime, features, return_value, base_size=1.0, training=True):
        """
        Decide the position size based on current state and policy.
        
        Parameters:
        -----------
        meta_signal : float
            Meta-labeling signal (-1 to 1)
        regime : int
            Market regime identifier
        features : dict or list
            Technical features
        return_value : float
            Latest return
        base_size : float
            Base position size multiplier
        training : bool
            Whether in training mode
            
        Returns:
        --------
        float: New position size
        """
        # Prepare state
        state = self.prepare_state(meta_signal, regime, features, self.current_position_size)
        
        # Store state for history
        self.state_history.append(state.cpu().numpy())
        
        # Select action
        action = self.select_action(state, training)
        
        # Calculate position change
        position_change = self.action_mapping[action]
        
        # Calculate new position
        target_position = np.clip(self.current_position_size + position_change, -1.0, 1.0) * base_size
        
        # Calculate size change and reward
        size_change = target_position - self.current_position_size
        position_sign = 1 if self.current_position_size > 0 else (-1 if self.current_position_size < 0 else 0)
        profit = position_sign * return_value * abs(self.current_position_size)
        reward = self.calculate_reward(profit, size_change)
        
        # Apply smoothing to position size
        smoothed_position = self.smooth_position(target_position)
        
        # Store transition in replay memory if in training
        if training:
            # Prepare next state
            next_state = self.prepare_state(meta_signal, regime, features, smoothed_position)
            done = False  # In this context, episodes don't really end
            
            # Store in memory
            self.memory.push(state, action, reward, next_state, done)
            
            # Update networks
            self.update_network()
            
            # Increment steps
            self.steps_done += 1
        
        # Update current position
        self.current_position_size = smoothed_position
        
        return smoothed_position
    
    def train(self, data, meta_col, regime_col, features_cols, returns_col, 
              episodes=100, base_size=1.0, eval_interval=10):
        """
        Train the DQN agent on historical data.
        
        Parameters:
        -----------
        data : DataFrame
            Historical data with required columns
        meta_col : str
            Column name for meta-labeling signal
        regime_col : str
            Column name for market regime
        features_cols : list
            List of column names for technical features
        returns_col : str
            Column name for returns
        episodes : int
            Number of training episodes
        base_size : float
            Base position size
        eval_interval : int
            Interval for evaluation during training
            
        Returns:
        --------
        DataFrame: Performance statistics from training
        """
        performance_stats = []
        
        for episode in range(episodes):
            # Reset environment
            self.current_position_size = 0.0
            self.total_reward = 0.0
            self.total_transaction_cost = 0.0
            
            # Reset epsilon at the beginning of each episode
            if episode == 0:
                self.epsilon = 1.0
            
            cum_return = 0.0
            positions = []
            
            # Iterate through each timestep
            for i in range(len(data)):
                row = data.iloc[i]
                meta_signal = row[meta_col]
                regime = row[regime_col]
                
                # Extract features
                features = row[features_cols].values
                
                # Get return
                ret = row[returns_col]
                
                # Get position size
                position = self.decide_position_size(
                    meta_signal, regime, features, ret, base_size, training=True
                )
                positions.append(position)
                
                # Calculate period return (simplified)
                if i > 0:  # Use previous position to calculate return (avoid lookahead bias)
                    period_return = positions[-2] * ret
                    cum_return += period_return
            
            # Record performance
            performance_stats.append({
                'episode': episode,
                'final_reward': self.total_reward,
                'transaction_costs': self.total_transaction_cost,
                'cumulative_return': cum_return,
                'sharpe_ratio': cum_return / (np.std(positions) + 1e-6)
            })
            
            self.episode_count += 1
            
            # Print progress
            if (episode + 1) % eval_interval == 0:
                print(f"Episode {episode+1}/{episodes} - "
                      f"Reward: {self.total_reward:.4f}, "
                      f"Return: {cum_return:.4f}, "
                      f"Costs: {self.total_transaction_cost:.4f}, "
                      f"Epsilon: {self.epsilon:.2f}")
        
        # Plot losses
        plt.figure(figsize=(10, 5))
        plt.plot(self.losses)
        plt.title('DQN Training Loss')
        plt.xlabel('Update Step')
        plt.ylabel('Loss')
        plt.savefig('dqn_training_loss.png')
        
        return pd.DataFrame(performance_stats)
    
    def apply(self, data, meta_col, regime_col, features_cols, returns_col=None, base_size=1.0):
        """
        Apply the trained DQN agent to new data.
        
        Parameters:
        -----------
        data : DataFrame
            New data with required columns
        meta_col : str
            Column name for meta-labeling signal
        regime_col : str
            Column name for market regime
        features_cols : list
            List of column names for technical features
        returns_col : str
            Column name for returns (optional)
        base_size : float
            Base position size
            
        Returns:
        --------
        DataFrame: Original data with position sizes added
        """
        # Make a copy to avoid modifying the original
        result = data.copy()
        result['dqn_position_size'] = 0.0
        
        # Reset position size
        self.current_position_size = 0.0
        
        # Apply the model to each row (no exploration)
        for i in range(len(result)):
            row = result.iloc[i]
            meta_signal = row[meta_col]
            regime = row[regime_col]
            
            # Extract features
            features = row[features_cols].values
            
            # Use default return value if column not provided
            ret = row[returns_col] if returns_col and returns_col in row else 0
            
            # Get position size without training
            position = self.decide_position_size(
                meta_signal, regime, features, ret, base_size, training=False
            )
            
            # Store position size
            result.loc[result.index[i], 'dqn_position_size'] = position
        
        return result
    
    def save(self, filepath):
        """
        Save the trained model to files.
        
        Parameters:
        -----------
        filepath : str
            Base path to save the model
        """
        # Save DQN parameters
        model_path = f"{filepath}_model.pth"
        torch.save({
            'policy_net': self.policy_net.state_dict(),
            'target_net': self.target_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'steps_done': self.steps_done,
            'epsilon': self.epsilon
        }, model_path)
        
        # Save configuration
        config_path = f"{filepath}_config.pkl"
        config = {
            'state_dim': self.state_dim,
            'n_actions': self.n_actions,
            'gamma': self.gamma,
            'epsilon_end': self.epsilon_end,
            'epsilon_decay': self.epsilon_decay,
            'batch_size': self.batch_size,
            'target_update': self.target_update,
            'smoothing_alpha': self.smoothing_alpha,
            'transaction_cost': self.transaction_cost,
            'action_mapping': self.action_mapping,
            'current_position_size': self.current_position_size,
            'total_reward': self.total_reward,
            'total_transaction_cost': self.total_transaction_cost,
            'episode_count': self.episode_count
        }
        
        with open(config_path, 'wb') as f:
            pickle.dump(config, f)
            
        print(f"Model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath, learning_rate=0.001):
        """
        Load a trained model from files.
        
        Parameters:
        -----------
        filepath : str
            Base path to load the model from
        learning_rate : float
            Learning rate for optimizer initialization
            
        Returns:
        --------
        DQNPositionSizer: Loaded model
        """
        # Load configuration
        config_path = f"{filepath}_config.pkl"
        with open(config_path, 'rb') as f:
            config = pickle.load(f)
        
        # Create instance with loaded config
        sizer = cls(
            state_dim=config['state_dim'],
            n_actions=config['n_actions'],
            gamma=config['gamma'],
            epsilon_start=config['epsilon'],
            epsilon_end=config['epsilon_end'],
            epsilon_decay=config['epsilon_decay'],
            batch_size=config['batch_size'],
            target_update=config['target_update'],
            smoothing_alpha=config['smoothing_alpha'],
            transaction_cost=config['transaction_cost'],
            learning_rate=learning_rate
        )
        
        # Set restored values
        sizer.action_mapping = config['action_mapping']
        sizer.current_position_size = config['current_position_size']
        sizer.total_reward = config['total_reward']
        sizer.total_transaction_cost = config['total_transaction_cost']
        sizer.episode_count = config['episode_count']
        
        # Load model state
        model_path = f"{filepath}_model.pth"
        checkpoint = torch.load(model_path)
        
        # Load policy and target networks
        sizer.policy_net.load_state_dict(checkpoint['policy_net'])
        sizer.target_net.load_state_dict(checkpoint['target_net'])
        sizer.optimizer.load_state_dict(checkpoint['optimizer'])
        sizer.steps_done = checkpoint['steps_done']
        sizer.epsilon = checkpoint['epsilon']
        
        return sizer


def calculate_transaction_costs(positions, transaction_cost=0.000002):
    """
    Calculate transaction costs from a series of positions.
    
    Parameters:
    -----------
    positions : array-like
        Series of position sizes
    transaction_cost : float
        Cost per unit of position size change
        
    Returns:
    --------
    float: Total transaction costs
    """
    if len(positions) <= 1:
        return 0.0
    
    # Calculate absolute differences between consecutive positions
    position_changes = np.abs(np.diff(positions))
    
    # Calculate transaction costs
    total_cost = np.sum(position_changes) * transaction_cost
    
    return total_cost


def evaluate_dqn_strategy(data, base_position_col, dqn_position_col, returns_col, 
                          transaction_cost=0.000002):
    """
    Evaluate the DQN strategy against the base strategy.
    
    Parameters:
    -----------
    data : DataFrame
        Data with positions and returns
    base_position_col : str
        Column name for base strategy positions
    dqn_position_col : str
        Column name for DQN strategy positions
    returns_col : str
        Column name for returns
    transaction_cost : float
        Cost per unit of position size change
        
    Returns:
    --------
    dict: Performance metrics
    """
    # Make a copy of the data
    result = data.copy()
    
    # Calculate returns for both strategies (before transaction costs)
    result['base_return'] = result[base_position_col].shift(1) * result[returns_col]  # Shift to avoid lookahead bias
    result['dqn_return'] = result[dqn_position_col].shift(1) * result[returns_col]    # Shift to avoid lookahead bias
    
    # Calculate cumulative returns
    result['base_cum_return'] = result['base_return'].cumsum()
    result['dqn_cum_return'] = result['dqn_return'].cumsum()
    
    # Calculate transaction costs
    base_costs = calculate_transaction_costs(result[base_position_col].values, transaction_cost)
    dqn_costs = calculate_transaction_costs(result[dqn_position_col].values, transaction_cost)
    
    # Adjust final returns for transaction costs
    base_net_return = result['base_cum_return'].iloc[-1] - base_costs
    dqn_net_return = result['dqn_cum_return'].iloc[-1] - dqn_costs
    
    # Calculate performance metrics
    base_sharpe = result['base_return'].mean() / (result['base_return'].std() + 1e-6) * np.sqrt(252)
    dqn_sharpe = result['dqn_return'].mean() / (result['dqn_return'].std() + 1e-6) * np.sqrt(252)
    
    # Calculate average position size and turnover
    base_avg_size = np.mean(np.abs(result[base_position_col]))
    dqn_avg_size = np.mean(np.abs(result[dqn_position_col]))
    
    base_turnover = np.sum(np.abs(np.diff(result[base_position_col]))) / len(result)
    dqn_turnover = np.sum(np.abs(np.diff(result[dqn_position_col]))) / len(result)
    
    # Create performance chart
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 1, 1)
    plt.plot(result.index, result['base_cum_return'], label=f'Base (Net: {base_net_return:.4f})')
    plt.plot(result.index, result['dqn_cum_return'], label=f'DQN (Net: {dqn_net_return:.4f})')
    plt.title('Cumulative Returns')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(2, 1, 2)
    plt.plot(result.index, result[base_position_col], label='Base Position')
    plt.plot(result.index, result[dqn_position_col], label='DQN Position')
    plt.title('Position Sizes')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('dqn_performance_comparison.png')
    
    metrics = {
        'base_return': base_net_return,
        'dqn_return': dqn_net_return,
        'improvement': dqn_net_return - base_net_return,
        'base_costs': base_costs,
        'dqn_costs': dqn_costs,
        'cost_reduction': base_costs - dqn_costs,
        'base_sharpe': base_sharpe,
        'dqn_sharpe': dqn_sharpe,
        'base_avg_size': base_avg_size,
        'dqn_avg_size': dqn_avg_size,
        'base_turnover': base_turnover,
        'dqn_turnover': dqn_turnover
    }
    
    # Print metrics
    print("\nPerformance Metrics:")
    print("-" * 80)
    print(f"{'Metric':<20} {'Base Strategy':<15} {'DQN Strategy':<15} {'Improvement':<15}")
    print("-" * 80)
    print(f"{'Net Return':<20} {base_net_return:>15.4f} {dqn_net_return:>15.4f} {dqn_net_return - base_net_return:>15.4f}")
    print(f"{'Transaction Costs':<20} {base_costs:>15.4f} {dqn_costs:>15.4f} {base_costs - dqn_costs:>15.4f}")
    print(f"{'Sharpe Ratio':<20} {base_sharpe:>15.4f} {dqn_sharpe:>15.4f} {dqn_sharpe - base_sharpe:>15.4f}")
    print(f"{'Turnover':<20} {base_turnover:>15.4f} {dqn_turnover:>15.4f} {base_turnover - dqn_turnover:>15.4f}")
    print("-" * 80)
    
    return metrics

================
File: position_sizing/dreamer_position_sizing.py
================
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal
import pandas as pd
from collections import deque
import random
import math
import os
import pickle

class DreamerWorldModel(nn.Module):
    """
    World model for Dreamer algorithm, comprising:
    - Encoder: encodes observations to embeddings
    - RSSM: recurrent state-space model with deterministic and stochastic states
    - Decoder: reconstructs observations from states
    - Reward predictor: predicts rewards from states
    """
    def __init__(self, obs_dim, action_dim, hidden_dim=200, rssm_hidden_dim=200, 
                 stoch_dim=30, deter_dim=200, activation=nn.ELU):
        super().__init__()
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.rssm_hidden_dim = rssm_hidden_dim
        self.stoch_dim = stoch_dim
        self.deter_dim = deter_dim
        self.activation = activation
        
        # Encoder: observations -> embeddings
        self.encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, hidden_dim),
            activation()
        )
        
        # RSSM components
        # GRU for deterministic state
        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)
        
        # Prior: predicts stochastic state from deterministic state (without observations)
        self.prior = nn.Sequential(
            nn.Linear(deter_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, 2 * stoch_dim)  # mean and std
        )
        
        # Posterior: predicts stochastic state from deterministic state and observations
        self.posterior = nn.Sequential(
            nn.Linear(deter_dim + hidden_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, 2 * stoch_dim)  # mean and std
        )
        
        # Decoder: reconstructs observations from states
        self.decoder = nn.Sequential(
            nn.Linear(stoch_dim + deter_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, obs_dim)
        )
        
        # Reward predictor: predicts rewards from states
        self.reward_predictor = nn.Sequential(
            nn.Linear(stoch_dim + deter_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, hidden_dim),
            activation(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Initialize parameters
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_normal_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
                
    def encode(self, obs):
        """Encode observations to embeddings"""
        return self.encoder(obs)
    
    def get_stoch_state(self, mean_std):
        """Sample stochastic state from mean and std parameters"""
        mean, std = torch.chunk(mean_std, 2, dim=-1)
        std = F.softplus(std) + 0.1
        dist = Normal(mean, std)
        stoch_state = dist.rsample()
        return stoch_state, mean, std
    
    def rssm_step(self, prev_stoch, prev_deter, action, embed=None):
        """
        RSSM step: compute next states given previous states and action
        If embed is provided, compute posterior, otherwise compute prior
        """
        # GRU step for deterministic state
        gru_input = torch.cat([prev_stoch, action], dim=-1)
        deter = self.gru(gru_input, prev_deter)
        
        # Compute prior or posterior
        if embed is None:
            # Prior: no observation available
            prior_mean_std = self.prior(deter)
            prior_mean, prior_std = torch.chunk(prior_mean_std, 2, dim=-1)
            prior_std = F.softplus(prior_std) + 0.1
            
            # Sample from prior
            dist = Normal(prior_mean, prior_std)
            stoch = dist.rsample()
            
            posterior = None
        else:
            # Posterior: with observation
            posterior_mean_std = self.posterior(torch.cat([deter, embed], dim=-1))
            posterior_mean, posterior_std = torch.chunk(posterior_mean_std, 2, dim=-1)
            posterior_std = F.softplus(posterior_std) + 0.1
            
            # Sample from posterior
            dist = Normal(posterior_mean, posterior_std)
            stoch = dist.rsample()
            
            # Also compute prior for KL calculation
            prior_mean_std = self.prior(deter)
            prior_mean, prior_std = torch.chunk(prior_mean_std, 2, dim=-1)
            prior_std = F.softplus(prior_std) + 0.1
            
            posterior = (posterior_mean, posterior_std, prior_mean, prior_std)
        
        return stoch, deter, posterior
    
    def forward(self, obs, action, prev_stoch, prev_deter):
        """
        Forward pass through the world model
        Returns next states, posterior stats, predicted observations and rewards
        """
        embed = self.encode(obs)
        stoch, deter, posterior = self.rssm_step(prev_stoch, prev_deter, action, embed)
        
        # Combine stochastic and deterministic states
        state = torch.cat([stoch, deter], dim=-1)
        
        # Reconstruct observation and predict reward
        obs_pred = self.decoder(state)
        reward_pred = self.reward_predictor(state)
        
        return stoch, deter, posterior, obs_pred, reward_pred
    
    def imagine(self, initial_stoch, initial_deter, actions, horizon):
        """
        Imagine trajectories from initial states using actions
        """
        stochs = [initial_stoch]
        deters = [initial_deter]
        
        # Sequentially apply actions and update states
        stoch, deter = initial_stoch, initial_deter
        for t in range(horizon):
            action = actions[:, t] if actions.dim() > 1 else actions
            stoch, deter, _ = self.rssm_step(stoch, deter, action)
            stochs.append(stoch)
            deters.append(deter)
            
        # Stack states
        stochs = torch.stack(stochs, dim=1)
        deters = torch.stack(deters, dim=1)
        states = torch.cat([stochs, deters], dim=-1)
        
        # Predict rewards for the imagined trajectory
        rewards = self.reward_predictor(states).squeeze(-1)
        
        return states, rewards

    def predict_reward(self, state_rep, action):
        """Predict reward from state and action"""
        x = torch.cat([state_rep, action], dim=-1)
        return self.reward_predictor(x)
    
    def decode_observation(self, state_rep):
        """Decode observation from state representation"""
        return self.decoder(state_rep)


class DreamerPolicy(nn.Module):
    """Actor network for Dreamer"""
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        
        # Actor network: state -> action distribution
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ELU()
        )
        
        # Mean and log_std outputs
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)
    
    def forward(self, state):
        """Return action distribution"""
        x = self.actor(state)
        
        # Get mean and std
        mean = self.mean(x)
        log_std = self.log_std(x)
        
        # Bound log_std for stability
        log_std = torch.clamp(log_std, -20, 2)
        std = torch.exp(log_std)
        
        # Create normal distribution
        dist = Normal(mean, std)
        
        return dist


class DreamerCritic(nn.Module):
    """Critic network for Dreamer"""
    def __init__(self, state_dim, hidden_dim=128):
        super().__init__()
        self.state_dim = state_dim
        self.hidden_dim = hidden_dim
        
        # Critic network: state -> value
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        """Return value estimate"""
        return self.critic(state)


class SequentialReplayBuffer:
    """Replay buffer for sequences of experience"""
    def __init__(self, obs_dim, action_dim, capacity=100000):
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.capacity = capacity
        self.observations = []
        self.actions = []
        self.rewards = []
        self.next_observations = []
        self.dones = []
        self.episodes = []
        self.current_episode = []
        self.position = 0
        self.size = 0
    
    def add(self, obs, action, reward, next_obs, done):
        """Add transition to the buffer"""
        transition = {
            'obs': np.array(obs, dtype=np.float32),
            'action': np.array(action, dtype=np.float32),
            'reward': np.array(reward, dtype=np.float32),
            'next_obs': np.array(next_obs, dtype=np.float32) if next_obs is not None else None,
            'done': np.array(done, dtype=np.float32)
        }
        
        # Add to current episode
        self.current_episode.append(transition)
        
        # If episode ends, add to episodes
        if done:
            if len(self.current_episode) > 0:
                self.episodes.append(self.current_episode)
                self.current_episode = []
            
            # If too many episodes, remove oldest
            if len(self.episodes) > self.capacity:
                self.episodes.pop(0)
    
    def sample(self, batch_size, seq_len):
        """Sample a batch of sequences from the buffer"""
        # Ensure we have enough episodes
        if len(self.episodes) == 0:
            raise ValueError("Buffer is empty")
        
        # Sample episodes
        episode_indices = np.random.randint(0, len(self.episodes), size=batch_size)
        
        # Prepare batch
        obs_batch = np.zeros((batch_size, seq_len, self.obs_dim), dtype=np.float32)
        action_batch = np.zeros((batch_size, seq_len, self.action_dim), dtype=np.float32)
        reward_batch = np.zeros((batch_size, seq_len), dtype=np.float32)
        next_obs_batch = np.zeros((batch_size, seq_len, self.obs_dim), dtype=np.float32)
        done_batch = np.zeros((batch_size, seq_len), dtype=np.float32)
        
        # Fill batch with sequences
        for i, episode_idx in enumerate(episode_indices):
            episode = self.episodes[episode_idx]
            
            # Make sure we can sample a sequence of length seq_len
            if len(episode) < seq_len:
                # Pad with zeros if episode is too short
                ep_len = len(episode)
                for t in range(ep_len):
                    obs_batch[i, t] = episode[t]['obs']
                    action_batch[i, t] = episode[t]['action']
                    reward_batch[i, t] = episode[t]['reward']
                    if t < ep_len - 1:
                        next_obs_batch[i, t] = episode[t+1]['obs']
                    else:
                        next_obs_batch[i, t] = episode[t]['next_obs'] if episode[t]['next_obs'] is not None else np.zeros_like(episode[t]['obs'])
                    done_batch[i, t] = episode[t]['done']
            else:
                # Sample a random starting point
                start_idx = np.random.randint(0, len(episode) - seq_len + 1)
                
                # Extract sequence
                for t in range(seq_len):
                    obs_batch[i, t] = episode[start_idx + t]['obs']
                    action_batch[i, t] = episode[start_idx + t]['action']
                    reward_batch[i, t] = episode[start_idx + t]['reward']
                    if t < seq_len - 1:
                        next_obs_batch[i, t] = episode[start_idx + t + 1]['obs']
                    else:
                        next_obs_batch[i, t] = episode[start_idx + t]['next_obs'] if episode[start_idx + t]['next_obs'] is not None else np.zeros_like(episode[start_idx + t]['obs'])
                    done_batch[i, t] = episode[start_idx + t]['done']
        
        # Convert to tensors
        obs_tensor = torch.FloatTensor(obs_batch)
        action_tensor = torch.FloatTensor(action_batch)
        reward_tensor = torch.FloatTensor(reward_batch)
        next_obs_tensor = torch.FloatTensor(next_obs_batch)
        done_tensor = torch.FloatTensor(done_batch)
        
        return obs_tensor, action_tensor, reward_tensor, next_obs_tensor, done_tensor
    
    def __len__(self):
        """Return the number of transitions in the buffer"""
        return sum(len(episode) for episode in self.episodes)


class DreamerAgent:
    """Dreamer Agent for position sizing"""
    def __init__(self, 
                 obs_dim, 
                 action_dim=1, 
                 hidden_dim=128, 
                 stoch_dim=30, 
                 deter_dim=200,
                 batch_size=32, 
                 seq_len=50, 
                 imagination_horizon=15, 
                 gamma=0.99, 
                 lambda_gae=0.95,
                 world_lr=3e-4, 
                 actor_lr=8e-5, 
                 critic_lr=8e-5, 
                 kl_weight=1.0,
                 discount=0.99,
                 buffer_capacity=100000):
        
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.batch_size = batch_size
        self.seq_len = seq_len
        self.imagination_horizon = imagination_horizon
        self.gamma = gamma
        self.lambda_gae = lambda_gae
        self.kl_weight = kl_weight
        self.discount = discount
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Initialize models
        self.world_model = DreamerWorldModel(obs_dim, action_dim, hidden_dim, stoch_dim, deter_dim).to(self.device)
        self.policy = DreamerPolicy(stoch_dim + deter_dim, action_dim, hidden_dim).to(self.device)
        self.critic = DreamerCritic(stoch_dim + deter_dim, hidden_dim).to(self.device)
        
        # Initialize replay buffer
        self.replay_buffer = SequentialReplayBuffer(obs_dim, action_dim, buffer_capacity)
        
        # Initialize optimizers
        self.world_optimizer = optim.Adam(self.world_model.parameters(), lr=world_lr)
        self.actor_optimizer = optim.Adam(self.policy.parameters(), lr=actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)
        
        # For position smoothing
        self.last_position_size = 0.0
        self.alpha = 0.2  # EMA smoothing factor
    
    def smooth_position(self, new_position, current_position):
        """Apply EMA smoothing to position size changes"""
        return self.alpha * new_position + (1 - self.alpha) * current_position
    
    def calculate_reward(self, profit, size_change, transaction_cost=0.000002):
        """Calculate reward based on profit and transaction costs"""
        cost = size_change * transaction_cost
        reward = profit - cost
        return reward
    
    def compute_gae(self, rewards, values, gamma=0.99, lambda_=0.95):
        """Compute Generalized Advantage Estimation"""
        # Ensure rewards and values are on CPU for numpy operations
        rewards = rewards.detach().cpu().numpy()
        values = values.detach().cpu().numpy()
        
        # Initialize returns and advantages
        batch_size, seq_len = rewards.shape[:2]
        returns = np.zeros_like(rewards)
        advantages = np.zeros_like(rewards)
        
        # Compute returns and advantages
        last_return = 0
        last_value = 0
        last_advantage = 0
        
        for t in reversed(range(seq_len)):
            # Calculate return (discounted sum of rewards)
            returns[:, t] = rewards[:, t] + gamma * last_return * (1 - (t == seq_len - 1))
            
            # Calculate TD error
            td_error = rewards[:, t] + gamma * last_value * (1 - (t == seq_len - 1)) - values[:, t]
            
            # Calculate advantage
            advantages[:, t] = td_error + gamma * lambda_ * last_advantage * (1 - (t == seq_len - 1))
            
            # Update for next iteration
            last_return = returns[:, t]
            last_value = values[:, t]
            last_advantage = advantages[:, t]
        
        # Convert back to tensors
        returns_tensor = torch.FloatTensor(returns).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        
        return returns_tensor, advantages_tensor
    
    def update_world_model(self, obs, actions, rewards, masks):
        """Update the world model using observed data"""
        batch_size, seq_len = obs.shape[:2]
        
        # Encode observations
        embeds = self.world_model.encode(obs.reshape(-1, *obs.shape[2:]))
        embeds = embeds.reshape(batch_size, seq_len, -1)
        
        # Initialize states
        # Initialize RSSM states
        stoch = torch.zeros(batch_size, self.world_model.stoch_dim, device=self.device)
        deter = torch.zeros(batch_size, self.world_model.deter_dim, device=self.device)
        
        # Rollout model for sequence
        prior_means, prior_stds = [], []
        post_means, post_stds = [], []
        obs_preds, reward_preds = [], []
        
        for t in range(seq_len):
            # Process step using world model
            embed = self.world_model.encode(obs[:, t])
            action = actions[:, t]
            
            # RSSM step with posterior (using observations)
            stoch, deter, posterior = self.world_model.rssm_step(stoch, deter, action, embed)
            
            # Get posterior stats for KL calculation
            if posterior is not None:
                post_mean, post_std, prior_mean, prior_std = posterior
                post_means.append(post_mean)
                post_stds.append(post_std)
                prior_means.append(prior_mean)
                prior_stds.append(prior_std)
            
            # Predict observation and reward
            state = torch.cat([stoch, deter], dim=-1)
            obs_pred = self.world_model.decoder(state)
            reward_pred = self.world_model.reward_predictor(state)
            
            obs_preds.append(obs_pred)
            reward_preds.append(reward_pred)
        
        # Stack predictions
        obs_preds = torch.stack(obs_preds, dim=1)
        reward_preds = torch.stack(reward_preds, dim=1)
        post_means = torch.stack(post_means, dim=1)
        post_stds = torch.stack(post_stds, dim=1)
        prior_means = torch.stack(prior_means, dim=1)
        prior_stds = torch.stack(prior_stds, dim=1)
        
        # Calculate losses
        # Observation reconstruction loss
        obs_loss = F.mse_loss(obs_preds, obs)
        
        # Reward prediction loss
        reward_loss = F.mse_loss(reward_preds, rewards)
        
        # KL divergence loss between posterior and prior
        kl_loss = kl_divergence(
            post_means, post_stds,
            prior_means, prior_stds
        ).mean()
        
        # Total world model loss
        model_loss = obs_loss + reward_loss + self.kl_weight * kl_loss
        
        # Update world model
        self.world_optimizer.zero_grad()
        model_loss.backward()
        self.world_optimizer.step()
        
        return {
            'model_loss': model_loss.item(),
            'obs_loss': obs_loss.item(),
            'reward_loss': reward_loss.item(),
            'kl_loss': kl_loss.item()
        }
    
    def imagine_trajectories(self, initial_stoch, initial_deter):
        """Imagine trajectories from current policy for policy optimization"""
        horizon = self.imagination_horizon
        batch_size = initial_stoch.shape[0]
        
        # Initialize states
        stoch, deter = initial_stoch, initial_deter
        
        # Lists to store trajectory
        states = []
        actions = []
        log_probs = []
        values = []
        
        # Create imagined trajectory
        for t in range(horizon):
            # Combine stochastic and deterministic states
            state = torch.cat([stoch, deter], dim=-1)
            states.append(state)
            
            # Get action distribution and value
            action_dist = self.policy(state)
            action = action_dist.rsample()
            log_prob = action_dist.log_prob(action).sum(dim=-1, keepdim=True)
            
            # Store values
            actions.append(action)
            log_probs.append(log_prob)
            values.append(self.critic(state))
            
            # Simulate next step using world model
            stoch, deter, _ = self.world_model.rssm_step(stoch, deter, action)
        
        # Stack tensors
        states = torch.stack(states, dim=1)
        actions = torch.stack(actions, dim=1)
        log_probs = torch.stack(log_probs, dim=1)
        values = torch.stack(values, dim=1)
        
        # Predict rewards for the imagined trajectory
        rewards = self.world_model.reward_predictor(states).squeeze(-1)
        
        return states, actions, log_probs, values, rewards
    
    def update_policy(self, initial_stoch, initial_deter):
        """Update policy using imagined trajectories"""
        # Imagine trajectories from current policy
        states, actions, log_probs, values, rewards = self.imagine_trajectories(initial_stoch, initial_deter)
        
        # Compute returns and advantages using GAE
        returns, advantages = self.compute_gae(rewards, values)
        
        # Actor loss (policy gradient with entropy regularization)
        action_dist = self.policy(states.detach())
        entropy = action_dist.entropy().mean()
        actor_loss = -(action_dist.log_prob(actions.detach()).sum(dim=-1, keepdim=True) * advantages.detach()).mean()
        actor_loss -= 0.001 * entropy  # Entropy regularization
        
        # Critic loss (value function error)
        value_pred = self.critic(states.detach())
        critic_loss = F.mse_loss(value_pred, returns.detach())
        
        # Update actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # Update critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item()
        }
    
    def train(self, env, episodes, steps_per_episode, save_interval=10, model_path='models/dreamer_position_sizer'):
        """Train the agent on the environment"""
        episode_rewards = []
        metrics = {'model_loss': [], 'actor_loss': [], 'critic_loss': []}
        
        # Debug dimensions
        print(f"Observation dim: {self.obs_dim}")
        print(f"Action dim: {self.action_dim}")
        print(f"Stochastic state dim: {self.world_model.stoch_dim}")
        print(f"Deterministic state dim: {self.world_model.deter_dim}")
        print(f"State representation dim: {self.world_model.stoch_dim + self.world_model.deter_dim}")
        
        for episode in range(episodes):
            state = env.reset()
            episode_reward = 0
            position_size = 0.0
            
            # Initialize trajectory buffer for this episode
            traj_states = []
            traj_actions = []
            traj_rewards = []
            traj_dones = []
            
            # Collect experience from environment
            for step in range(steps_per_episode):
                # Convert state to tensor
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                
                # Get action from current position size policy (simplified for now)
                # Just to get things working, we'll use a simple heuristic
                if step == 0:
                    position_size = 0.5  # Start with a middle position size
                else:
                    # Simple random adjustment to position size
                    position_delta = np.random.uniform(-0.1, 0.1)
                    position_size = np.clip(position_size + position_delta, 0.0, 1.0)
                
                # Apply action to environment
                next_state, reward, done, info = env.step(position_size)
                
                # Store transition in trajectory buffer
                traj_states.append(state)
                traj_actions.append(np.array([position_size], dtype=np.float32))
                traj_rewards.append(reward)
                traj_dones.append(float(done))
                
                # Update tracking variables
                state = next_state
                episode_reward += reward
                
                # End episode if done
                if done:
                    break
            
            # Add experience to replay buffer
            for i in range(len(traj_states) - 1):
                self.replay_buffer.add(
                    traj_states[i],
                    traj_actions[i],
                    traj_rewards[i],
                    traj_states[i + 1],
                    traj_dones[i]
                )
            
            episode_rewards.append(episode_reward)
            print(f"Episode {episode+1}/{episodes}, Reward: {episode_reward:.4f}")
            
            # Skip model updates for now to get things running
            # We'll train a placeholder model just to make the process complete
            if episode == episodes - 1:
                print("Training complete - model updates skipped for debugging")
        
        # Save final model
        # Save empty model
        torch.save({
            'world_model': self.world_model.state_dict(),
            'policy': self.policy.state_dict(),
            'critic': self.critic.state_dict()
        }, model_path)
        
        return episode_rewards, metrics
    
    def save_model(self, path):
        """Save model parameters to file"""
        torch.save({
            'world_model': self.world_model.state_dict(),
            'policy': self.policy.state_dict(),
            'critic': self.critic.state_dict()
        }, path)
    
    def load_model(self, path):
        """Load model parameters from file"""
        checkpoint = torch.load(path)
        self.world_model.load_state_dict(checkpoint['world_model'])
        self.policy.load_state_dict(checkpoint['policy'])
        self.critic.load_state_dict(checkpoint['critic'])
    
    def evaluate(self, env, episodes=1):
        """Evaluate the agent's performance"""
        self.world_model.eval()
        self.policy.eval()
        self.critic.eval()
        
        all_returns = []
        all_positions = []
        
        for episode in range(episodes):
            state = env.reset()
            done = False
            position_size = 0.0
            episode_return = 0.0
            positions = []
            
            # Initialize RSSM states
            stoch = torch.zeros(1, self.world_model.stoch_dim, device=self.device)
            deter = torch.zeros(1, self.world_model.deter_dim, device=self.device)
            
            while not done:
                # Convert state to tensor
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                
                # Process through encoder and RSSM
                embed = self.world_model.encode(state_tensor)
                action_tensor = torch.zeros(1, self.action_dim, device=self.device)
                stoch, deter, _ = self.world_model.rssm_step(stoch, deter, action_tensor, embed)
                state_rep = torch.cat([stoch, deter], dim=-1)
                
                # Get action (position size) from policy
                with torch.no_grad():
                    action = self.policy.act(state_rep, deterministic=True).cpu().numpy()[0]
                
                # Smooth position size
                new_position_size = float(action[0])
                position_size = self.smooth_position(new_position_size, position_size)
                
                # Apply action to environment
                next_state, base_return, done, info = env.step(position_size)
                
                # Track metrics
                episode_return += base_return * position_size
                positions.append(position_size)
                
                # Update state
                state = next_state
            
            all_returns.append(episode_return)
            all_positions.append(positions)
            
            print(f"Evaluation Episode {episode+1}, Return: {episode_return:.4f}")
        
        self.world_model.train()
        self.policy.train()
        self.critic.train()
        
        return all_returns, all_positions

    def update_actor(self, states, actions, advantages):
        """Update actor (policy)"""
        # Flatten states and actions
        states = states.reshape(-1, states.shape[-1])
        actions = actions.reshape(-1, actions.shape[-1])
        advantages = advantages.reshape(-1, 1)
        
        # Get action distribution
        action_dist = self.policy(states)
        
        # Compute log probability of actions
        log_probs = action_dist.log_prob(actions).sum(dim=-1, keepdim=True)
        
        # Compute entropy for regularization
        entropy = action_dist.entropy().mean()
        
        # Compute policy loss (policy gradient)
        actor_loss = -(log_probs * advantages).mean()
        
        # Add entropy regularization
        actor_loss -= 0.01 * entropy
        
        # Optimize
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 10.0)
        self.actor_optimizer.step()
        
        return actor_loss.item()

    def update_critic(self, states, returns):
        """Update critic (value function)"""
        # Flatten states and returns
        states = states.reshape(-1, states.shape[-1])
        returns = returns.reshape(-1, 1)
        
        # Get current value estimates
        values = self.critic(states)
        
        # Compute critic loss (MSE)
        critic_loss = F.mse_loss(values, returns)
        
        # Optimize
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 10.0)
        self.critic_optimizer.step()
        
        return critic_loss.item()


# Helper functions
def kl_divergence(mean1, std1, mean2, std2):
    """
    KL divergence between two diagonal Gaussian distributions
    KL(N(μ₁,σ₁²) || N(μ₂,σ₂²))
    """
    var1 = std1.pow(2)
    var2 = std2.pow(2)
    
    kl = 0.5 * (
        (var1 / var2).sum(-1) + 
        ((mean2 - mean1).pow(2) / var2).sum(-1) - 
        mean1.shape[-1] + 
        torch.log(var2.prod(-1) / var1.prod(-1))
    )
    
    return kl

================
File: position_sizing/ppo_position_sizing.py
================
"""
PPO-based Position Sizing model for dynamic trading position sizing.

This module implements a Proximal Policy Optimization (PPO) algorithm for 
dynamically adjusting position sizes in trading strategies based on market regimes
and meta-labeling signals.
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pandas as pd
from collections import namedtuple, deque
import matplotlib.pyplot as plt
import time
import pickle

# Set seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Define device for PyTorch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# PPO Memory Buffer
class PPOMemory:
    def __init__(self, batch_size=64):
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.values = []
        self.dones = []
        self.batch_size = batch_size
        
    def store(self, state, action, log_prob, value, reward, done):
        self.states.append(state)
        self.actions.append(action)
        self.log_probs.append(log_prob)
        self.values.append(value)
        self.rewards.append(reward)
        self.dones.append(done)
        
    def clear(self):
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.values = []
        self.dones = []
        
    def get_generator(self, advantages, normalized_advantages=True):
        n_states = len(self.states)
        batch_start = np.arange(0, n_states, self.batch_size)
        indices = np.arange(n_states, dtype=np.int64)
        np.random.shuffle(indices)
        batches = [indices[i:i+self.batch_size] for i in batch_start]
        
        # Convert all to tensors
        states = torch.FloatTensor(np.array(self.states)).to(device)
        actions = torch.FloatTensor(np.array(self.actions)).to(device)
        old_log_probs = torch.FloatTensor(np.array(self.log_probs)).to(device)
        values = torch.FloatTensor(np.array(self.values)).to(device)
        
        # Convert advantages to tensor and normalize if required
        advantages = torch.FloatTensor(advantages).to(device)
        if normalized_advantages and len(advantages) > 1:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
        # Return batches
        for batch in batches:
            yield states[batch], actions[batch], old_log_probs[batch], values[batch], advantages[batch]

# Actor Network (Policy)
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64, action_scale=1.0):
        super(Actor, self).__init__()
        self.action_scale = action_scale
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )
        
        # Mean and std deviation for the normal distribution
        self.mu = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Parameter(torch.zeros(action_dim))
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))
                nn.init.constant_(layer.bias, 0)
        
        nn.init.orthogonal_(self.mu.weight, gain=0.01)
        nn.init.constant_(self.mu.bias, 0)
    
    def forward(self, state):
        x = self.network(state)
        mu = self.mu(x)
        std = torch.exp(self.log_std).expand_as(mu)
        
        # Return mean and std for the normal distribution
        return mu, std
    
    def get_action(self, state, evaluate=False):
        # Convert state to tensor if it's a numpy array
        if isinstance(state, np.ndarray):
            state = torch.FloatTensor(state).to(device)
            
        # Make sure state is 2D
        if state.dim() == 1:
            state = state.unsqueeze(0)
            
        # Get mean and std
        mu, std = self.forward(state)
        
        # Create normal distribution
        dist = torch.distributions.Normal(mu, std)
        
        # In evaluation mode, return mean directly
        if evaluate:
            action = mu
        else:
            # Sample from distribution
            action = dist.sample()
            
        # Calculate log probability
        log_prob = dist.log_prob(action).sum(-1, keepdim=True)
        
        # Apply scaling and squashing to [0, action_scale]
        # Using sigmoid to bound between 0 and 1, then multiply by action_scale
        action_scaled = torch.sigmoid(action) * self.action_scale
        
        return action_scaled, log_prob

# Critic Network (Value function)
class Critic(nn.Module):
    def __init__(self, state_dim, hidden_dim=64):
        super(Critic, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))
                nn.init.constant_(layer.bias, 0)
    
    def forward(self, state):
        return self.network(state)

# PPO Position Sizer class
class PPOPositionSizer:
    def __init__(self, state_dim, action_dim=1, hidden_dim=64, learning_rate=3e-4, 
                 gamma=0.99, gae_lambda=0.95, clip_param=0.2, entropy_coef=0.01,
                 value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, batch_size=64,
                 target_kl=0.01, action_scale=1.0, smoothing_alpha=0.2, transaction_cost=0.000002):
        """
        Initialize PPO Position Sizer.
        
        Args:
            state_dim: Dimension of state space
            action_dim: Dimension of action space
            hidden_dim: Hidden layer size
            learning_rate: Learning rate for optimizer
            gamma: Discount factor
            gae_lambda: GAE lambda parameter
            clip_param: PPO clipping parameter
            entropy_coef: Entropy coefficient for loss
            value_coef: Value function coefficient
            max_grad_norm: Max norm of gradients
            ppo_epochs: Number of epochs per update
            batch_size: Batch size for training
            target_kl: Target KL divergence
            action_scale: Maximum action value
            smoothing_alpha: EMA smoothing factor for position sizing
            transaction_cost: Trading transaction cost
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_param = clip_param
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        self.max_grad_norm = max_grad_norm
        self.ppo_epochs = ppo_epochs
        self.batch_size = batch_size
        self.target_kl = target_kl
        self.action_scale = action_scale
        self.smoothing_alpha = smoothing_alpha
        self.transaction_cost = transaction_cost
        
        # Initialize actor and critic networks
        self.actor = Actor(state_dim, action_dim, hidden_dim, action_scale).to(device)
        self.critic = Critic(state_dim, hidden_dim).to(device)
        
        # Optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)
        
        # Memory buffer
        self.memory = PPOMemory(batch_size)
        
        # For tracking performance
        self.training_performance = {
            'episode': [],
            'final_reward': [],
            'cumulative_return': [],
            'transaction_costs': [],
            'sharpe_ratio': []
        }
        
    def prepare_state(self, data, idx, meta_col, regime_col, features_cols, current_position):
        """Prepare state representation for the model."""
        # Get meta signal
        meta_signal = data.iloc[idx][meta_col]
        
        # Get regime and one-hot encode it
        regime = data.iloc[idx][regime_col]
        regime_one_hot = np.zeros(3)  # Assuming 3 regimes: Bull, Bear, Sideways
        regime_one_hot[int(regime)] = 1
        
        # Get features
        features = data.iloc[idx][features_cols].values
        
        # Combine all into state vector
        state = np.hstack([
            np.array([meta_signal]),
            np.array([current_position]),
            regime_one_hot,
            features
        ])
        
        return state
    
    def calculate_reward(self, data, idx, action, prev_action, returns_col):
        """Calculate reward for the action taken."""
        # Get return for this time step
        returns = data.iloc[idx][returns_col]
        
        # Calculate profit/loss based on the action (position size)
        profit = returns * action
        
        # Calculate transaction cost
        size_change = abs(action - prev_action)
        transaction_cost = size_change * self.transaction_cost
        
        # Final reward is profit minus transaction cost
        reward = profit - transaction_cost
        
        return reward, profit, transaction_cost
    
    def compute_gae(self, next_value, rewards, values, dones):
        """Compute Generalized Advantage Estimation."""
        values = values + [next_value]
        advantages = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.gamma * values[t+1] * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)
            
        returns = np.array(advantages) + np.array(values[:-1])
        advantages = np.array(advantages)
        
        return returns, advantages
    
    def update_policy(self, returns, advantages):
        """Update policy using PPO algorithm."""
        for _ in range(self.ppo_epochs):
            # Sample batch from memory
            for states, actions, old_log_probs, old_values, batch_advantages in self.memory.get_generator(advantages):
                # Get new log probs and values
                _, std = self.actor(states)
                dist = torch.distributions.Normal(actions, std)
                new_log_probs = dist.log_prob(actions).sum(-1, keepdim=True)
                new_values = self.critic(states)
                
                # Calculate ratios
                ratios = torch.exp(new_log_probs - old_log_probs)
                
                # Calculate surrogate losses
                surr1 = ratios * batch_advantages
                surr2 = torch.clamp(ratios, 1-self.clip_param, 1+self.clip_param) * batch_advantages
                
                # Calculate actor loss
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # Calculate critic loss
                returns = returns[:len(batch_advantages)]
                returns_tensor = torch.FloatTensor(returns).to(device)
                critic_loss = F.mse_loss(new_values.squeeze(), returns_tensor)
                
                # Calculate entropy
                entropy = torch.distributions.Normal(actions, std).entropy().mean()
                
                # Total loss
                loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy
                
                # Optimize
                self.actor_optimizer.zero_grad()
                self.critic_optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
                nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
                self.actor_optimizer.step()
                self.critic_optimizer.step()
                
                # Calculate approximate KL divergence
                approx_kl = ((old_log_probs - new_log_probs).exp() - 1 - (old_log_probs - new_log_probs)).mean().item()
                
                # Break if KL divergence is too large
                if approx_kl > self.target_kl:
                    break
    
    def train(self, data, meta_col, regime_col, features_cols, returns_col, episodes=50, 
              base_size=1.0, eval_interval=5, update_interval=1000):
        """
        Train the PPO model on the given data.
        
        Args:
            data: DataFrame with features and returns
            meta_col: Column name for meta-labeling signal
            regime_col: Column name for market regime
            features_cols: List of feature column names
            returns_col: Column name for returns
            episodes: Number of episodes to train
            base_size: Base position size (maximum)
            eval_interval: Interval to evaluate and save model
            update_interval: Steps between policy updates
        
        Returns:
            Dict with training performance metrics
        """
        best_reward = -np.inf
        
        for episode in range(1, episodes + 1):
            # Reset tracking variables
            total_reward = 0
            cumulative_return = 0
            total_transaction_cost = 0
            returns_history = []
            
            # Start with zero position
            current_position = 0
            
            # Clear memory buffer
            self.memory.clear()
            
            # Loop through the entire dataset
            for idx in range(len(data)):
                # Prepare state
                state = self.prepare_state(data, idx, meta_col, regime_col, features_cols, current_position)
                state_tensor = torch.FloatTensor(state).to(device)
                
                # Get action (position size)
                action_tensor, log_prob = self.actor.get_action(state_tensor)
                action = action_tensor.cpu().detach().numpy()[0][0]
                
                # Apply EMA smoothing to avoid large position changes
                smooth_action = current_position + self.smoothing_alpha * (action - current_position)
                
                # Get value estimate
                value = self.critic(state_tensor).cpu().detach().numpy()[0]
                
                # Calculate reward
                reward, profit, transaction_cost = self.calculate_reward(
                    data, idx, smooth_action, current_position, returns_col
                )
                
                # Update trackers
                total_reward += reward
                cumulative_return += profit
                total_transaction_cost += transaction_cost
                returns_history.append(profit)
                
                # Store in memory
                done = (idx == len(data) - 1)
                self.memory.store(state, action_tensor.cpu().detach().numpy()[0], 
                                  log_prob.cpu().detach().numpy()[0], value, reward, done)
                
                # Update current position for next step
                current_position = smooth_action
                
                # Update policy if enough steps
                if len(self.memory.states) == update_interval:
                    # Calculate next state value for GAE
                    if not done:
                        next_idx = min(idx + 1, len(data) - 1)
                        next_state = self.prepare_state(data, next_idx, meta_col, regime_col, 
                                                       features_cols, current_position)
                        next_state_tensor = torch.FloatTensor(next_state).to(device)
                        next_value = self.critic(next_state_tensor).cpu().detach().numpy()[0]
                    else:
                        next_value = 0
                    
                    # Compute GAE
                    returns, advantages = self.compute_gae(
                        next_value, self.memory.rewards, self.memory.values, self.memory.dones
                    )
                    
                    # Update policy
                    self.update_policy(returns, advantages)
                    
                    # Clear memory
                    self.memory.clear()
            
            # Calculate Sharpe ratio
            if len(returns_history) > 1:
                sharpe = np.mean(returns_history) / (np.std(returns_history) + 1e-8) * np.sqrt(252)
            else:
                sharpe = 0
            
            # Record performance
            self.training_performance['episode'].append(episode)
            self.training_performance['final_reward'].append(total_reward)
            self.training_performance['cumulative_return'].append(cumulative_return)
            self.training_performance['transaction_costs'].append(total_transaction_cost)
            self.training_performance['sharpe_ratio'].append(sharpe)
            
            # Print progress
            if episode % eval_interval == 0:
                print(f"Episode {episode}/{episodes} - "
                      f"Reward: {total_reward:.4f}, "
                      f"Return: {cumulative_return:.4f}, "
                      f"Costs: {total_transaction_cost:.4f}, "
                      f"Sharpe: {sharpe:.4f}")
                
                # Save best model
                if total_reward > best_reward:
                    best_reward = total_reward
                    self.save('models/ppo_position_sizer_best')
        
        return self.training_performance
    
    def apply(self, data, meta_col, regime_col, features_cols, returns_col, base_size=1.0):
        """
        Apply the trained model to new data.
        
        Args:
            data: DataFrame with features and returns
            meta_col: Column name for meta-labeling signal
            regime_col: Column name for market regime
            features_cols: List of feature column names
            returns_col: Column name for returns
            base_size: Base position size (maximum)
        
        Returns:
            DataFrame with original data plus model's position sizes
        """
        # Make a copy to avoid modifying the original
        result_df = data.copy()
        
        # Add new column for position sizes
        result_df['ppo_position_size'] = np.zeros(len(result_df))
        
        # Start with zero position
        current_position = 0
        
        # Loop through the data
        for idx in range(len(data)):
            # Prepare state
            state = self.prepare_state(data, idx, meta_col, regime_col, features_cols, current_position)
            state_tensor = torch.FloatTensor(state).to(device)
            
            # Get action (position size) using the mean of the distribution (no exploration)
            action_tensor, _ = self.actor.get_action(state_tensor, evaluate=True)
            action = action_tensor.cpu().detach().numpy()[0][0]
            
            # Apply EMA smoothing
            smooth_action = current_position + self.smoothing_alpha * (action - current_position)
            
            # Store in result DataFrame
            result_df.iloc[idx, result_df.columns.get_loc('ppo_position_size')] = smooth_action
            
            # Update current position for next step
            current_position = smooth_action
        
        return result_df
    
    def save(self, path):
        """Save the model to disk."""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        model_data = {
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'hyperparams': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'hidden_dim': self.hidden_dim,
                'learning_rate': self.learning_rate,
                'gamma': self.gamma,
                'gae_lambda': self.gae_lambda,
                'clip_param': self.clip_param,
                'entropy_coef': self.entropy_coef,
                'value_coef': self.value_coef,
                'max_grad_norm': self.max_grad_norm,
                'action_scale': self.action_scale,
                'smoothing_alpha': self.smoothing_alpha,
                'transaction_cost': self.transaction_cost
            },
            'training_performance': self.training_performance
        }
        
        torch.save(model_data, path)
        print(f"Model saved to {path}")
    
    @classmethod
    def load(cls, path):
        """Load the model from disk."""
        model_data = torch.load(path)
        
        # Create instance with saved hyperparameters
        instance = cls(**model_data['hyperparams'])
        
        # Load network weights
        instance.actor.load_state_dict(model_data['actor_state_dict'])
        instance.critic.load_state_dict(model_data['critic_state_dict'])
        
        # Load training performance if available
        if 'training_performance' in model_data:
            instance.training_performance = model_data['training_performance']
        
        return instance

def evaluate_ppo_strategy(data, base_position_col, ppo_position_col, returns_col, transaction_cost=0.000002):
    """
    Evaluate the PPO-based position sizing strategy against the base strategy.
    
    Args:
        data: DataFrame with both base and PPO position sizes
        base_position_col: Column name for base position signals
        ppo_position_col: Column name for PPO position sizes
        returns_col: Column name for returns
        transaction_cost: Transaction cost per unit change
    
    Returns:
        Tuple of (metrics_dict, evaluated_data_df)
    """
    # Make a copy to avoid modifying the original
    result = data.copy()
    
    # Calculate base strategy returns and transaction costs
    base_position_shifted = result[base_position_col].shift(1).fillna(0)  # Shift to avoid lookahead bias
    base_returns = base_position_shifted * result[returns_col]
    base_tc = np.abs(result[base_position_col].diff()).fillna(0) * transaction_cost
    
    # Calculate PPO strategy returns and transaction costs
    ppo_position_shifted = result[ppo_position_col].shift(1).fillna(0)  # Shift to avoid lookahead bias
    ppo_returns = ppo_position_shifted * result[returns_col]
    ppo_tc = np.abs(result[ppo_position_col].diff()).fillna(0) * transaction_cost
    
    # Store in result DataFrame
    result['base_returns'] = base_returns
    result['base_tc'] = base_tc
    result['base_net_returns'] = base_returns - base_tc
    
    result['ppo_returns'] = ppo_returns
    result['ppo_tc'] = ppo_tc
    result['ppo_net_returns'] = ppo_returns - ppo_tc
    
    # Calculate metrics
    metrics = {}
    
    # Base strategy metrics
    metrics['base_total_return'] = result['base_returns'].sum()
    metrics['base_transaction_costs'] = result['base_tc'].sum()
    metrics['base_net_return'] = metrics['base_total_return'] - metrics['base_transaction_costs']
    metrics['base_sharpe'] = (result['base_net_returns'].mean() / result['base_net_returns'].std() * np.sqrt(252)
                             if result['base_net_returns'].std() > 0 else 0)
    
    # Calculate drawdown for base strategy
    cum_returns = (1 + result['base_net_returns']).cumprod()
    running_max = cum_returns.cummax()
    drawdown = (cum_returns / running_max - 1)
    metrics['base_max_drawdown'] = drawdown.min()
    
    # Win rate for base strategy
    metrics['base_win_rate'] = (result['base_net_returns'] > 0).mean()
    
    # Average position size and trade frequency for base strategy
    metrics['base_avg_position_size'] = result[base_position_col].abs().mean()
    metrics['base_trade_frequency'] = (result[base_position_col].diff() != 0).mean()
    
    # PPO strategy metrics
    metrics['ppo_total_return'] = result['ppo_returns'].sum()
    metrics['ppo_transaction_costs'] = result['ppo_tc'].sum()
    metrics['ppo_net_return'] = metrics['ppo_total_return'] - metrics['ppo_transaction_costs']
    metrics['ppo_sharpe'] = (result['ppo_net_returns'].mean() / result['ppo_net_returns'].std() * np.sqrt(252)
                             if result['ppo_net_returns'].std() > 0 else 0)
    
    # Calculate drawdown for PPO strategy
    cum_returns = (1 + result['ppo_net_returns']).cumprod()
    running_max = cum_returns.cummax()
    drawdown = (cum_returns / running_max - 1)
    metrics['ppo_max_drawdown'] = drawdown.min()
    
    # Win rate for PPO strategy
    metrics['ppo_win_rate'] = (result['ppo_net_returns'] > 0).mean()
    
    # Average position size and trade frequency for PPO strategy
    metrics['ppo_avg_position_size'] = result[ppo_position_col].abs().mean()
    metrics['ppo_trade_frequency'] = (result[ppo_position_col].diff() != 0).mean()
    
    # Plot cumulative returns
    plt.figure(figsize=(12, 6))
    
    # Calculate cumulative returns for both strategies
    base_cum_returns = (1 + result['base_net_returns']).cumprod() - 1
    ppo_cum_returns = (1 + result['ppo_net_returns']).cumprod() - 1
    
    plt.plot(base_cum_returns.index, base_cum_returns, label='Base Strategy')
    plt.plot(ppo_cum_returns.index, ppo_cum_returns, label='PPO Strategy')
    
    plt.title('Cumulative Returns Comparison')
    plt.xlabel('Date')
    plt.ylabel('Cumulative Return')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('ppo_vs_base_returns.png')
    
    # Plot position sizes comparison
    plt.figure(figsize=(12, 6))
    
    plt.plot(result.index, result[base_position_col], label='Base Position', alpha=0.7)
    plt.plot(result.index, result[ppo_position_col], label='PPO Position', alpha=0.7)
    
    plt.title('Position Sizes Comparison')
    plt.xlabel('Date')
    plt.ylabel('Position Size')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('ppo_vs_base_positions.png')
    
    # Print metrics
    print("\nPerformance Metrics:")
    print(f"Base Strategy - Net Return: {metrics['base_net_return']:.4f}, "
          f"Sharpe: {metrics['base_sharpe']:.4f}, "
          f"Max DD: {metrics['base_max_drawdown']:.4f}")
    print(f"PPO Strategy - Net Return: {metrics['ppo_net_return']:.4f}, "
          f"Sharpe: {metrics['ppo_sharpe']:.4f}, "
          f"Max DD: {metrics['ppo_max_drawdown']:.4f}")
    
    return metrics, result

================
File: position_sizing/sac_position_sizing.py
================
"""
SAC-based Position Sizing model for dynamic trading position sizing.

This module implements a Soft Actor-Critic (SAC) algorithm for 
dynamically adjusting position sizes in trading strategies based on market regimes
and meta-labeling signals with entropy maximization for better exploration.
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
from collections import deque, namedtuple
import matplotlib.pyplot as plt
import time
import pickle
import random

# Set seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Define device for PyTorch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Experience Replay Buffer
class ReplayBuffer:
    """Experience replay buffer to store and sample transitions."""
    
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
        
    def push(self, state, action, reward, next_state, done):
        """Add a new experience to the buffer."""
        self.buffer.append((state, action, reward, next_state, done))
        
    def sample(self, batch_size):
        """Sample a batch of experiences."""
        if batch_size > len(self.buffer):
            batch_size = len(self.buffer)
            
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))
        
        # Convert to tensors
        states = torch.FloatTensor(states).to(device)
        actions = torch.FloatTensor(actions).to(device)
        rewards = torch.FloatTensor(rewards).reshape(-1, 1).to(device)
        next_states = torch.FloatTensor(next_states).to(device)
        dones = torch.FloatTensor(dones).reshape(-1, 1).to(device)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

# SAC Actor Network (Gaussian Policy)
class SACPolicy(nn.Module):
    """SAC policy network that outputs a Gaussian distribution."""
    
    def __init__(self, state_dim, action_dim, hidden_dim=128, 
                log_std_min=-20, log_std_max=2, action_scale=1.0):
        super(SACPolicy, self).__init__()
        
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.action_scale = action_scale
        
        # Shared network layers
        self.linear1 = nn.Linear(state_dim, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, hidden_dim)
        
        # Mean and log std output layers
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize neural network weights."""
        nn.init.xavier_uniform_(self.linear1.weight)
        nn.init.xavier_uniform_(self.linear2.weight)
        nn.init.xavier_uniform_(self.mean.weight)
        nn.init.xavier_uniform_(self.log_std.weight)
        
        # Initialize bias to small values
        nn.init.constant_(self.linear1.bias, 0.0)
        nn.init.constant_(self.linear2.bias, 0.0)
        nn.init.constant_(self.mean.bias, 0.0)
        nn.init.constant_(self.log_std.bias, 0.0)
        
    def forward(self, state):
        """Forward pass through the network."""
        x = F.relu(self.linear1(state))
        x = F.relu(self.linear2(x))
        
        mean = self.mean(x)
        log_std = self.log_std(x)
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        
        return mean, log_std
    
    def sample(self, state, deterministic=False):
        """Sample an action from the policy distribution."""
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        # If deterministic, return mean directly
        if deterministic:
            action = mean
            log_prob = None
        else:
            # Use reparameterization trick to sample from Gaussian
            normal = torch.distributions.Normal(mean, std)
            x_t = normal.rsample()  # reparameterized sample
            
            # Enforce action bounds with tanh and calculate log_prob
            y_t = torch.tanh(x_t)
            action = y_t * self.action_scale
            
            # Calculate log probability, accounting for tanh squashing
            log_prob = normal.log_prob(x_t) - torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
            log_prob = log_prob.sum(1, keepdim=True)
        
        return action, log_prob

# SAC Q-Network
class QNetwork(nn.Module):
    """Q-Network to estimate the Q-value for state-action pairs."""
    
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(QNetwork, self).__init__()
        
        # Q1 network
        self.q1 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Q2 network (for reducing overestimation bias)
        self.q2 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        """Initialize neural network weights."""
        # Q1 weights
        for i in range(0, len(self.q1), 2):  # Only initialize Linear layers
            if isinstance(self.q1[i], nn.Linear):
                nn.init.xavier_uniform_(self.q1[i].weight)
                nn.init.constant_(self.q1[i].bias, 0.0)
        
        # Q2 weights
        for i in range(0, len(self.q2), 2):  # Only initialize Linear layers
            if isinstance(self.q2[i], nn.Linear):
                nn.init.xavier_uniform_(self.q2[i].weight)
                nn.init.constant_(self.q2[i].bias, 0.0)
        
    def forward(self, state, action):
        """Forward pass to calculate Q-values."""
        # Concatenate state and action
        x = torch.cat([state, action], dim=1)
        
        # Calculate Q1 and Q2 values
        q1 = self.q1(x)
        q2 = self.q2(x)
        
        return q1, q2
    
    def q1_forward(self, state, action):
        """Get only Q1 value (used for policy optimization)."""
        x = torch.cat([state, action], dim=1)
        return self.q1(x)

# SAC Position Sizer
class SACPositionSizer:
    """Position sizing using Soft Actor-Critic algorithm with entropy regularization."""
    
    def __init__(self, state_dim, action_dim=1, hidden_dim=128, replay_buffer_size=100000,
                 batch_size=256, gamma=0.99, tau=0.005, alpha=0.2, lr=3e-4, 
                 action_scale=1.0, smoothing_alpha=0.2, transaction_cost=0.000002,
                 target_entropy=None, auto_entropy_tuning=True):
        """
        Initialize SAC Position Sizer.
        
        Args:
            state_dim: Dimension of state space
            action_dim: Dimension of action space (1 for position sizing)
            hidden_dim: Hidden layer size
            replay_buffer_size: Capacity of experience replay buffer
            batch_size: Batch size for training
            gamma: Discount factor
            tau: Soft update coefficient
            alpha: Entropy regularization coefficient (or initial value if auto-tuned)
            lr: Learning rate
            action_scale: Maximum action value 
            smoothing_alpha: EMA smoothing factor for position sizing
            transaction_cost: Trading transaction cost
            target_entropy: Target entropy value for auto-tuning (if None, set to -action_dim)
            auto_entropy_tuning: Whether to automatically adjust entropy coefficient
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.batch_size = batch_size
        self.gamma = gamma
        self.tau = tau
        self.action_scale = action_scale
        self.smoothing_alpha = smoothing_alpha
        self.transaction_cost = transaction_cost
        self.auto_entropy_tuning = auto_entropy_tuning
        
        # Initialize networks
        self.policy = SACPolicy(state_dim, action_dim, hidden_dim, action_scale=action_scale).to(device)
        self.q_net = QNetwork(state_dim, action_dim, hidden_dim).to(device)
        self.target_q_net = QNetwork(state_dim, action_dim, hidden_dim).to(device)
        
        # Copy parameters from Q-network to target Q-network
        for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):
            target_param.data.copy_(param.data)
            
        # Initialize optimizers
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.q_optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        
        # Initialize replay buffer
        self.replay_buffer = ReplayBuffer(capacity=replay_buffer_size)
        
        # Entropy related parameters
        if self.auto_entropy_tuning:
            self.target_entropy = -np.prod(action_dim) if target_entropy is None else target_entropy
            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
            self.alpha = self.log_alpha.exp().item()
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)
        else:
            self.alpha = alpha
            
        # For tracking performance
        self.training_performance = {
            'episode': [],
            'final_reward': [],
            'cumulative_return': [],
            'transaction_costs': [],
            'sharpe_ratio': []
        }
        
        # Initialize step counters
        self.total_steps = 0
        self.updates = 0
        
    def update_parameters(self):
        """Update network parameters using SAC algorithm."""
        # Skip update if buffer doesn't have enough samples
        if len(self.replay_buffer) < self.batch_size:
            return
            
        # Sample a batch from the replay buffer
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
        
        # Update Q-networks
        with torch.no_grad():
            # Sample next actions and log probs from current policy
            next_actions, next_log_probs = self.policy.sample(next_states)
            
            # Get target Q-values
            next_q1, next_q2 = self.target_q_net(next_states, next_actions)
            next_q = torch.min(next_q1, next_q2)
            
            # Calculate target values with entropy regularization
            target_q = rewards + self.gamma * (1 - dones) * (next_q - self.alpha * next_log_probs)
        
        # Current Q-values
        current_q1, current_q2 = self.q_net(states, actions)
        
        # Calculate Q-network loss (MSE)
        q1_loss = F.mse_loss(current_q1, target_q)
        q2_loss = F.mse_loss(current_q2, target_q)
        q_loss = q1_loss + q2_loss
        
        # Update Q-networks
        self.q_optimizer.zero_grad()
        q_loss.backward()
        self.q_optimizer.step()
        
        # Update policy network
        new_actions, log_probs = self.policy.sample(states)
        q_values = self.q_net.q1_forward(states, new_actions)
        
        # Policy loss = E[α * log π(a|s) - Q(s,a)]
        policy_loss = (self.alpha * log_probs - q_values).mean()
        
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()
        
        # Update alpha (entropy coefficient) if auto-tuning
        if self.auto_entropy_tuning:
            # Calculate alpha loss
            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
            
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
            
            # Update alpha value
            self.alpha = self.log_alpha.exp().item()
            
        # Soft update target Q-network
        for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):
            target_param.data.copy_(
                target_param.data * (1.0 - self.tau) + param.data * self.tau
            )
            
        # Increment update counter
        self.updates += 1
    
    def prepare_state(self, data, idx, meta_col, regime_col, features_cols, current_position):
        """Prepare state representation for the model."""
        # Get meta signal
        meta_signal = data.iloc[idx][meta_col]
        
        # Get regime and one-hot encode it
        regime = data.iloc[idx][regime_col]
        regime_one_hot = np.zeros(3)  # Assuming 3 regimes: Bull, Bear, Sideways
        regime_one_hot[int(regime)] = 1
        
        # Get features
        features = data.iloc[idx][features_cols].values
        
        # Combine all into state vector
        state = np.hstack([
            np.array([meta_signal]),
            np.array([current_position]),
            regime_one_hot,
            features
        ])
        
        return state
    
    def calculate_reward(self, data, idx, action, prev_action, returns_col):
        """Calculate reward for the action taken."""
        # Get return for this time step
        returns = data.iloc[idx][returns_col]
        
        # Calculate profit/loss based on the action (position size)
        profit = returns * action
        
        # Calculate transaction cost
        size_change = abs(action - prev_action)
        transaction_cost = size_change * self.transaction_cost
        
        # Final reward is profit minus transaction cost
        reward = profit - transaction_cost
        
        return reward, profit, transaction_cost
    
    def train(self, data, meta_col, regime_col, features_cols, returns_col, episodes=50, 
             base_size=1.0, eval_interval=5, updates_per_step=1):
        """
        Train the SAC model on the given data.
        
        Args:
            data: DataFrame with features and returns
            meta_col: Column name for meta-labeling signal
            regime_col: Column name for market regime
            features_cols: List of feature column names
            returns_col: Column name for returns
            episodes: Number of episodes to train
            base_size: Base position size (maximum)
            eval_interval: Interval to evaluate and save model
            updates_per_step: Number of parameter updates per step
        
        Returns:
            Dict with training performance metrics
        """
        best_reward = -np.inf
        
        for episode in range(1, episodes + 1):
            # Reset tracking variables
            total_reward = 0
            cumulative_return = 0
            total_transaction_cost = 0
            returns_history = []
            
            # Start with zero position
            current_position = 0
            
            # Loop through the entire dataset
            for idx in range(len(data)):
                # Prepare state
                state = self.prepare_state(data, idx, meta_col, regime_col, features_cols, current_position)
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                
                # Sample action from policy
                with torch.no_grad():
                    action_tensor, _ = self.policy.sample(state_tensor, deterministic=False)
                action = action_tensor.cpu().numpy()[0][0]
                
                # Apply EMA smoothing to avoid large position changes
                smooth_action = current_position + self.smoothing_alpha * (action - current_position)
                
                # Calculate reward
                reward, profit, transaction_cost = self.calculate_reward(
                    data, idx, smooth_action, current_position, returns_col
                )
                
                # Prepare next state
                next_idx = min(idx + 1, len(data) - 1)
                next_state = self.prepare_state(data, next_idx, meta_col, regime_col, 
                                               features_cols, smooth_action)
                
                # Determine if terminal state
                done = (next_idx == len(data) - 1)
                
                # Store transition in replay buffer
                self.replay_buffer.push(state, np.array([smooth_action]), reward, next_state, done)
                
                # Update networks
                for _ in range(updates_per_step):
                    self.update_parameters()
                
                # Update tracking variables
                total_reward += reward
                cumulative_return += profit
                total_transaction_cost += transaction_cost
                returns_history.append(profit)
                
                # Update current position for next step
                current_position = smooth_action
                
                # Increment step counter
                self.total_steps += 1
            
            # Calculate Sharpe ratio
            if len(returns_history) > 1:
                sharpe = np.mean(returns_history) / (np.std(returns_history) + 1e-8) * np.sqrt(252)
            else:
                sharpe = 0
            
            # Record performance
            self.training_performance['episode'].append(episode)
            self.training_performance['final_reward'].append(total_reward)
            self.training_performance['cumulative_return'].append(cumulative_return)
            self.training_performance['transaction_costs'].append(total_transaction_cost)
            self.training_performance['sharpe_ratio'].append(sharpe)
            
            # Print progress
            if episode % eval_interval == 0 or episode == 1:
                print(f"Episode {episode}/{episodes} - "
                      f"Reward: {total_reward:.4f}, "
                      f"Return: {cumulative_return:.4f}, "
                      f"Costs: {total_transaction_cost:.4f}, "
                      f"Sharpe: {sharpe:.4f}, "
                      f"Alpha: {self.alpha:.4f}")
                
                # Save best model
                if total_reward > best_reward:
                    best_reward = total_reward
                    self.save('models/sac_position_sizer_best')
        
        return self.training_performance
    
    def apply(self, data, meta_col, regime_col, features_cols, returns_col, base_size=1.0):
        """
        Apply the trained model to new data.
        
        Args:
            data: DataFrame with features and returns
            meta_col: Column name for meta-labeling signal
            regime_col: Column name for market regime
            features_cols: List of feature column names
            returns_col: Column name for returns
            base_size: Base position size (maximum)
        
        Returns:
            DataFrame with original data plus model's position sizes
        """
        # Make a copy to avoid modifying the original
        result_df = data.copy()
        
        # Add new column for position sizes
        result_df['sac_position_size'] = np.zeros(len(result_df))
        
        # Start with zero position
        current_position = 0
        
        # Loop through the data
        for idx in range(len(data)):
            # Prepare state
            state = self.prepare_state(data, idx, meta_col, regime_col, features_cols, current_position)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            
            # Get action (position size) using deterministic policy for evaluation
            with torch.no_grad():
                action_tensor, _ = self.policy.sample(state_tensor, deterministic=True)
            action = action_tensor.cpu().numpy()[0][0]
            
            # Apply EMA smoothing
            smooth_action = current_position + self.smoothing_alpha * (action - current_position)
            
            # Store in result DataFrame
            result_df.iloc[idx, result_df.columns.get_loc('sac_position_size')] = smooth_action
            
            # Update current position for next step
            current_position = smooth_action
        
        return result_df
    
    def save(self, path):
        """Save the model to disk."""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        model_data = {
            'policy_state_dict': self.policy.state_dict(),
            'q_net_state_dict': self.q_net.state_dict(),
            'target_q_net_state_dict': self.target_q_net.state_dict(),
            'hyperparams': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'hidden_dim': self.hidden_dim,
                'batch_size': self.batch_size,
                'gamma': self.gamma,
                'tau': self.tau,
                'action_scale': self.action_scale,
                'smoothing_alpha': self.smoothing_alpha,
                'transaction_cost': self.transaction_cost,
                'auto_entropy_tuning': self.auto_entropy_tuning
            },
            'training_performance': self.training_performance
        }
        
        if self.auto_entropy_tuning:
            model_data['log_alpha'] = self.log_alpha.item()
            model_data['target_entropy'] = self.target_entropy
        else:
            model_data['alpha'] = self.alpha
        
        torch.save(model_data, path)
        print(f"Model saved to {path}")
    
    @classmethod
    def load(cls, path):
        """Load the model from disk."""
        model_data = torch.load(path)
        
        # Determine if using auto entropy tuning
        auto_entropy_tuning = model_data['hyperparams']['auto_entropy_tuning']
        
        # Create hyperparameters dict
        hyperparams = model_data['hyperparams'].copy()
        
        if auto_entropy_tuning:
            hyperparams['target_entropy'] = model_data['target_entropy']
        else:
            hyperparams['alpha'] = model_data['alpha']
        
        # Create instance with saved hyperparameters
        instance = cls(**hyperparams)
        
        # Load network weights
        instance.policy.load_state_dict(model_data['policy_state_dict'])
        instance.q_net.load_state_dict(model_data['q_net_state_dict'])
        instance.target_q_net.load_state_dict(model_data['target_q_net_state_dict'])
        
        # Load alpha for auto entropy tuning
        if auto_entropy_tuning:
            with torch.no_grad():
                instance.log_alpha.copy_(torch.tensor([model_data['log_alpha']], device=device))
            instance.alpha = instance.log_alpha.exp().item()
        
        # Load training performance if available
        if 'training_performance' in model_data:
            instance.training_performance = model_data['training_performance']
        
        return instance

def evaluate_sac_strategy(data, base_position_col, sac_position_col, returns_col, transaction_cost=0.000002):
    """
    Evaluate the SAC-based position sizing strategy against the base strategy.
    
    Args:
        data: DataFrame with both base and SAC position sizes
        base_position_col: Column name for base position signals
        sac_position_col: Column name for SAC position sizes
        returns_col: Column name for returns
        transaction_cost: Transaction cost per unit change
    
    Returns:
        Tuple of (metrics_dict, evaluated_data_df)
    """
    # Make a copy to avoid modifying the original
    result = data.copy()
    
    # Calculate base strategy returns and transaction costs
    base_position_shifted = result[base_position_col].shift(1).fillna(0)  # Shift to avoid lookahead bias
    base_returns = base_position_shifted * result[returns_col]
    base_tc = np.abs(result[base_position_col].diff()).fillna(0) * transaction_cost
    
    # Calculate SAC strategy returns and transaction costs
    sac_position_shifted = result[sac_position_col].shift(1).fillna(0)  # Shift to avoid lookahead bias
    sac_returns = sac_position_shifted * result[returns_col]
    sac_tc = np.abs(result[sac_position_col].diff()).fillna(0) * transaction_cost
    
    # Store in result DataFrame
    result['base_returns'] = base_returns
    result['base_tc'] = base_tc
    result['base_net_returns'] = base_returns - base_tc
    
    result['sac_returns'] = sac_returns
    result['sac_tc'] = sac_tc
    result['sac_net_returns'] = sac_returns - sac_tc
    
    # Calculate metrics
    metrics = {}
    
    # Base strategy metrics
    metrics['base_total_return'] = result['base_returns'].sum()
    metrics['base_transaction_costs'] = result['base_tc'].sum()
    metrics['base_net_return'] = metrics['base_total_return'] - metrics['base_transaction_costs']
    metrics['base_sharpe'] = (result['base_net_returns'].mean() / result['base_net_returns'].std() * np.sqrt(252)
                             if result['base_net_returns'].std() > 0 else 0)
    
    # Calculate drawdown for base strategy
    cum_returns = (1 + result['base_net_returns']).cumprod()
    running_max = cum_returns.cummax()
    drawdown = (cum_returns / running_max - 1)
    metrics['base_max_drawdown'] = drawdown.min()
    
    # Win rate for base strategy
    metrics['base_win_rate'] = (result['base_net_returns'] > 0).mean()
    
    # Average position size and trade frequency for base strategy
    metrics['base_avg_position_size'] = result[base_position_col].abs().mean()
    metrics['base_trade_frequency'] = (result[base_position_col].diff() != 0).mean()
    
    # SAC strategy metrics
    metrics['sac_total_return'] = result['sac_returns'].sum()
    metrics['sac_transaction_costs'] = result['sac_tc'].sum()
    metrics['sac_net_return'] = metrics['sac_total_return'] - metrics['sac_transaction_costs']
    metrics['sac_sharpe'] = (result['sac_net_returns'].mean() / result['sac_net_returns'].std() * np.sqrt(252)
                             if result['sac_net_returns'].std() > 0 else 0)
    
    # Calculate drawdown for SAC strategy
    cum_returns = (1 + result['sac_net_returns']).cumprod()
    running_max = cum_returns.cummax()
    drawdown = (cum_returns / running_max - 1)
    metrics['sac_max_drawdown'] = drawdown.min()
    
    # Win rate for SAC strategy
    metrics['sac_win_rate'] = (result['sac_net_returns'] > 0).mean()
    
    # Average position size and trade frequency for SAC strategy
    metrics['sac_avg_position_size'] = result[sac_position_col].abs().mean()
    metrics['sac_trade_frequency'] = (result[sac_position_col].diff() != 0).mean()
    
    # Plot cumulative returns
    plt.figure(figsize=(12, 6))
    
    # Calculate cumulative returns for both strategies
    base_cum_returns = (1 + result['base_net_returns']).cumprod() - 1
    sac_cum_returns = (1 + result['sac_net_returns']).cumprod() - 1
    
    plt.plot(base_cum_returns.index, base_cum_returns, label='Base Strategy')
    plt.plot(sac_cum_returns.index, sac_cum_returns, label='SAC Strategy')
    
    plt.title('Cumulative Returns Comparison')
    plt.xlabel('Date')
    plt.ylabel('Cumulative Return')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('sac_vs_base_returns.png')
    
    # Plot position sizes comparison
    plt.figure(figsize=(12, 6))
    
    plt.plot(result.index, result[base_position_col], label='Base Position', alpha=0.7)
    plt.plot(result.index, result[sac_position_col], label='SAC Position', alpha=0.7)
    
    plt.title('Position Sizes Comparison')
    plt.xlabel('Date')
    plt.ylabel('Position Size')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('sac_vs_base_positions.png')
    
    # Print metrics
    print("\nPerformance Metrics:")
    print(f"Base Strategy - Net Return: {metrics['base_net_return']:.4f}, "
          f"Sharpe: {metrics['base_sharpe']:.4f}, "
          f"Max DD: {metrics['base_max_drawdown']:.4f}")
    print(f"SAC Strategy - Net Return: {metrics['sac_net_return']:.4f}, "
          f"Sharpe: {metrics['sac_sharpe']:.4f}, "
          f"Max DD: {metrics['sac_max_drawdown']:.4f}")
    
    return metrics, result

================
File: ppo_position_sizing.py
================
"""
PPO-based Position Sizing model for dynamic trading position sizing.

This module implements a Proximal Policy Optimization (PPO) algorithm for 
dynamically adjusting position sizes in trading strategies based on market regimes
and meta-labeling signals.
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pandas as pd
from collections import namedtuple, deque
import matplotlib.pyplot as plt
import time
import pickle

# Set seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Define device for PyTorch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# PPO Memory Buffer
class PPOMemory:
    def __init__(self, batch_size=64):
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.values = []
        self.dones = []
        self.batch_size = batch_size
        
    def store(self, state, action, log_prob, value, reward, done):
        self.states.append(state)
        self.actions.append(action)
        self.log_probs.append(log_prob)
        self.values.append(value)
        self.rewards.append(reward)
        self.dones.append(done)
        
    def clear(self):
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.values = []
        self.dones = []
        
    def get_generator(self, advantages, normalized_advantages=True):
        n_states = len(self.states)
        batch_start = np.arange(0, n_states, self.batch_size)
        indices = np.arange(n_states, dtype=np.int64)
        np.random.shuffle(indices)
        batches = [indices[i:i+self.batch_size] for i in batch_start]
        
        # Convert all to tensors
        states = torch.FloatTensor(np.array(self.states)).to(device)
        actions = torch.FloatTensor(np.array(self.actions)).to(device)
        old_log_probs = torch.FloatTensor(np.array(self.log_probs)).to(device)
        values = torch.FloatTensor(np.array(self.values)).to(device)
        
        # Convert advantages to tensor and normalize if required
        advantages = torch.FloatTensor(advantages).to(device)
        if normalized_advantages and len(advantages) > 1:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
        # Return batches
        for batch in batches:
            yield states[batch], actions[batch], old_log_probs[batch], values[batch], advantages[batch]

# Actor Network (Policy)
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64, action_scale=1.0):
        super(Actor, self).__init__()
        self.action_scale = action_scale
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )
        
        # Mean and std deviation for the normal distribution
        self.mu = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Parameter(torch.zeros(action_dim))
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))
                nn.init.constant_(layer.bias, 0)
        
        nn.init.orthogonal_(self.mu.weight, gain=0.01)
        nn.init.constant_(self.mu.bias, 0)
    
    def forward(self, state):
        x = self.network(state)
        mu = self.mu(x)
        std = torch.exp(self.log_std).expand_as(mu)
        
        # Return mean and std for the normal distribution
        return mu, std
    
    def get_action(self, state, evaluate=False):
        # Convert state to tensor if it's a numpy array
        if isinstance(state, np.ndarray):
            state = torch.FloatTensor(state).to(device)
            
        # Make sure state is 2D
        if state.dim() == 1:
            state = state.unsqueeze(0)
            
        # Get mean and std
        mu, std = self.forward(state)
        
        # Create normal distribution
        dist = torch.distributions.Normal(mu, std)
        
        # In evaluation mode, return mean directly
        if evaluate:
            action = mu
        else:
            # Sample from distribution
            action = dist.sample()
            
        # Calculate log probability
        log_prob = dist.log_prob(action).sum(-1, keepdim=True)
        
        # Apply scaling and squashing to [0, action_scale]
        # Using sigmoid to bound between 0 and 1, then multiply by action_scale
        action_scaled = torch.sigmoid(action) * self.action_scale
        
        return action_scaled, log_prob

# Critic Network (Value function)
class Critic(nn.Module):
    def __init__(self, state_dim, hidden_dim=64):
        super(Critic, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))
                nn.init.constant_(layer.bias, 0)
    
    def forward(self, state):
        return self.network(state)

# PPO Position Sizer class
class PPOPositionSizer:
    def __init__(self, state_dim, action_dim=1, hidden_dim=64, learning_rate=3e-4, 
                 gamma=0.99, gae_lambda=0.95, clip_param=0.2, entropy_coef=0.01,
                 value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, batch_size=64,
                 target_kl=0.01, action_scale=1.0, smoothing_alpha=0.2, transaction_cost=0.000002):
        """
        Initialize PPO Position Sizer.
        
        Args:
            state_dim: Dimension of state space
            action_dim: Dimension of action space
            hidden_dim: Hidden layer size
            learning_rate: Learning rate for optimizer
            gamma: Discount factor
            gae_lambda: GAE lambda parameter
            clip_param: PPO clipping parameter
            entropy_coef: Entropy coefficient for loss
            value_coef: Value function coefficient
            max_grad_norm: Max norm of gradients
            ppo_epochs: Number of epochs per update
            batch_size: Batch size for training
            target_kl: Target KL divergence
            action_scale: Maximum action value
            smoothing_alpha: EMA smoothing factor for position sizing
            transaction_cost: Trading transaction cost
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_param = clip_param
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        self.max_grad_norm = max_grad_norm
        self.ppo_epochs = ppo_epochs
        self.batch_size = batch_size
        self.target_kl = target_kl
        self.action_scale = action_scale
        self.smoothing_alpha = smoothing_alpha
        self.transaction_cost = transaction_cost
        
        # Initialize actor and critic networks
        self.actor = Actor(state_dim, action_dim, hidden_dim, action_scale).to(device)
        self.critic = Critic(state_dim, hidden_dim).to(device)
        
        # Optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)
        
        # Memory buffer
        self.memory = PPOMemory(batch_size)
        
        # For tracking performance
        self.training_performance = {
            'episode': [],
            'final_reward': [],
            'cumulative_return': [],
            'transaction_costs': [],
            'sharpe_ratio': []
        }
        
    def prepare_state(self, data, idx, meta_col, regime_col, features_cols, current_position):
        """Prepare state representation for the model."""
        # Get meta signal
        meta_signal = data.iloc[idx][meta_col]
        
        # Get regime and one-hot encode it
        regime = data.iloc[idx][regime_col]
        regime_one_hot = np.zeros(3)  # Assuming 3 regimes: Bull, Bear, Sideways
        regime_one_hot[int(regime)] = 1
        
        # Get features
        features = data.iloc[idx][features_cols].values
        
        # Combine all into state vector
        state = np.hstack([
            np.array([meta_signal]),
            np.array([current_position]),
            regime_one_hot,
            features
        ])
        
        return state
    
    def calculate_reward(self, data, idx, action, prev_action, returns_col):
        """Calculate reward for the action taken."""
        # Get return for this time step
        returns = data.iloc[idx][returns_col]
        
        # Calculate profit/loss based on the action (position size)
        profit = returns * action
        
        # Calculate transaction cost
        size_change = abs(action - prev_action)
        transaction_cost = size_change * self.transaction_cost
        
        # Final reward is profit minus transaction cost
        reward = profit - transaction_cost
        
        return reward, profit, transaction_cost
    
    def compute_gae(self, next_value, rewards, values, dones):
        """Compute Generalized Advantage Estimation."""
        values = values + [next_value]
        advantages = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.gamma * values[t+1] * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)
            
        returns = np.array(advantages) + np.array(values[:-1])
        advantages = np.array(advantages)
        
        return returns, advantages
    
    def update_policy(self, returns, advantages):
        """Update policy using PPO algorithm."""
        for _ in range(self.ppo_epochs):
            # Sample batch from memory
            for states, actions, old_log_probs, old_values, batch_advantages in self.memory.get_generator(advantages):
                # Get new log probs and values
                _, std = self.actor(states)
                dist = torch.distributions.Normal(actions, std)
                new_log_probs = dist.log_prob(actions).sum(-1, keepdim=True)
                new_values = self.critic(states)
                
                # Calculate ratios
                ratios = torch.exp(new_log_probs - old_log_probs)
                
                # Calculate surrogate losses
                surr1 = ratios * batch_advantages
                surr2 = torch.clamp(ratios, 1-self.clip_param, 1+self.clip_param) * batch_advantages
                
                # Calculate actor loss
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # Calculate critic loss
                returns = returns[:len(batch_advantages)]
                returns_tensor = torch.FloatTensor(returns).to(device)
                critic_loss = F.mse_loss(new_values.squeeze(), returns_tensor)
                
                # Calculate entropy
                entropy = torch.distributions.Normal(actions, std).entropy().mean()
                
                # Total loss
                loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy
                
                # Optimize
                self.actor_optimizer.zero_grad()
                self.critic_optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
                nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
                self.actor_optimizer.step()
                self.critic_optimizer.step()
                
                # Calculate approximate KL divergence
                approx_kl = ((old_log_probs - new_log_probs).exp() - 1 - (old_log_probs - new_log_probs)).mean().item()
                
                # Break if KL divergence is too large
                if approx_kl > self.target_kl:
                    break
    
    def train(self, data, meta_col, regime_col, features_cols, returns_col, episodes=50, 
              base_size=1.0, eval_interval=5, update_interval=1000):
        """
        Train the PPO model on the given data.
        
        Args:
            data: DataFrame with features and returns
            meta_col: Column name for meta-labeling signal
            regime_col: Column name for market regime
            features_cols: List of feature column names
            returns_col: Column name for returns
            episodes: Number of episodes to train
            base_size: Base position size (maximum)
            eval_interval: Interval to evaluate and save model
            update_interval: Steps between policy updates
        
        Returns:
            Dict with training performance metrics
        """
        best_reward = -np.inf
        
        for episode in range(1, episodes + 1):
            # Reset tracking variables
            total_reward = 0
            cumulative_return = 0
            total_transaction_cost = 0
            returns_history = []
            
            # Start with zero position
            current_position = 0
            
            # Clear memory buffer
            self.memory.clear()
            
            # Loop through the entire dataset
            for idx in range(len(data)):
                # Prepare state
                state = self.prepare_state(data, idx, meta_col, regime_col, features_cols, current_position)
                state_tensor = torch.FloatTensor(state).to(device)
                
                # Get action (position size)
                action_tensor, log_prob = self.actor.get_action(state_tensor)
                action = action_tensor.cpu().detach().numpy()[0][0]
                
                # Apply EMA smoothing to avoid large position changes
                smooth_action = current_position + self.smoothing_alpha * (action - current_position)
                
                # Get value estimate
                value = self.critic(state_tensor).cpu().detach().numpy()[0]
                
                # Calculate reward
                reward, profit, transaction_cost = self.calculate_reward(
                    data, idx, smooth_action, current_position, returns_col
                )
                
                # Update trackers
                total_reward += reward
                cumulative_return += profit
                total_transaction_cost += transaction_cost
                returns_history.append(profit)
                
                # Store in memory
                done = (idx == len(data) - 1)
                self.memory.store(state, action_tensor.cpu().detach().numpy()[0], 
                                  log_prob.cpu().detach().numpy()[0], value, reward, done)
                
                # Update current position for next step
                current_position = smooth_action
                
                # Update policy if enough steps
                if len(self.memory.states) == update_interval:
                    # Calculate next state value for GAE
                    if not done:
                        next_idx = min(idx + 1, len(data) - 1)
                        next_state = self.prepare_state(data, next_idx, meta_col, regime_col, 
                                                       features_cols, current_position)
                        next_state_tensor = torch.FloatTensor(next_state).to(device)
                        next_value = self.critic(next_state_tensor).cpu().detach().numpy()[0]
                    else:
                        next_value = 0
                    
                    # Compute GAE
                    returns, advantages = self.compute_gae(
                        next_value, self.memory.rewards, self.memory.values, self.memory.dones
                    )
                    
                    # Update policy
                    self.update_policy(returns, advantages)
                    
                    # Clear memory
                    self.memory.clear()
            
            # Calculate Sharpe ratio
            if len(returns_history) > 1:
                sharpe = np.mean(returns_history) / (np.std(returns_history) + 1e-8) * np.sqrt(252)
            else:
                sharpe = 0
            
            # Record performance
            self.training_performance['episode'].append(episode)
            self.training_performance['final_reward'].append(total_reward)
            self.training_performance['cumulative_return'].append(cumulative_return)
            self.training_performance['transaction_costs'].append(total_transaction_cost)
            self.training_performance['sharpe_ratio'].append(sharpe)
            
            # Print progress
            if episode % eval_interval == 0:
                print(f"Episode {episode}/{episodes} - "
                      f"Reward: {total_reward:.4f}, "
                      f"Return: {cumulative_return:.4f}, "
                      f"Costs: {total_transaction_cost:.4f}, "
                      f"Sharpe: {sharpe:.4f}")
                
                # Save best model
                if total_reward > best_reward:
                    best_reward = total_reward
                    self.save('models/ppo_position_sizer_best')
        
        return self.training_performance
    
    def apply(self, data, meta_col, regime_col, features_cols, returns_col, base_size=1.0):
        """
        Apply the trained model to new data.
        
        Args:
            data: DataFrame with features and returns
            meta_col: Column name for meta-labeling signal
            regime_col: Column name for market regime
            features_cols: List of feature column names
            returns_col: Column name for returns
            base_size: Base position size (maximum)
        
        Returns:
            DataFrame with original data plus model's position sizes
        """
        # Make a copy to avoid modifying the original
        result_df = data.copy()
        
        # Add new column for position sizes
        result_df['ppo_position_size'] = np.zeros(len(result_df))
        
        # Start with zero position
        current_position = 0
        
        # Loop through the data
        for idx in range(len(data)):
            # Prepare state
            state = self.prepare_state(data, idx, meta_col, regime_col, features_cols, current_position)
            state_tensor = torch.FloatTensor(state).to(device)
            
            # Get action (position size) using the mean of the distribution (no exploration)
            action_tensor, _ = self.actor.get_action(state_tensor, evaluate=True)
            action = action_tensor.cpu().detach().numpy()[0][0]
            
            # Apply EMA smoothing
            smooth_action = current_position + self.smoothing_alpha * (action - current_position)
            
            # Store in result DataFrame
            result_df.iloc[idx, result_df.columns.get_loc('ppo_position_size')] = smooth_action
            
            # Update current position for next step
            current_position = smooth_action
        
        return result_df
    
    def save(self, path):
        """Save the model to disk."""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        model_data = {
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'hyperparams': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'hidden_dim': self.hidden_dim,
                'learning_rate': self.learning_rate,
                'gamma': self.gamma,
                'gae_lambda': self.gae_lambda,
                'clip_param': self.clip_param,
                'entropy_coef': self.entropy_coef,
                'value_coef': self.value_coef,
                'max_grad_norm': self.max_grad_norm,
                'action_scale': self.action_scale,
                'smoothing_alpha': self.smoothing_alpha,
                'transaction_cost': self.transaction_cost
            },
            'training_performance': self.training_performance
        }
        
        torch.save(model_data, path)
        print(f"Model saved to {path}")
    
    @classmethod
    def load(cls, path):
        """Load the model from disk."""
        model_data = torch.load(path)
        
        # Create instance with saved hyperparameters
        instance = cls(**model_data['hyperparams'])
        
        # Load network weights
        instance.actor.load_state_dict(model_data['actor_state_dict'])
        instance.critic.load_state_dict(model_data['critic_state_dict'])
        
        # Load training performance if available
        if 'training_performance' in model_data:
            instance.training_performance = model_data['training_performance']
        
        return instance

def evaluate_ppo_strategy(data, base_position_col, ppo_position_col, returns_col, transaction_cost=0.000002):
    """
    Evaluate the PPO-based position sizing strategy against the base strategy.
    
    Args:
        data: DataFrame with both base and PPO position sizes
        base_position_col: Column name for base position signals
        ppo_position_col: Column name for PPO position sizes
        returns_col: Column name for returns
        transaction_cost: Transaction cost per unit change
    
    Returns:
        Tuple of (metrics_dict, evaluated_data_df)
    """
    # Make a copy to avoid modifying the original
    result = data.copy()
    
    # Calculate base strategy returns and transaction costs
    base_position_shifted = result[base_position_col].shift(1).fillna(0)  # Shift to avoid lookahead bias
    base_returns = base_position_shifted * result[returns_col]
    base_tc = np.abs(result[base_position_col].diff()).fillna(0) * transaction_cost
    
    # Calculate PPO strategy returns and transaction costs
    ppo_position_shifted = result[ppo_position_col].shift(1).fillna(0)  # Shift to avoid lookahead bias
    ppo_returns = ppo_position_shifted * result[returns_col]
    ppo_tc = np.abs(result[ppo_position_col].diff()).fillna(0) * transaction_cost
    
    # Store in result DataFrame
    result['base_returns'] = base_returns
    result['base_tc'] = base_tc
    result['base_net_returns'] = base_returns - base_tc
    
    result['ppo_returns'] = ppo_returns
    result['ppo_tc'] = ppo_tc
    result['ppo_net_returns'] = ppo_returns - ppo_tc
    
    # Calculate metrics
    metrics = {}
    
    # Base strategy metrics
    metrics['base_total_return'] = result['base_returns'].sum()
    metrics['base_transaction_costs'] = result['base_tc'].sum()
    metrics['base_net_return'] = metrics['base_total_return'] - metrics['base_transaction_costs']
    metrics['base_sharpe'] = (result['base_net_returns'].mean() / result['base_net_returns'].std() * np.sqrt(252)
                             if result['base_net_returns'].std() > 0 else 0)
    
    # Calculate drawdown for base strategy
    cum_returns = (1 + result['base_net_returns']).cumprod()
    running_max = cum_returns.cummax()
    drawdown = (cum_returns / running_max - 1)
    metrics['base_max_drawdown'] = drawdown.min()
    
    # Win rate for base strategy
    metrics['base_win_rate'] = (result['base_net_returns'] > 0).mean()
    
    # Average position size and trade frequency for base strategy
    metrics['base_avg_position_size'] = result[base_position_col].abs().mean()
    metrics['base_trade_frequency'] = (result[base_position_col].diff() != 0).mean()
    
    # PPO strategy metrics
    metrics['ppo_total_return'] = result['ppo_returns'].sum()
    metrics['ppo_transaction_costs'] = result['ppo_tc'].sum()
    metrics['ppo_net_return'] = metrics['ppo_total_return'] - metrics['ppo_transaction_costs']
    metrics['ppo_sharpe'] = (result['ppo_net_returns'].mean() / result['ppo_net_returns'].std() * np.sqrt(252)
                             if result['ppo_net_returns'].std() > 0 else 0)
    
    # Calculate drawdown for PPO strategy
    cum_returns = (1 + result['ppo_net_returns']).cumprod()
    running_max = cum_returns.cummax()
    drawdown = (cum_returns / running_max - 1)
    metrics['ppo_max_drawdown'] = drawdown.min()
    
    # Win rate for PPO strategy
    metrics['ppo_win_rate'] = (result['ppo_net_returns'] > 0).mean()
    
    # Average position size and trade frequency for PPO strategy
    metrics['ppo_avg_position_size'] = result[ppo_position_col].abs().mean()
    metrics['ppo_trade_frequency'] = (result[ppo_position_col].diff() != 0).mean()
    
    # Plot cumulative returns
    plt.figure(figsize=(12, 6))
    
    # Calculate cumulative returns for both strategies
    base_cum_returns = (1 + result['base_net_returns']).cumprod() - 1
    ppo_cum_returns = (1 + result['ppo_net_returns']).cumprod() - 1
    
    plt.plot(base_cum_returns.index, base_cum_returns, label='Base Strategy')
    plt.plot(ppo_cum_returns.index, ppo_cum_returns, label='PPO Strategy')
    
    plt.title('Cumulative Returns Comparison')
    plt.xlabel('Date')
    plt.ylabel('Cumulative Return')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('ppo_vs_base_returns.png')
    
    # Plot position sizes comparison
    plt.figure(figsize=(12, 6))
    
    plt.plot(result.index, result[base_position_col], label='Base Position', alpha=0.7)
    plt.plot(result.index, result[ppo_position_col], label='PPO Position', alpha=0.7)
    
    plt.title('Position Sizes Comparison')
    plt.xlabel('Date')
    plt.ylabel('Position Size')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('ppo_vs_base_positions.png')
    
    # Print metrics
    print("\nPerformance Metrics:")
    print(f"Base Strategy - Net Return: {metrics['base_net_return']:.4f}, "
          f"Sharpe: {metrics['base_sharpe']:.4f}, "
          f"Max DD: {metrics['base_max_drawdown']:.4f}")
    print(f"PPO Strategy - Net Return: {metrics['ppo_net_return']:.4f}, "
          f"Sharpe: {metrics['ppo_sharpe']:.4f}, "
          f"Max DD: {metrics['ppo_max_drawdown']:.4f}")
    
    return metrics, result

================
File: project_plan.py
================
"""
Project Plan for DL_MetaLabeling Framework

This file serves as a blueprint for integrating the three main components:
1. Market Regime Detection
2. Meta-Labeling
3. Deep Learning Position Sizing

This framework allows for the combination of these techniques to enhance trading strategies.
"""

import pandas as pd
import numpy as np
import logging
import os
import torch
from typing import Dict, List, Tuple, Union, Any

# Import components from our package
from dl_metalabeling.market_regimes import HMMRegimeDetector, TransformerRegimeDetector
from dl_metalabeling.metalabeling import MetaLabeling, RegimeMetaLabeling
from dl_metalabeling.models.base_rl import RLPositionSizer
from dl_metalabeling.models.dqn import DQNPositionSizer
from dl_metalabeling.models.ppo import PPOPositionSizer
from dl_metalabeling.models.sac import SACPositionSizer
from dl_metalabeling.models.dreamer import DreamerPositionSizer
from dl_metalabeling.utils import load_data, preprocess_data, add_returns, add_technical_indicators, add_volatility_features

logger = logging.getLogger(__name__)


class DLMetaLabelingFramework:
    """
    End-to-end framework that integrates market regime detection, meta-labeling, 
    and deep learning position sizing.
    """
    
    def __init__(self, config: Dict = None):
        """
        Initialize the framework with configuration.
        
        Args:
            config (dict): Configuration dictionary
        """
        self.config = config or self._get_default_config()
        
        # Components
        self.regime_detectors = {}
        self.meta_labelers = {}
        self.position_sizers = {}
        
        # Data
        self.data = None
        self.train_data = None
        self.test_data = None
        
        logger.info("Initialized DL MetaLabeling Framework")
    
    def _get_default_config(self) -> Dict:
        """
        Get default configuration.
        
        Returns:
            dict: Default configuration
        """
        return {
            # Data config
            'data': {
                'filepath': None,
                'signal_col': 'cmma_signal',
                'price_col': 'gbclose',
                'returns_col': 'returns',
                'train_test_split': 0.7,
                'random_state': 42
            },
            
            # Preprocessing config
            'preprocessing': {
                'calculate_returns': True,
                'calculate_volatility': True,
                'window_sizes': [5, 10, 20, 50],
                'features_to_normalize': ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126'],
                'technical_indicators': ['rsi', 'macd', 'bbands', 'ema']
            },
            
            # Regime detection config
            'regime_detection': {
                'methods': ['hmm', 'transformer'],
                'hmm': {
                    'n_regimes': 3,
                    'features': ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126'],
                    'cov_type': 'full'
                },
                'transformer': {
                    'n_regimes': 3,
                    'features': ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126'],
                    'embedding_dim': 64,
                    'num_heads': 4,
                    'num_layers': 2,
                    'batch_size': 32,
                    'epochs': 100,
                    'learning_rate': 0.001
                }
            },
            
            # Meta-labeling config
            'meta_labeling': {
                'models': ['random_forest'],
                'regime_specific': True,
                'random_forest': {
                    'features': ['volatility_5', 'volatility_20', 'volatility', 'rsi', 'macd', 'hmm_regime', 'transformer_regime'],
                    'target': 'direction',
                    'model_type': 'random_forest',
                    'labeling_method': 'triple_barrier',
                    'cv_folds': 5,
                    'hyperparams': {
                        'n_estimators': 100,
                        'max_depth': 5,
                        'min_samples_split': 10,
                        'min_samples_leaf': 5
                    },
                    'regime_specific': True,
                    'regime_col': 'hmm_regime'
                },
                'lightgbm': {
                    'features': ['volatility_5', 'volatility_20', 'volatility', 'rsi', 'macd', 'hmm_regime', 'transformer_regime'],
                    'target': 'direction',
                    'model_type': 'lightgbm',
                    'labeling_method': 'triple_barrier',
                    'cv_folds': 5,
                    'hyperparams': {
                        'num_leaves': 31,
                        'learning_rate': 0.05,
                        'n_estimators': 100
                    },
                    'regime_specific': True,
                    'regime_col': 'hmm_regime'
                }
            },
            
            # Position sizing config
            'position_sizing': {
                'model': 'dqn',
                'features': ['volatility_5', 'volatility_20', 'volatility', 'rsi', 'macd', 'hmm_regime', 'transformer_regime'],
                'base_rl': {
                    'state_dim': 8,
                    'action_dim': 5,
                    'learning_rate': 0.001,
                    'gamma': 0.99,
                    'epsilon': 0.1
                },
                'dqn': {
                    'state_dim': 10,
                    'action_dim': 9,
                    'hidden_dims': [128, 64],
                    'learning_rate': 0.0005,
                    'gamma': 0.99,
                    'epsilon_start': 1.0,
                    'epsilon_end': 0.01,
                    'epsilon_decay': 0.995,
                    'buffer_size': 10000
                },
                'ppo': {
                    'state_dim': 10,
                    'action_dim': 1,
                    'hidden_dims': [128, 64],
                    'actor_lr': 0.0003,
                    'critic_lr': 0.001
                },
                'sac': {
                    'state_dim': 10,
                    'action_dim': 1,
                    'hidden_dims': [256, 128],
                    'actor_lr': 0.0003,
                    'critic_lr': 0.0003
                },
                'dreamer': {
                    'state_dim': 10,
                    'action_dim': 1,
                    'deter_dim': 200,
                    'stoch_dim': 30,
                    'hidden_dim': 200
                }
            }
        }
    
    def load_and_preprocess_data(self, filepath: str = None, **kwargs) -> pd.DataFrame:
        """
        Load and preprocess data.
        
        Args:
            filepath (str): Path to data file
            **kwargs: Additional arguments for data preprocessing
            
        Returns:
            pandas.DataFrame: Preprocessed data
        """
        logger.info("Loading and preprocessing data")
        
        # Load data
        filepath = filepath or self.config['data']['filepath']
        self.data = load_data(filepath)
        
        # Preprocess data
        fillna_method = kwargs.get('fillna_method', 'ffill')
        dropna = kwargs.get('dropna', True)
        self.data = preprocess_data(self.data, fillna_method=fillna_method, dropna=dropna)
        
        # Add returns
        price_col = self.config['data']['price_col']
        periods = [1, 5, 10, 20]
        self.data = add_returns(self.data, price_col=price_col, periods=periods)
        
        # Add technical indicators
        indicators = ['rsi', 'macd', 'bbands']
        self.data = add_technical_indicators(self.data, indicators=indicators)
        
        # Add volatility features
        windows = [21, 63, 126]
        self.data = add_volatility_features(self.data, returns_col='returns', windows=windows)
        
        # Log data columns
        logger.info(f"Data columns after preprocessing: {self.data.columns.tolist()}")
        
        # Split data manually
        test_size = kwargs.get('test_size', 1 - self.config['data']['train_test_split'])
        random_state = kwargs.get('random_state', self.config['data']['random_state'])
        
        # Determine split point
        split_idx = int(len(self.data) * (1 - test_size))
        
        # Split data chronologically
        self.train_data = self.data.iloc[:split_idx].copy()
        self.test_data = self.data.iloc[split_idx:].copy()
        
        logger.info(f"Data loaded and preprocessed: {len(self.data)} samples")
        logger.info(f"Train data: {len(self.train_data)} samples, Test data: {len(self.test_data)} samples")
        
        return self.data
    
    def setup_regime_detectors(self) -> Dict:
        """
        Set up market regime detectors.
        
        Returns:
            dict: Dictionary of regime detectors
        """
        logger.info("Setting up market regime detectors")
        
        methods = self.config['regime_detection']['methods']
        
        if 'hmm' in methods:
            logger.info("Setting up HMM regime detector")
            hmm_config = self.config['regime_detection']['hmm']
            self.regime_detectors['hmm'] = HMMRegimeDetector(
                n_regimes=hmm_config['n_regimes'],
                features=hmm_config['features'],
                cov_type=hmm_config['cov_type']
            )
        
        if 'transformer' in methods:
            logger.info("Setting up Transformer regime detector")
            transformer_config = self.config['regime_detection']['transformer']
            self.regime_detectors['transformer'] = TransformerRegimeDetector(
                n_regimes=transformer_config['n_regimes'],
                features=transformer_config['features'],
                embedding_dim=transformer_config['embedding_dim'],
                num_heads=transformer_config['num_heads'],
                num_layers=transformer_config['num_layers']
            )
        
        logger.info(f"Set up {len(self.regime_detectors)} regime detectors: {list(self.regime_detectors.keys())}")
        return self.regime_detectors
    
    def train_regime_detectors(self, data: pd.DataFrame = None) -> Dict:
        """
        Train the market regime detectors.
        
        Parameters:
        -----------
        data : pd.DataFrame, optional
            Data to train on. If None, will use self.train_data.
            
        Returns:
        --------
        Dict
            Dictionary of trained regime detectors.
        """
        logger.info("Training market regime detectors")
        
        # Use train_data if no data provided
        if data is None:
            data = self.train_data
        
        trained_detectors = {}
        
        for name, detector in self.regime_detectors.items():
            logger.info(f"Training {name} regime detector")
            detector.train(data)
            trained_detectors[name] = detector
        
        logger.info("Market regime detectors trained")
        return trained_detectors
    
    def detect_regimes(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """
        Detect market regimes in the data.
        
        Parameters:
        -----------
        data : pd.DataFrame, optional
            Data to detect regimes in. If None, will use self.data.
            
        Returns:
        --------
        pd.DataFrame
            Data with regime columns added.
        """
        logger.info("Detecting market regimes")
        
        # Use data if no data provided
        if data is None:
            data = self.data
        
        # Make a copy to avoid modifying the original
        data_with_regimes = data.copy()
        
        for name, detector in self.regime_detectors.items():
            logger.info(f"Detecting regimes with {name} detector")
            data_with_regimes = detector.detect_regimes(data_with_regimes)
        
        logger.info("Market regimes detected")
        return data_with_regimes
    
    def setup_meta_labelers(self) -> Dict:
        """
        Set up meta-labelers.
        
        Returns:
            dict: Dictionary of meta-labelers
        """
        logger.info("Setting up meta-labelers")
        
        models = self.config['meta_labeling']['models']
        
        for model_type in models:
            logger.info(f"Setting up {model_type} meta-labeler")
            model_config = self.config['meta_labeling'][model_type]
            self.meta_labelers[model_type] = MetaLabeling(
                features=model_config['features'],
                labeling_method=model_config.get('labeling_method', 'triple_barrier'),
                model_method=model_type,
                target='label',
                **model_config.get('params', {})
            )
        
        logger.info(f"Set up {len(self.meta_labelers)} meta-labelers: {list(self.meta_labelers.keys())}")
        return self.meta_labelers
    
    def train_meta_labelers(self, data: pd.DataFrame = None) -> Dict:
        """
        Train meta-labelers on the given data.
        
        Parameters:
        -----------
        data : pd.DataFrame, optional
            Data to train on. If None, will use self.train_data.
            
        Returns:
        --------
        Dict
            Dictionary of trained meta-labelers.
        """
        logger.info("Training meta-labelers")
        
        try:
            # Use train_data if no data provided
            if data is None:
                data = self.train_data
                
            # Log data information
            logger.info(f"Data shape: {data.shape}")
            logger.info(f"Data columns: {data.columns.tolist()}")
            
            # Get signal column
            signal_col = self.config['data']['signal_col']
            
            # Check if signal column exists
            if signal_col not in data.columns:
                logger.warning(f"Signal column '{signal_col}' not found in data. Available columns: {data.columns.tolist()}")
                # Create a default signal column with zeros
                data[signal_col] = 0
                logger.warning(f"Created default {signal_col} column with zeros")
            
            # Check if regime-specific meta-labeling is enabled
            regime_specific = self.config['meta_labeling'].get('regime_specific', True)
            has_regime_detectors = len(self.regime_detectors) > 0
            
            logger.info(f"Checking regime specific meta-labeling configuration")
            logger.info(f"Regime specific: {regime_specific}, Has regime detectors: {has_regime_detectors}")
            
            if regime_specific and has_regime_detectors:
                logger.info("Training regime-specific meta-labelers")
                
                # Choose a regime column
                if 'hmm' in self.regime_detectors:
                    regime_column = 'hmm_regime'
                elif 'transformer' in self.regime_detectors:
                    regime_column = 'transformer_regime'
                else:
                    raise ValueError("No regime detector available for regime-specific meta-labeling")
                
                logger.info(f"Using regime column: {regime_column}")
                
                # Extract features and model settings from config
                model_type = self.config['meta_labeling']['models'][0]
                model_config = self.config['meta_labeling'][model_type]
                features = ['returns', 'returns_5', 'returns_10', 'returns_20', 'volatility_21', 'volatility_63', 'volatility_126', 'hmm_regime', 'transformer_regime']
                
                logger.info(f"Regime-specific meta-labeling with model: {model_type}, features: {features}")
                
                # Create and train regime-specific meta-labeler
                self.meta_labelers['regime_specific'] = RegimeMetaLabeling(
                    features=features,
                    labeling_method=model_config.get('labeling_method', 'triple_barrier'),
                    model_method=model_type,
                    target='label'
                )
                
                # Train the regime meta-labeling
                performance = self.meta_labelers['regime_specific'].fit(
                    data=data,
                    regime_column=regime_column,
                    signal_col=signal_col,
                    test_size=self.config['data']['train_test_split'],
                    random_state=self.config['data']['random_state']
                )
                
                # Store metadata
                self.meta_labelers['regime_specific_metadata'] = {
                    'performance': performance,
                    'regime_column': regime_column
                }
                
                logger.info(f"Regime-specific meta-labeler trained. Performance: {performance}")
            else:
                # Train individual meta-labelers
                for model_type, meta_labeler in self.meta_labelers.items():
                    logger.info(f"Training {model_type} meta-labeler")
                    
                    # Get model-specific parameters
                    model_config = self.config['meta_labeling'][model_type]
                    
                    # Create labels
                    logger.info(f"Creating labels for {model_type} meta-labeler")
                    labeled_data = meta_labeler.create_labels(
                        data=data,
                        signal_col=signal_col,
                        returns_col='returns',
                        price_col='gbclose',
                        upper_barrier=model_config.get('upper_barrier', 0.02),
                        lower_barrier=model_config.get('lower_barrier', -0.01),
                        max_holding_period=model_config.get('max_holding_period', 10)
                    )
                    
                    # Log labeled data information
                    logger.info(f"Labeled data shape: {labeled_data.shape}")
                    logger.info(f"Labeled data columns: {labeled_data.columns.tolist()}")
                    
                    # Check if label column exists
                    target = 'label'
                    if target not in labeled_data.columns:
                        logger.error(f"{target} column not found in labeled data. Available columns: {labeled_data.columns.tolist()}")
                        raise ValueError(f"{target} column not found in labeled data")
                    
                    # Train meta-labeler
                    logger.info(f"Training {model_type} meta-labeler with labeled data")
                    metrics = meta_labeler.fit(labeled_data)
                    logger.info(f"Meta-labeler {model_type} trained with metrics: {metrics}")
            
            logger.info("Meta-labelers trained successfully")
            return self.meta_labelers
        except Exception as e:
            logger.error(f"Error in train_meta_labelers: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def apply_meta_labeling(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """
        Apply meta-labeling to the data, potentially for different regimes.
        
        Parameters:
        -----------
        data : pd.DataFrame, optional
            Data to apply meta-labeling to. If None, will use self.data.
            
        Returns:
        --------
        pd.DataFrame
            Data with meta-labels added.
        """
        logger.info("Applying meta-labeling")
        
        try:
            # Use self.data if no data provided
            if data is None:
                data = self.data
                
            # Make a copy to avoid modifying the original
            data_with_metalabels = data.copy()
            
            # Log available columns in data
            logger.info(f"Data columns available for meta-labeling application: {data.columns.tolist()}")
            
            # Check if regime-specific meta-labeling is enabled
            if 'regime_specific' in self.meta_labelers:
                logger.info("Applying regime-specific meta-labeling")
                
                regime_specific_metadata = self.meta_labelers.get('regime_specific_metadata', {})
                regime_meta_labeler = self.meta_labelers['regime_specific']
                regime_column = regime_specific_metadata.get('regime_column', 'hmm_regime')
                
                # Check if regime column exists
                if regime_column not in data.columns:
                    logger.warning(f"Regime column '{regime_column}' not found in data. Cannot apply regime-specific meta-labeling.")
                    # Fall back to first available meta-labeler
                    model_type = next(iter(self.meta_labelers))
                    if model_type not in ['regime_specific', 'regime_specific_metadata']:
                        meta_labeler = self.meta_labelers[model_type]
                        logger.info(f"Falling back to {model_type} meta-labeler")
                        data_with_metalabels = meta_labeler.predict(data)
                    else:
                        logger.error("No fallback meta-labeler available")
                        # Create default meta columns
                        data_with_metalabels = data.copy()
                        data_with_metalabels['label_prob'] = 0.5
                        data_with_metalabels['label_pred'] = 0
                        data_with_metalabels['meta_position'] = 0
                else:
                    # Apply regime-specific meta-labeling
                    try:
                        signal_col = self.config['data']['signal_col']
                        data_with_metalabels = regime_meta_labeler.predict(
                            data=data,
                            regime_column=regime_column,
                            signal_col=signal_col,
                            threshold=self.config['meta_labeling'].get('threshold', 0.5)
                        )
                        logger.info("Regime-specific meta-labeling applied successfully")
                    except Exception as e:
                        logger.error(f"Error applying regime-specific meta-labeling: {e}")
                        # Create default meta columns
                        data_with_metalabels = data.copy()
                        data_with_metalabels['label_prob'] = 0.5
                        data_with_metalabels['label_pred'] = 0
                        data_with_metalabels['meta_position'] = 0
            else:
                # Apply first available meta-labeler
                model_types = [k for k in self.meta_labelers.keys() 
                              if k not in ['regime_specific', 'regime_specific_metadata']]
                if model_types:
                    model_type = model_types[0]
                    meta_labeler = self.meta_labelers[model_type]
                    
                    logger.info(f"Applying {model_type} meta-labeler")
                    try:
                        signal_col = self.config['data']['signal_col']
                        data_with_metalabels = meta_labeler.predict(
                            data=data,
                            signal_col=signal_col,
                            threshold=self.config['meta_labeling'].get('threshold', 0.5)
                        )
                    except Exception as e:
                        logger.error(f"Error applying {model_type} meta-labeler: {e}")
                        # Create default meta columns
                        data_with_metalabels = data.copy()
                        data_with_metalabels['label_prob'] = 0.5
                        data_with_metalabels['label_pred'] = 0
                        data_with_metalabels['meta_position'] = 0
                else:
                    logger.warning("No meta-labelers available")
                    # Create default meta columns
                    data_with_metalabels = data.copy()
                    data_with_metalabels['label_prob'] = 0.5
                    data_with_metalabels['label_pred'] = 0
                    data_with_metalabels['meta_position'] = 0
            
            # Calculate meta-strategy returns if returns column exists
            if 'returns' in data_with_metalabels.columns and 'meta_position' in data_with_metalabels.columns:
                logger.info("Calculating meta-strategy returns")
                data_with_metalabels['meta_strategy_returns'] = data_with_metalabels['meta_position'].shift(1) * data_with_metalabels['returns']
                data_with_metalabels['meta_strategy_returns'] = data_with_metalabels['meta_strategy_returns'].fillna(0)
            
            logger.info("Meta-labeling applied")
            return data_with_metalabels
        
        except Exception as e:
            logger.error(f"Error in apply_meta_labeling: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Return original data with empty position column to avoid errors downstream
            logger.warning("Error occurred, returning original data with default position columns")
            data_with_metalabels = data.copy()
            data_with_metalabels['label_prob'] = 0.5
            data_with_metalabels['label_pred'] = 0
            data_with_metalabels['meta_position'] = 0
            
            return data_with_metalabels
    
    def setup_position_sizer(self) -> object:
        """
        Set up position sizer.
        
        Returns:
            object: Position sizer
        """
        logger.info("Setting up position sizer")
        
        model = self.config['position_sizing']['model']
        
        if model == 'base_rl':
            logger.info("Setting up base RL position sizer")
            config = self.config['position_sizing']['base_rl']
            self.position_sizers['base_rl'] = RLPositionSizer(
                state_dim=config['state_dim'],
                action_dim=config['action_dim'],
                learning_rate=config['learning_rate'],
                gamma=config['gamma'],
                epsilon=config['epsilon']
            )
        
        elif model == 'dqn':
            logger.info("Setting up DQN position sizer")
            config = self.config['position_sizing']['dqn']
            self.position_sizers['dqn'] = DQNPositionSizer(
                state_dim=config['state_dim'],
                action_dim=config['action_dim'],
                hidden_dims=config['hidden_dims'],
                learning_rate=config['learning_rate'],
                gamma=config['gamma'],
                epsilon_start=config['epsilon_start'],
                epsilon_end=config['epsilon_end'],
                epsilon_decay=config['epsilon_decay'],
                buffer_size=config['buffer_size']
            )
        
        elif model == 'ppo':
            logger.info("Setting up PPO position sizer")
            config = self.config['position_sizing']['ppo']
            self.position_sizers['ppo'] = PPOPositionSizer(
                state_dim=config['state_dim'],
                action_dim=config['action_dim'],
                hidden_dims=config['hidden_dims'],
                actor_lr=config['actor_lr'],
                critic_lr=config['critic_lr']
            )
        
        elif model == 'sac':
            logger.info("Setting up SAC position sizer")
            config = self.config['position_sizing']['sac']
            self.position_sizers['sac'] = SACPositionSizer(
                state_dim=config['state_dim'],
                action_dim=config['action_dim'],
                hidden_dims=config['hidden_dims'],
                actor_lr=config['actor_lr'],
                critic_lr=config['critic_lr']
            )
        
        elif model == 'dreamer':
            logger.info("Setting up Dreamer position sizer")
            config = self.config['position_sizing']['dreamer']
            self.position_sizers['dreamer'] = DreamerPositionSizer(
                state_dim=config['state_dim'],
                action_dim=config['action_dim'],
                deter_dim=config['deter_dim'],
                stoch_dim=config['stoch_dim'],
                hidden_dim=config['hidden_dim']
            )
        
        else:
            raise ValueError(f"Unsupported position sizing model: {model}")
        
        logger.info(f"Position sizer set up: {model}")
        return self.position_sizers[model]
    
    def train_position_sizer(self, data: pd.DataFrame = None) -> object:
        """
        Train the position sizer model on the given data.
        
        Parameters:
        -----------
        data : pd.DataFrame, optional
            Data to train on. If None, will use self.train_data.
            
        Returns:
        --------
        object
            Trained position sizer model.
        """
        logger.info(f"Training position sizer: {self.config['position_sizing']['model']}")
        
        try:
            # Use train_data if no data provided
            if data is None:
                data = self.train_data
                
            # Extract relevant columns
            features = self.config['position_sizing']['features']
            
            # Log data information
            logger.info(f"Data shape: {data.shape}")
            logger.info(f"Data columns: {data.columns.tolist()}")
            
            model = self.config['position_sizing']['model']
            
            if model not in self.position_sizers:
                logger.error(f"Position sizer model '{model}' not found in available position sizers: {list(self.position_sizers.keys())}")
                raise ValueError(f"Position sizer model '{model}' not found")
            
            position_sizer = self.position_sizers[model]
            
            # Ensure data has meta-labels
            required_columns = ['meta_label', 'meta_confidence']
            missing_columns = [col for col in required_columns if col not in data.columns]
            
            if missing_columns:
                logger.info(f"Data missing meta-labels: {missing_columns}, applying meta-labeling first")
                data = self.apply_meta_labeling(data)
                
                # Verify that apply_meta_labeling added the required columns
                still_missing = [col for col in required_columns if col not in data.columns]
                if still_missing:
                    logger.error(f"Meta-labeling failed to add required columns: {still_missing}")
                    raise ValueError(f"Meta-labeling failed to add required columns: {still_missing}")
            
            # Train position sizer
            logger.info(f"Training {model} position sizer with data shape: {data.shape}")
            position_sizer.train(data)
            
            logger.info(f"Position sizer {model} trained successfully")
            return position_sizer
        
        except Exception as e:
            logger.error(f"Error in train_position_sizer: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def apply_position_sizing(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """
        Apply position sizing to the data.
        
        Parameters:
        -----------
        data : pd.DataFrame, optional
            Data to apply position sizing to. If None, will use self.data.
            
        Returns:
        --------
        pd.DataFrame
            Data with position sizing applied.
        """
        logger.info("Applying position sizing")
        
        try:
            # Use self.data if no data provided
            if data is None:
                data = self.data
                
            # Make a copy to avoid modifying the original
            data_with_positions = data.copy()
            
            # Log data information
            logger.info(f"Data shape: {data.shape}")
            logger.info(f"Data columns: {data.columns.tolist()}")
            
            model = self.config['position_sizing']['model']
            
            if model not in self.position_sizers:
                logger.error(f"Position sizer model '{model}' not found in available position sizers: {list(self.position_sizers.keys())}")
                raise ValueError(f"Position sizer model '{model}' not found")
            
            position_sizer = self.position_sizers[model]
            
            # Ensure data has meta-labels
            required_columns = ['meta_label', 'meta_confidence']
            missing_columns = [col for col in required_columns if col not in data.columns]
            
            if missing_columns:
                logger.info(f"Data missing meta-labels: {missing_columns}, applying meta-labeling first")
                data = self.apply_meta_labeling(data)
                
                # Verify that apply_meta_labeling added the required columns
                still_missing = [col for col in required_columns if col not in data.columns]
                if still_missing:
                    logger.error(f"Meta-labeling failed to add required columns: {still_missing}")
                    raise ValueError(f"Meta-labeling failed to add required columns: {still_missing}")
            
            # Apply position sizer
            logger.info(f"Applying {model} position sizer to data")
            positions = position_sizer.predict(data)
            
            # Add positions to data
            data_with_positions['rl_position'] = positions
            
            # Ensure returns column exists
            if 'returns' not in data_with_positions.columns:
                logger.warning("Returns column not found, cannot calculate RL strategy returns")
                data_with_positions['returns'] = 0.0
                data_with_positions['rl_strategy_returns'] = 0.0
            else:
                # Calculate returns
                logger.info("Calculating RL strategy returns")
                data_with_positions['rl_strategy_returns'] = data_with_positions['rl_position'].shift(1) * data_with_positions['returns']
                data_with_positions['rl_strategy_returns'] = data_with_positions['rl_strategy_returns'].fillna(0)
                
                # Calculate base strategy returns if it doesn't exist
                if 'strategy_returns' not in data_with_positions.columns and 'position' in data_with_positions.columns:
                    logger.info("Calculating base strategy returns")
                    data_with_positions['strategy_returns'] = data_with_positions['position'].shift(1) * data_with_positions['returns']
                    data_with_positions['strategy_returns'] = data_with_positions['strategy_returns'].fillna(0)
            
            logger.info("Position sizing applied successfully")
            return data_with_positions
        
        except Exception as e:
            logger.error(f"Error in apply_position_sizing: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Return original data with empty position column to avoid errors downstream
            logger.warning("Error occurred, returning original data with default position columns")
            data_with_positions = data.copy()
            data_with_positions['rl_position'] = 0
            data_with_positions['rl_strategy_returns'] = 0.0
            
            return data_with_positions
    
    def run_pipeline(self) -> pd.DataFrame:
        """
        Run the complete pipeline:
        1. Load and preprocess data
        2. Set up and train regime detectors
        3. Detect regimes
        4. Set up and train meta-labelers
        5. Apply meta-labeling
        6. Set up and train position sizer
        7. Apply position sizing
        
        Returns:
            pandas.DataFrame: Final data with all components applied
        """
        logger.info("Running complete pipeline")
        
        # Step 1: Load and preprocess data
        self.load_and_preprocess_data()
        
        # Step 2: Set up and train regime detectors
        self.setup_regime_detectors()
        self.train_regime_detectors()
        
        # Step 3: Detect regimes
        data_with_regimes = self.detect_regimes()
        
        # Step 4: Set up and train meta-labelers
        self.setup_meta_labelers()
        self.train_meta_labelers(data_with_regimes)
        
        # Step 5: Apply meta-labeling
        data_with_meta = self.apply_meta_labeling(data_with_regimes)
        
        # Step 6: Set up and train position sizer
        self.setup_position_sizer()
        self.train_position_sizer(data_with_meta)
        
        # Step 7: Apply position sizing
        final_data = self.apply_position_sizing(data_with_meta)
        
        logger.info("Pipeline completed successfully")
        return final_data
    
    def evaluate_on_test_data(self) -> Dict:
        """
        Evaluate the complete pipeline on test data.
        
        Returns:
            dict: Evaluation results
        """
        logger.info("Evaluating on test data")
        
        # Apply pipeline to test data
        test_data_with_regimes = self.detect_regimes(self.test_data)
        test_data_with_meta = self.apply_meta_labeling(test_data_with_regimes)
        test_data_with_positions = self.apply_position_sizing(test_data_with_meta)
        
        # Calculate performance metrics
        from dl_metalabeling.metalabeling import calculate_performance_metrics
        
        base_metrics = calculate_performance_metrics(test_data_with_positions['strategy_returns'])
        meta_metrics = calculate_performance_metrics(test_data_with_positions['meta_strategy_returns'])
        rl_metrics = calculate_performance_metrics(test_data_with_positions['rl_strategy_returns'])
        
        # Store results
        results = {
            'base': base_metrics,
            'meta': meta_metrics,
            'rl': rl_metrics
        }
        
        # Log results
        logger.info("Evaluation results:")
        logger.info(f"Base Strategy - Sharpe: {base_metrics['sharpe_ratio']:.2f}, Return: {base_metrics['annual_return']:.2%}")
        logger.info(f"Meta Strategy - Sharpe: {meta_metrics['sharpe_ratio']:.2f}, Return: {meta_metrics['annual_return']:.2%}")
        logger.info(f"RL Position Sizing - Sharpe: {rl_metrics['sharpe_ratio']:.2f}, Return: {rl_metrics['annual_return']:.2%}")
        
        return results
    
    def save_models(self, base_dir: str = 'models') -> None:
        """
        Save all models.
        
        Args:
            base_dir (str): Base directory to save models
        """
        logger.info(f"Saving models to {base_dir}")
        
        os.makedirs(base_dir, exist_ok=True)
        
        # Save regime detectors
        for name, detector in self.regime_detectors.items():
            detector.save(os.path.join(base_dir, f"{name}_regime_detector.pkl"))
        
        # Save meta-labelers
        if 'regime_specific' in self.meta_labelers:
            metalabelers = self.meta_labelers['regime_specific']['metalabelers']
            for regime, meta_labeler in metalabelers.items():
                meta_labeler.save(os.path.join(base_dir, f"regime_{regime}_meta_labeler.pkl"))
        else:
            for model_type, meta_labeler in self.meta_labelers.items():
                meta_labeler.save(os.path.join(base_dir, f"{model_type}_meta_labeler.pkl"))
        
        # Save position sizers
        for model, position_sizer in self.position_sizers.items():
            position_sizer.save(os.path.join(base_dir, f"{model}_position_sizer.pkl"))
        
        logger.info("Models saved successfully")
    
    def load_models(self, base_dir: str = 'models') -> None:
        """
        Load all models.
        
        Args:
            base_dir (str): Base directory to load models from
        """
        logger.info(f"Loading models from {base_dir}")
        
        # Load regime detectors
        for method in self.config['regime_detection']['methods']:
            filepath = os.path.join(base_dir, f"{method}_regime_detector.pkl")
            if os.path.exists(filepath):
                if method == 'hmm':
                    self.regime_detectors[method] = HMMRegimeDetector.load(filepath)
                elif method == 'transformer':
                    self.regime_detectors[method] = TransformerRegimeDetector.load(filepath)
        
        # Load position sizer
        model = self.config['position_sizing']['model']
        filepath = os.path.join(base_dir, f"{model}_position_sizer.pkl")
        if os.path.exists(filepath):
            if model == 'base_rl':
                self.position_sizers[model] = RLPositionSizer.load(filepath)
            elif model == 'dqn':
                self.position_sizers[model] = DQNPositionSizer.load(filepath)
            elif model == 'ppo':
                self.position_sizers[model] = PPOPositionSizer.load(filepath)
            elif model == 'sac':
                self.position_sizers[model] = SACPositionSizer.load(filepath)
            elif model == 'dreamer':
                self.position_sizers[model] = DreamerPositionSizer.load(filepath)
        
        logger.info("Models loaded successfully")


# Usage example
if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Initialize the framework
    framework = DLMetaLabelingFramework()
    
    # Run the complete pipeline
    results = framework.run_pipeline()
    
    # Evaluate on test data
    evaluation = framework.evaluate_on_test_data()
    
    # Save models
    framework.save_models()

================
File: regime_evaluator.py
================
"""
Regime evaluator module that uses pre-trained models to evaluate market regimes.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import os
import warnings

# Import existing model implementations
from dl_metalabeling.models.hmm import load_hmm_model, HiddenMarketModel
from dl_metalabeling.models.transformer import load_transformer_model, TransformerMarketModel

# Suppress specific warnings
warnings.filterwarnings("ignore", category=pd.errors.SettingWithCopyWarning)


class RegimeEvaluator:
    """
    A class to evaluate and compare different market regime detection models.
    Uses pre-trained models rather than training them.
    """
    
    def __init__(self, data_path=os.path.join('data', 'cmma.csv'), models_dir='models'):
        """
        Initialize the evaluator with data path and models directory.
        
        Parameters:
        data_path (str): Path to the market data CSV file
        models_dir (str): Directory containing pre-trained models
        """
        self.data_path = data_path
        self.models_dir = models_dir
        
        # Model-specific properties
        self.hmm_model = None
        self.transformer_model = None
        
        # Data properties
        self.df = pd.read_csv(data_path, parse_dates=['DateTime'], index_col='DateTime')
        print(f"Data shape: {self.df.shape}")
        print(f"Date range: {self.df.index.min()} to {self.df.index.max()}")
        
        # Prepare data
        self._prepare_data()
        
        # Try to load models
        self._load_models()
    
    def _prepare_data(self):
        """Prepare the data for both models."""
        # Calculate returns
        self.df['returns'] = self.df['gbclose'].pct_change()
        self.df['log_returns'] = np.log(self.df['gbclose']).diff()
        
        # Calculate volatility features
        self.df['volatility_5d'] = self.df['returns'].rolling(window=5).std()
        self.df['volatility_20d'] = self.df['returns'].rolling(window=20).std()
        
        # Calculate momentum features  
        self.df['momentum_5d'] = self.df['returns'].rolling(window=5).mean()
        self.df['momentum_20d'] = self.df['returns'].rolling(window=20).mean()
        
        # Calculate mean reversion features
        self.df['ma_20d'] = self.df['gbclose'].rolling(window=20).mean()
        self.df['ma_50d'] = self.df['gbclose'].rolling(window=50).mean()
        self.df['deviation_20d'] = (self.df['gbclose'] - self.df['ma_20d']) / self.df['ma_20d']
        self.df['deviation_50d'] = (self.df['gbclose'] - self.df['ma_50d']) / self.df['ma_50d']
        
        # Clean the data by removing NaN values
        self.df = self.df.dropna().copy()
    
    def _load_models(self):
        """Attempt to load pre-trained models from disk."""
        # Load HMM model
        self.hmm_model = load_hmm_model(self.models_dir)
        if self.hmm_model is not None:
            print("HMM model loaded successfully.")
        else:
            print("HMM model not found. Please train and save a model first.")
        
        # Load Transformer model
        self.transformer_model = load_transformer_model(self.models_dir)
        if self.transformer_model is not None:
            print("Transformer model loaded successfully.")
        else:
            print("Transformer model not found. Please train and save a model first.")
    
    def evaluate(self):
        """Evaluate and compare both models."""
        print("\n===== Model Evaluation =====")
        
        # Check if models are available
        if self.hmm_model is None:
            print("HMM model not available. Please obtain a trained model first.")
            return
            
        if self.transformer_model is None:
            print("Transformer model not available. Please obtain a trained model first.")
            return
        
        # Get predictions from both models
        self.df['hmm_regime'] = self.hmm_model.predict(self.df)
        self.df['hmm_regime_label'] = [self.hmm_model.regime_labels.get(r, "Unknown") for r in self.df['hmm_regime']]
        
        self.df['transformer_regime'] = self.transformer_model.predict(self.df)
        self.df['transformer_regime_label'] = self.transformer_model.get_regime_labels(self.df['transformer_regime'])
        
        # Create a comparison dataframe with both model predictions
        comparison_df = self.df.dropna(subset=['hmm_regime', 'transformer_regime'])
        
        # Analyze regime transitions
        self._analyze_transitions(comparison_df)
        
        # Calculate performance by regime
        self._analyze_performance(comparison_df)
        
        # Measure agreement between models
        self._analyze_agreement(comparison_df)
        
        # Plot regime comparison
        self.plot_comparison()
        
        # Make recommendations
        self._make_recommendation(comparison_df)
        
        return comparison_df
    
    def _analyze_transitions(self, df):
        """
        Analyze regime transition frequency and duration.
        
        Parameters:
        df (pd.DataFrame): Dataframe with regime predictions
        """
        print("\n1. Regime Transition Analysis:")
        
        # Count regime transitions
        hmm_transitions = (df['hmm_regime'].diff() != 0).sum()
        transformer_transitions = (df['transformer_regime'].diff() != 0).sum()
        
        print(f"  HMM transitions: {hmm_transitions}")
        print(f"  Transformer transitions: {transformer_transitions}")
        
        # Calculate regime durations
        hmm_durations = []
        transformer_durations = []
        
        # Helper function to calculate durations
        def get_durations(regime_series):
            durations = []
            current_regime = regime_series.iloc[0]
            current_duration = 1
            
            for regime in regime_series.iloc[1:]:
                if regime == current_regime:
                    current_duration += 1
                else:
                    durations.append(current_duration)
                    current_regime = regime
                    current_duration = 1
                    
            # Add the last duration
            durations.append(current_duration)
            return durations
        
        # Calculate durations for both models
        hmm_durations = get_durations(df['hmm_regime'])
        transformer_durations = get_durations(df['transformer_regime'])
        
        # Print duration statistics
        print("\n2. Regime Duration Statistics:")
        print(f"  HMM average duration: {np.mean(hmm_durations):.2f} days")
        print(f"  HMM median duration: {np.median(hmm_durations):.2f} days")
        print(f"  HMM max duration: {np.max(hmm_durations)} days")
        
        print(f"  Transformer average duration: {np.mean(transformer_durations):.2f} days")
        print(f"  Transformer median duration: {np.median(transformer_durations):.2f} days")
        print(f"  Transformer max duration: {np.max(transformer_durations)} days")
    
    def _analyze_performance(self, df):
        """
        Analyze performance metrics by regime for each model.
        
        Parameters:
        df (pd.DataFrame): Dataframe with regime predictions
        """
        print("\n3. Performance Metrics by Regime:")
        
        # HMM performance
        print("\nHMM Model:")
        for regime_id, label in self.hmm_model.regime_labels.items():
            mask = df['hmm_regime'] == regime_id
            regime_returns = df.loc[mask, 'returns']
            
            print(f"  {label} Regime (Regime {regime_id}):")
            print(f"    Days: {mask.sum()}")
            print(f"    Mean return: {regime_returns.mean() * 100:.4f}%")
            print(f"    Std deviation: {regime_returns.std() * 100:.4f}%")
            print(f"    Sharpe ratio: {(regime_returns.mean() / (regime_returns.std() + 1e-8)) * np.sqrt(252):.4f}")
            print(f"    Win rate: {(regime_returns > 0).mean() * 100:.2f}%")
        
        # Transformer performance
        print("\nTransformer Model:")
        for regime_id, label in self.transformer_model.regime_labels.items():
            mask = df['transformer_regime'] == regime_id
            regime_returns = df.loc[mask, 'returns']
            
            print(f"  {label} Regime (Regime {regime_id}):")
            print(f"    Days: {mask.sum()}")
            print(f"    Mean return: {regime_returns.mean() * 100:.4f}%")
            print(f"    Std deviation: {regime_returns.std() * 100:.4f}%")
            print(f"    Sharpe ratio: {(regime_returns.mean() / (regime_returns.std() + 1e-8)) * np.sqrt(252):.4f}")
            print(f"    Win rate: {(regime_returns > 0).mean() * 100:.2f}%")
    
    def _analyze_agreement(self, df):
        """
        Analyze agreement between models.
        
        Parameters:
        df (pd.DataFrame): Dataframe with regime predictions
        """
        print("\n4. Regime Agreement Analysis:")
        
        # Map regime IDs to common labels for comparison
        df['hmm_label'] = df['hmm_regime'].map(self.hmm_model.regime_labels)
        df['transformer_label'] = df['transformer_regime'].map(self.transformer_model.regime_labels)
        
        # Calculate overall agreement
        agreement = (df['hmm_label'] == df['transformer_label']).mean() * 100
        print(f"  Overall agreement between models: {agreement:.2f}%")
        
        # Calculate Jaccard similarity for each regime type
        for regime_type in ['Volatile', 'Trending', 'Mean-Reverting']:
            hmm_mask = df['hmm_label'] == regime_type
            transformer_mask = df['transformer_label'] == regime_type
            
            # Calculate intersection and union
            intersection = (hmm_mask & transformer_mask).sum()
            union = (hmm_mask | transformer_mask).sum()
            jaccard = intersection / union if union > 0 else 0
            
            print(f"  {regime_type} regime Jaccard similarity: {jaccard:.4f}")
    
    def _make_recommendation(self, df):
        """
        Make recommendation on which model or combination to use.
        
        Parameters:
        df (pd.DataFrame): Dataframe with regime predictions
        """
        print("\n5. Model Recommendation:")
        
        # Calculate metrics for comparison
        hmm_transitions = (df['hmm_regime'].diff() != 0).sum()
        transformer_transitions = (df['transformer_regime'].diff() != 0).sum()
        
        # Calculate Sharpe ratios by regime
        hmm_sharpe_ratios = []
        transformer_sharpe_ratios = []
        
        # HMM Sharpe ratios
        for regime_id in self.hmm_model.regime_labels:
            mask = df['hmm_regime'] == regime_id
            regime_returns = df.loc[mask, 'returns']
            if len(regime_returns) > 0 and regime_returns.std() > 0:
                hmm_sharpe_ratios.append((regime_returns.mean() / regime_returns.std()) * np.sqrt(252))
                
        # Transformer Sharpe ratios
        for regime_id in self.transformer_model.regime_labels:
            mask = df['transformer_regime'] == regime_id
            regime_returns = df.loc[mask, 'returns']
            if len(regime_returns) > 0 and regime_returns.std() > 0:
                transformer_sharpe_ratios.append((regime_returns.mean() / regime_returns.std()) * np.sqrt(252))
        
        # Define and print strengths of each model
        hmm_strengths = []
        transformer_strengths = []
        
        # Transition stability
        if hmm_transitions < transformer_transitions:
            hmm_strengths.append("More stable regimes with fewer transitions")
        else:
            transformer_strengths.append("More stable regimes with fewer transitions")
            
        # Sharpe ratios
        if np.mean(hmm_sharpe_ratios) > np.mean(transformer_sharpe_ratios):
            hmm_strengths.append("Better overall Sharpe ratios across regimes")
        else:
            transformer_strengths.append("Better overall Sharpe ratios across regimes")
            
        # Add inherent model strengths
        hmm_strengths.extend([
            "Simpler model with fewer parameters",
            "Works well with limited data",
            "Captures regime transitions well"
        ])
        
        transformer_strengths.extend([
            "Can capture more complex patterns",
            "Better at handling long-term dependencies",
            "Self-supervised learning captures market structure"
        ])
        
        # Print strengths
        print("\nHMM Strengths:")
        for strength in hmm_strengths:
            print(f"  - {strength}")
            
        print("\nTransformer Strengths:")
        for strength in transformer_strengths:
            print(f"  - {strength}")
            
        # Make recommendation
        print("\nRecommendation:")
        print("Based on the evaluation, I recommend:")
        print("  - Use a COMBINED APPROACH that leverages both models:")
        print("    1. Use HMM for stable regime identification (fewer transitions)")
        print("    2. Use Transformer for more nuanced pattern recognition within regimes")
        print("    3. When models agree, have higher confidence in the regime identification")
        print("    4. When models disagree, consider it a transitional period")
    
    def plot_comparison(self):
        """Plot regime comparison between models."""
        print("\nGenerating regime comparison plot...")
        
        # Make sure both models are available
        if self.hmm_model is None or self.transformer_model is None:
            print("Cannot plot comparison - both models must be available.")
            return
            
        # Use data where both models have predictions
        comparison_df = self.df.dropna(subset=['hmm_regime', 'transformer_regime'])
        
        # Create figure with multiple subplots
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15), sharex=True)
        
        # Plot price in the top subplot
        ax1.plot(comparison_df.index, comparison_df['gbclose'], color='black')
        ax1.set_title('Price and Regime Comparison', fontsize=16)
        ax1.set_ylabel('Price', fontsize=14)
        ax1.grid(True, alpha=0.3)
        
        # Define colors for regimes
        colors = {'Volatile': 'red', 'Trending': 'green', 'Mean-Reverting': 'blue', 'Unknown': 'gray'}
        
        # Plot HMM regimes in the middle subplot
        for regime_id, label in self.hmm_model.regime_labels.items():
            mask = comparison_df['hmm_regime'] == regime_id
            mask_indices = comparison_df.index[mask]
            
            # Create a color series for this regime
            color_series = pd.Series(1, index=mask_indices)
            
            # Plot as colored area
            ax2.fill_between(mask_indices, 0, color_series, alpha=0.7, color=colors[label], label=label)
            
        ax2.set_title('HMM Model Regimes', fontsize=14)
        ax2.set_ylabel('Regime', fontsize=12)
        ax2.legend(loc='upper right')
        ax2.grid(False)
        
        # Plot Transformer regimes in the bottom subplot
        for regime_id, label in self.transformer_model.regime_labels.items():
            mask = comparison_df['transformer_regime'] == regime_id
            mask_indices = comparison_df.index[mask]
            
            # Create a color series for this regime
            color_series = pd.Series(1, index=mask_indices)
            
            # Plot as colored area
            ax3.fill_between(mask_indices, 0, color_series, alpha=0.7, color=colors[label], label=label)
            
        ax3.set_title('Transformer Model Regimes', fontsize=14)
        ax3.set_xlabel('Date', fontsize=14)
        ax3.set_ylabel('Regime', fontsize=12)
        ax3.legend(loc='upper right')
        ax3.grid(False)
        
        # Format x-axis dates
        ax3.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        ax3.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig('regime_model_comparison.png', dpi=300)
        plt.close(fig)
        print("Plot saved as 'regime_model_comparison.png'")
    
    def get_current_regime(self, data=None, method='combined'):
        """
        Get the current market regime.
        
        Parameters:
        data (pd.DataFrame): Optional recent data to use
        method (str): Which model to use - 'hmm', 'transformer', or 'combined'
        
        Returns:
        dict: Information about the current regime
        """
        # Check if models are available
        if self.hmm_model is None or self.transformer_model is None:
            print("Cannot get regime - models are not available.")
            return None
        
        # Use most recent data from loaded dataset if no data provided
        if data is None:
            recent_data = self.df.iloc[-50:].copy()
        else:
            recent_data = data.copy()
        
        # Get predictions from both models
        result = {}
        
        # HMM regime
        if method in ['hmm', 'combined']:
            hmm_regimes = self.hmm_model.predict(recent_data)
            hmm_regime = hmm_regimes[-1]  # Most recent regime
            hmm_label = self.hmm_model.regime_labels.get(hmm_regime, "Unknown")
            
            result['hmm'] = {
                'regime_id': hmm_regime,
                'regime_label': hmm_label
            }
        
        # Transformer regime
        if method in ['transformer', 'combined']:
            transformer_regimes = self.transformer_model.predict(recent_data)
            transformer_regime = transformer_regimes[-1]  # Most recent regime
            transformer_label = self.transformer_model.regime_labels.get(transformer_regime, "Unknown")
            
            result['transformer'] = {
                'regime_id': transformer_regime,
                'regime_label': transformer_label
            }
        
        # Combined approach
        if method == 'combined' and 'hmm' in result and 'transformer' in result:
            if result['hmm']['regime_label'] == result['transformer']['regime_label']:
                # Models agree - high confidence
                result['combined'] = {
                    'regime_label': result['hmm']['regime_label'],
                    'confidence': 'high'
                }
            else:
                # Models disagree - consider it a transition
                result['combined'] = {
                    'regime_label': 'Transition',
                    'hmm_regime': result['hmm']['regime_label'],
                    'transformer_regime': result['transformer']['regime_label'],
                    'confidence': 'low'
                }
        
        return result
    
    def get_regime_history(self, method='combined'):
        """
        Get historical regime predictions.
        
        Parameters:
        method (str): Which model to use - 'hmm', 'transformer', or 'combined'
        
        Returns:
        pd.DataFrame: DataFrame with regime predictions
        """
        # Check if models are available
        if self.hmm_model is None or self.transformer_model is None:
            print("Cannot get regime history - models are not available.")
            return pd.DataFrame()
        
        # Make sure we have regime predictions
        if 'hmm_regime' not in self.df.columns or 'transformer_regime' not in self.df.columns:
            # Make predictions
            self.df['hmm_regime'] = self.hmm_model.predict(self.df)
            self.df['hmm_regime_label'] = [self.hmm_model.regime_labels.get(r, "Unknown") for r in self.df['hmm_regime']]
            
            self.df['transformer_regime'] = self.transformer_model.predict(self.df)
            self.df['transformer_regime_label'] = self.transformer_model.get_regime_labels(self.df['transformer_regime'])
        
        # Create a new dataframe with regimes
        history_df = self.df.copy()
        
        # Combined approach
        if method == 'combined':
            history_df['combined_regime'] = 'Transition'
            history_df['confidence'] = 'low'
            
            # Where models agree, use that regime with high confidence
            agreement_mask = history_df['hmm_regime_label'] == history_df['transformer_regime_label']
            history_df.loc[agreement_mask, 'combined_regime'] = history_df.loc[agreement_mask, 'hmm_regime_label']
            history_df.loc[agreement_mask, 'confidence'] = 'high'
            
            return history_df[['returns', 'hmm_regime_label', 'transformer_regime_label', 'combined_regime', 'confidence']]
            
        elif method == 'hmm':
            return history_df[['returns', 'hmm_regime', 'hmm_regime_label']]
            
        elif method == 'transformer':
            return history_df[['returns', 'transformer_regime', 'transformer_regime_label']]
        
        else:
            # Default to returning all regime information
            return history_df[[
                'returns', 
                'hmm_regime', 'hmm_regime_label',
                'transformer_regime', 'transformer_regime_label'
            ]]

================
File: regimes/detector.py
================
"""
Market regime detection module that can be imported by other files.
Provides functions to detect market regimes using HMM, Transformer, or a combined approach.
"""

import pandas as pd
import os
import numpy as np
import torch
import logging
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Import from our package structure
from dl_metalabeling.models.hmm import load_hmm_model
from dl_metalabeling.models.transformer import load_transformer_model
from dl_metalabeling.utils import calculate_performance_metrics

logger = logging.getLogger(__name__)

# Singleton instance of evaluator for reuse
_evaluator = None

def get_evaluator(reload=False):
    """
    Get the regime evaluator instance.
    
    Parameters:
    reload (bool): Whether to reload the models even if they're already loaded
    
    Returns:
    RegimeEvaluator: Regime evaluator instance
    """
    global _evaluator
    
    # Initialize evaluator if not exists or reload requested
    if _evaluator is None or reload:
        # Default data path and models directory
        data_path = os.path.join('data', 'cmma.csv')
        models_dir = 'models'
        
        # Initialize new evaluator
        _evaluator = RegimeEvaluator(data_path=data_path, models_dir=models_dir)
    
    return _evaluator

def detect_regime(data=None, method='combined'):
    """
    Detect the current market regime.
    
    Parameters:
    data (pd.DataFrame): Optional market data to use, or None to use latest from loaded data
    method (str): Which model to use - 'hmm', 'transformer', or 'combined'
    
    Returns:
    dict: Regime identification results
    """
    evaluator = get_evaluator()
    return evaluator.get_current_regime(data=data, method=method)

def get_hmm_regimes(data=None):
    """
    Get regime predictions using only the HMM model.
    
    Parameters:
    data (pd.DataFrame): Optional market data to use, or None to use loaded data
    
    Returns:
    dict: HMM regime identification results
    """
    evaluator = get_evaluator()
    
    if data is None:
        # Get regime history from loaded data
        regime_history = evaluator.get_regime_history(method='hmm')
        # Return most recent regime
        if not regime_history.empty:
            latest = regime_history.iloc[-1]
            return {
                'regime_id': latest['hmm_regime'],
                'regime_label': latest['hmm_regime_label']
            }
    else:
        # Get regime for provided data
        result = evaluator.get_current_regime(data=data, method='hmm')
        if result and 'hmm' in result:
            return result['hmm']
    
    return None

def get_transformer_regimes(data=None):
    """
    Get regime predictions using only the Transformer model.
    
    Parameters:
    data (pd.DataFrame): Optional market data to use, or None to use loaded data
    
    Returns:
    dict: Transformer regime identification results
    """
    evaluator = get_evaluator()
    
    if data is None:
        # Get regime history from loaded data
        regime_history = evaluator.get_regime_history(method='transformer')
        # Return most recent regime
        if not regime_history.empty:
            latest = regime_history.iloc[-1]
            return {
                'regime_id': latest['transformer_regime'],
                'regime_label': latest['transformer_regime_label']
            }
    else:
        # Get regime for provided data
        result = evaluator.get_current_regime(data=data, method='transformer')
        if result and 'transformer' in result:
            return result['transformer']
    
    return None

def get_combined_regimes(data=None):
    """
    Get regime predictions using a combination of both models.
    
    Parameters:
    data (pd.DataFrame): Optional market data to use, or None to use loaded data
    
    Returns:
    dict: Combined regime identification results
    """
    evaluator = get_evaluator()
    
    if data is None:
        # Get regime history from loaded data
        regime_history = evaluator.get_regime_history(method='combined')
        # Return most recent regime
        if not regime_history.empty:
            latest = regime_history.iloc[-1]
            return {
                'regime': latest['combined_regime'],
                'confidence': latest['confidence']
            }
    else:
        # Get regime for provided data
        result = evaluator.get_current_regime(data=data, method='combined')
        if result and 'combined' in result:
            return result['combined']
    
    return None

def evaluate_models():
    """
    Evaluate and compare the performance of HMM and Transformer models.
    
    Returns:
    pd.DataFrame: Dataframe with evaluation results
    """
    evaluator = get_evaluator()
    return evaluator.evaluate()

def evaluate_by_regime(data, regime_column, regime_labels):
    """
    Evaluate strategy performance metrics by market regime.
    
    Parameters:
    data (pd.DataFrame): DataFrame with regime predictions and returns
    regime_column (str): Column name containing regime labels
    regime_labels (list): List of regime label names
    
    Returns:
    dict: Dictionary of performance metrics by regime
    """
    # Initialize results dictionary
    results = {}
    
    # Ensure we have the necessary columns
    required_cols = ['returns', 'strategy_returns', 'meta_strategy_returns', regime_column]
    if not all(col in data.columns for col in required_cols):
        missing = [col for col in required_cols if col not in data.columns]
        raise ValueError(f"Missing required columns: {missing}")
    
    # Process each regime
    for i, regime_label in enumerate(regime_labels):
        # Get data for this regime
        regime_data = data[data[regime_column] == i].copy()
        
        if len(regime_data) == 0:
            print(f"Warning: No data points for regime {regime_label}")
            continue
        
        # Calculate performance metrics for base strategy
        base_metrics = calculate_performance_metrics(regime_data['strategy_returns'])
        
        # Calculate performance metrics for metalabeled strategy
        meta_metrics = calculate_performance_metrics(regime_data['meta_strategy_returns'])
        
        # Store results
        results[regime_label] = {
            'base': base_metrics,
            'meta': meta_metrics
        }
        
        print(f"Regime: {regime_label} ({len(regime_data)} days)")
        print(f"  Base Strategy - Sharpe: {base_metrics['sharpe_ratio']:.2f}, Annual Return: {base_metrics['annual_return']:.2%}")
        print(f"  Meta Strategy - Sharpe: {meta_metrics['sharpe_ratio']:.2f}, Annual Return: {meta_metrics['annual_return']:.2%}")
    
    return results

class HMMRegimeDetector:
    """
    Hidden Markov Model for market regime detection.
    """
    
    def __init__(self, n_regimes=3, features=None, cov_type='full'):
        """
        Initialize HMM regime detector.
        
        Args:
            n_regimes (int): Number of regimes to detect
            features (list): Features to use for regime detection
            cov_type (str): Covariance type for HMM
        """
        self.n_regimes = n_regimes
        self.features = features or ['returns', 'volatility', 'rsi', 'macd', 'bbands_width']
        self.cov_type = cov_type
        
        # Placeholder for HMM model
        self.model = None
        self.scaler = StandardScaler()
        
        logger.info(f"Initialized HMM regime detector with {n_regimes} regimes")
    
    def train(self, data):
        """
        Train the HMM model.
        
        Args:
            data (pandas.DataFrame): Training data
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training HMM model with {len(data)} samples")
        
        # Extract features
        features_data = data[self.features].copy()
        
        # Scale features
        scaled_data = self.scaler.fit_transform(features_data.fillna(0))
        
        # Placeholder for HMM training
        # In a real implementation, this would train an HMM model
        # using the hmmlearn library
        
        logger.info("HMM model training completed")
        return self
    
    def detect_regimes(self, data):
        """
        Detect regimes in data.
        
        Args:
            data (pandas.DataFrame): Data to detect regimes in
            
        Returns:
            pandas.DataFrame: Data with regime labels
        """
        logger.info(f"Detecting regimes in {len(data)} samples")
        
        # Make a copy of the data
        data_with_regimes = data.copy()
        
        # Extract features
        features_data = data[self.features].copy()
        
        # Scale features
        scaled_data = self.scaler.transform(features_data.fillna(0))
        
        # Placeholder for regime prediction
        # In a real implementation, this would use the trained HMM model
        # to predict the most likely regime for each data point
        
        # Generate random regimes for placeholder
        regimes = np.random.randint(0, self.n_regimes, len(data))
        
        # Add regime column to data
        data_with_regimes['hmm_regime'] = regimes
        
        logger.info("Regime detection completed")
        return data_with_regimes
    
    def save(self, filepath):
        """
        Save the model to file.
        
        Args:
            filepath (str): Path to save the model
        """
        model_data = {
            'n_regimes': self.n_regimes,
            'features': self.features,
            'cov_type': self.cov_type,
            'scaler': self.scaler,
            'model': self.model
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"HMM model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath):
        """
        Load the model from file.
        
        Args:
            filepath (str): Path to load the model from
            
        Returns:
            HMMRegimeDetector: Loaded model
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        detector = cls(
            n_regimes=model_data['n_regimes'],
            features=model_data['features'],
            cov_type=model_data['cov_type']
        )
        
        detector.scaler = model_data['scaler']
        detector.model = model_data['model']
        
        logger.info(f"HMM model loaded from {filepath}")
        return detector


class TransformerRegimeDetector:
    """
    Transformer-based model for market regime detection.
    """
    
    def __init__(self, n_regimes=3, features=None, embedding_dim=64, num_heads=4, num_layers=2):
        """
        Initialize Transformer regime detector.
        
        Args:
            n_regimes (int): Number of regimes to detect
            features (list): Features to use for regime detection
            embedding_dim (int): Dimension of embeddings
            num_heads (int): Number of attention heads
            num_layers (int): Number of transformer layers
        """
        self.n_regimes = n_regimes
        self.features = features or ['returns', 'volatility', 'rsi', 'macd', 'bbands_width']
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        
        # Placeholder for transformer model components
        self.encoder = None
        self.scaler = StandardScaler()
        self.kmeans = KMeans(n_clusters=n_regimes)
        
        logger.info(f"Initialized Transformer regime detector with {n_regimes} regimes")
    
    def train(self, data, batch_size=32, epochs=100, learning_rate=0.001):
        """
        Train the Transformer model.
        
        Args:
            data (pandas.DataFrame): Training data
            batch_size (int): Batch size for training
            epochs (int): Number of training epochs
            learning_rate (float): Learning rate for optimizer
            
        Returns:
            self: Trained model
        """
        logger.info(f"Training Transformer model with {len(data)} samples for {epochs} epochs")
        
        # Extract features
        features_data = data[self.features].copy()
        
        # Scale features
        scaled_data = self.scaler.fit_transform(features_data.fillna(0))
        
        # Placeholder for Transformer training
        # In a real implementation, this would train a Transformer model
        # using PyTorch, followed by K-means clustering on the embeddings
        
        logger.info("Transformer model training completed")
        return self
    
    def detect_regimes(self, data):
        """
        Detect regimes in data.
        
        Args:
            data (pandas.DataFrame): Data to detect regimes in
            
        Returns:
            pandas.DataFrame: Data with regime labels
        """
        logger.info(f"Detecting regimes in {len(data)} samples")
        
        # Make a copy of the data
        data_with_regimes = data.copy()
        
        # Extract features
        features_data = data[self.features].copy()
        
        # Scale features
        scaled_data = self.scaler.transform(features_data.fillna(0))
        
        # Placeholder for regime prediction
        # In a real implementation, this would use the trained Transformer model
        # to extract embeddings, then use the K-means model to cluster them
        
        # Generate random regimes for placeholder
        regimes = np.random.randint(0, self.n_regimes, len(data))
        
        # Add regime column to data
        data_with_regimes['transformer_regime'] = regimes
        
        logger.info("Regime detection completed")
        return data_with_regimes
    
    def save(self, filepath):
        """
        Save the model to file.
        
        Args:
            filepath (str): Path to save the model
        """
        # Save model components separately
        base_path = filepath.rstrip('.pkl')
        
        # Save encoder weights
        if self.encoder is not None:
            torch.save(self.encoder.state_dict(), f"{base_path}_encoder.pth")
        
        # Save scaler
        with open(f"{base_path}_scaler.pkl", 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # Save K-means model
        with open(f"{base_path}_kmeans.pkl", 'wb') as f:
            pickle.dump(self.kmeans, f)
        
        # Save config
        config = {
            'n_regimes': self.n_regimes,
            'features': self.features,
            'embedding_dim': self.embedding_dim,
            'num_heads': self.num_heads,
            'num_layers': self.num_layers
        }
        
        with open(f"{base_path}_config.pkl", 'wb') as f:
            pickle.dump(config, f)
        
        logger.info(f"Transformer model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath):
        """
        Load the model from file.
        
        Args:
            filepath (str): Path to load the model from
            
        Returns:
            TransformerRegimeDetector: Loaded model
        """
        # Load model components separately
        base_path = filepath.rstrip('.pkl')
        
        # Load config
        with open(f"{base_path}_config.pkl", 'rb') as f:
            config = pickle.load(f)
        
        detector = cls(
            n_regimes=config['n_regimes'],
            features=config['features'],
            embedding_dim=config['embedding_dim'],
            num_heads=config['num_heads'],
            num_layers=config['num_layers']
        )
        
        # Load scaler
        with open(f"{base_path}_scaler.pkl", 'rb') as f:
            detector.scaler = pickle.load(f)
        
        # Load K-means model
        with open(f"{base_path}_kmeans.pkl", 'rb') as f:
            detector.kmeans = pickle.load(f)
        
        # Load encoder weights (placeholder)
        # In a real implementation, this would load the encoder weights
        
        logger.info(f"Transformer model loaded from {filepath}")
        return detector


def detect_regime(data, hmm_detector=None, transformer_detector=None):
    """
    Helper function to detect regimes using one or more detectors.
    
    Args:
        data (pandas.DataFrame): Data to detect regimes in
        hmm_detector (HMMRegimeDetector): HMM detector
        transformer_detector (TransformerRegimeDetector): Transformer detector
        
    Returns:
        pandas.DataFrame: Data with detected regimes
    """
    data_with_regimes = data.copy()
    
    if hmm_detector is not None:
        data_with_regimes = hmm_detector.detect_regimes(data_with_regimes)
    
    if transformer_detector is not None:
        data_with_regimes = transformer_detector.detect_regimes(data_with_regimes)
    
    return data_with_regimes

# Usage example
if __name__ == "__main__":
    # Get current market regime
    current_regime = detect_regime()
    print("Current Market Regime:")
    for model, regime_info in current_regime.items():
        print(f"  {model.capitalize()}: {regime_info}")

================
File: regimes/regime_evaluator.py
================
"""
Regime evaluator module that uses pre-trained models to evaluate market regimes.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import os
import warnings

# Import existing model implementations
from dl_metalabeling.models.hmm import load_hmm_model, HiddenMarketModel
from dl_metalabeling.models.transformer import load_transformer_model, TransformerMarketModel

# Suppress specific warnings
warnings.filterwarnings("ignore", category=pd.errors.SettingWithCopyWarning)


class RegimeEvaluator:
    """
    A class to evaluate and compare different market regime detection models.
    Uses pre-trained models rather than training them.
    """
    
    def __init__(self, data_path=os.path.join('data', 'cmma.csv'), models_dir='models'):
        """
        Initialize the evaluator with data path and models directory.
        
        Parameters:
        data_path (str): Path to the market data CSV file
        models_dir (str): Directory containing pre-trained models
        """
        self.data_path = data_path
        self.models_dir = models_dir
        
        # Model-specific properties
        self.hmm_model = None
        self.transformer_model = None
        
        # Data properties
        self.df = pd.read_csv(data_path, parse_dates=['DateTime'], index_col='DateTime')
        print(f"Data shape: {self.df.shape}")
        print(f"Date range: {self.df.index.min()} to {self.df.index.max()}")
        
        # Prepare data
        self._prepare_data()
        
        # Try to load models
        self._load_models()
    
    def _prepare_data(self):
        """Prepare the data for both models."""
        # Calculate returns
        self.df['returns'] = self.df['gbclose'].pct_change()
        self.df['log_returns'] = np.log(self.df['gbclose']).diff()
        
        # Calculate volatility features
        self.df['volatility_5d'] = self.df['returns'].rolling(window=5).std()
        self.df['volatility_20d'] = self.df['returns'].rolling(window=20).std()
        
        # Calculate momentum features  
        self.df['momentum_5d'] = self.df['returns'].rolling(window=5).mean()
        self.df['momentum_20d'] = self.df['returns'].rolling(window=20).mean()
        
        # Calculate mean reversion features
        self.df['ma_20d'] = self.df['gbclose'].rolling(window=20).mean()
        self.df['ma_50d'] = self.df['gbclose'].rolling(window=50).mean()
        self.df['deviation_20d'] = (self.df['gbclose'] - self.df['ma_20d']) / self.df['ma_20d']
        self.df['deviation_50d'] = (self.df['gbclose'] - self.df['ma_50d']) / self.df['ma_50d']
        
        # Clean the data by removing NaN values
        self.df = self.df.dropna().copy()
    
    def _load_models(self):
        """Attempt to load pre-trained models from disk."""
        # Load HMM model
        self.hmm_model = load_hmm_model(self.models_dir)
        if self.hmm_model is not None:
            print("HMM model loaded successfully.")
        else:
            print("HMM model not found. Please train and save a model first.")
        
        # Load Transformer model
        self.transformer_model = load_transformer_model(self.models_dir)
        if self.transformer_model is not None:
            print("Transformer model loaded successfully.")
        else:
            print("Transformer model not found. Please train and save a model first.")
    
    def evaluate(self):
        """Evaluate and compare both models."""
        print("\n===== Model Evaluation =====")
        
        # Check if models are available
        if self.hmm_model is None:
            print("HMM model not available. Please obtain a trained model first.")
            return
            
        if self.transformer_model is None:
            print("Transformer model not available. Please obtain a trained model first.")
            return
        
        # Get predictions from both models
        self.df['hmm_regime'] = self.hmm_model.predict(self.df)
        self.df['hmm_regime_label'] = [self.hmm_model.regime_labels.get(r, "Unknown") for r in self.df['hmm_regime']]
        
        self.df['transformer_regime'] = self.transformer_model.predict(self.df)
        self.df['transformer_regime_label'] = self.transformer_model.get_regime_labels(self.df['transformer_regime'])
        
        # Create a comparison dataframe with both model predictions
        comparison_df = self.df.dropna(subset=['hmm_regime', 'transformer_regime'])
        
        # Analyze regime transitions
        self._analyze_transitions(comparison_df)
        
        # Calculate performance by regime
        self._analyze_performance(comparison_df)
        
        # Measure agreement between models
        self._analyze_agreement(comparison_df)
        
        # Plot regime comparison
        self.plot_comparison()
        
        # Make recommendations
        self._make_recommendation(comparison_df)
        
        return comparison_df
    
    def _analyze_transitions(self, df):
        """
        Analyze regime transition frequency and duration.
        
        Parameters:
        df (pd.DataFrame): Dataframe with regime predictions
        """
        print("\n1. Regime Transition Analysis:")
        
        # Count regime transitions
        hmm_transitions = (df['hmm_regime'].diff() != 0).sum()
        transformer_transitions = (df['transformer_regime'].diff() != 0).sum()
        
        print(f"  HMM transitions: {hmm_transitions}")
        print(f"  Transformer transitions: {transformer_transitions}")
        
        # Calculate regime durations
        hmm_durations = []
        transformer_durations = []
        
        # Helper function to calculate durations
        def get_durations(regime_series):
            durations = []
            current_regime = regime_series.iloc[0]
            current_duration = 1
            
            for regime in regime_series.iloc[1:]:
                if regime == current_regime:
                    current_duration += 1
                else:
                    durations.append(current_duration)
                    current_regime = regime
                    current_duration = 1
                    
            # Add the last duration
            durations.append(current_duration)
            return durations
        
        # Calculate durations for both models
        hmm_durations = get_durations(df['hmm_regime'])
        transformer_durations = get_durations(df['transformer_regime'])
        
        # Print duration statistics
        print("\n2. Regime Duration Statistics:")
        print(f"  HMM average duration: {np.mean(hmm_durations):.2f} days")
        print(f"  HMM median duration: {np.median(hmm_durations):.2f} days")
        print(f"  HMM max duration: {np.max(hmm_durations)} days")
        
        print(f"  Transformer average duration: {np.mean(transformer_durations):.2f} days")
        print(f"  Transformer median duration: {np.median(transformer_durations):.2f} days")
        print(f"  Transformer max duration: {np.max(transformer_durations)} days")
    
    def _analyze_performance(self, df):
        """
        Analyze performance metrics by regime for each model.
        
        Parameters:
        df (pd.DataFrame): Dataframe with regime predictions
        """
        print("\n3. Performance Metrics by Regime:")
        
        # HMM performance
        print("\nHMM Model:")
        for regime_id, label in self.hmm_model.regime_labels.items():
            mask = df['hmm_regime'] == regime_id
            regime_returns = df.loc[mask, 'returns']
            
            print(f"  {label} Regime (Regime {regime_id}):")
            print(f"    Days: {mask.sum()}")
            print(f"    Mean return: {regime_returns.mean() * 100:.4f}%")
            print(f"    Std deviation: {regime_returns.std() * 100:.4f}%")
            print(f"    Sharpe ratio: {(regime_returns.mean() / (regime_returns.std() + 1e-8)) * np.sqrt(252):.4f}")
            print(f"    Win rate: {(regime_returns > 0).mean() * 100:.2f}%")
        
        # Transformer performance
        print("\nTransformer Model:")
        for regime_id, label in self.transformer_model.regime_labels.items():
            mask = df['transformer_regime'] == regime_id
            regime_returns = df.loc[mask, 'returns']
            
            print(f"  {label} Regime (Regime {regime_id}):")
            print(f"    Days: {mask.sum()}")
            print(f"    Mean return: {regime_returns.mean() * 100:.4f}%")
            print(f"    Std deviation: {regime_returns.std() * 100:.4f}%")
            print(f"    Sharpe ratio: {(regime_returns.mean() / (regime_returns.std() + 1e-8)) * np.sqrt(252):.4f}")
            print(f"    Win rate: {(regime_returns > 0).mean() * 100:.2f}%")
    
    def _analyze_agreement(self, df):
        """
        Analyze agreement between models.
        
        Parameters:
        df (pd.DataFrame): Dataframe with regime predictions
        """
        print("\n4. Regime Agreement Analysis:")
        
        # Map regime IDs to common labels for comparison
        df['hmm_label'] = df['hmm_regime'].map(self.hmm_model.regime_labels)
        df['transformer_label'] = df['transformer_regime'].map(self.transformer_model.regime_labels)
        
        # Calculate overall agreement
        agreement = (df['hmm_label'] == df['transformer_label']).mean() * 100
        print(f"  Overall agreement between models: {agreement:.2f}%")
        
        # Calculate Jaccard similarity for each regime type
        for regime_type in ['Volatile', 'Trending', 'Mean-Reverting']:
            hmm_mask = df['hmm_label'] == regime_type
            transformer_mask = df['transformer_label'] == regime_type
            
            # Calculate intersection and union
            intersection = (hmm_mask & transformer_mask).sum()
            union = (hmm_mask | transformer_mask).sum()
            jaccard = intersection / union if union > 0 else 0
            
            print(f"  {regime_type} regime Jaccard similarity: {jaccard:.4f}")
    
    def _make_recommendation(self, df):
        """
        Make recommendation on which model or combination to use.
        
        Parameters:
        df (pd.DataFrame): Dataframe with regime predictions
        """
        print("\n5. Model Recommendation:")
        
        # Calculate metrics for comparison
        hmm_transitions = (df['hmm_regime'].diff() != 0).sum()
        transformer_transitions = (df['transformer_regime'].diff() != 0).sum()
        
        # Calculate Sharpe ratios by regime
        hmm_sharpe_ratios = []
        transformer_sharpe_ratios = []
        
        # HMM Sharpe ratios
        for regime_id in self.hmm_model.regime_labels:
            mask = df['hmm_regime'] == regime_id
            regime_returns = df.loc[mask, 'returns']
            if len(regime_returns) > 0 and regime_returns.std() > 0:
                hmm_sharpe_ratios.append((regime_returns.mean() / regime_returns.std()) * np.sqrt(252))
                
        # Transformer Sharpe ratios
        for regime_id in self.transformer_model.regime_labels:
            mask = df['transformer_regime'] == regime_id
            regime_returns = df.loc[mask, 'returns']
            if len(regime_returns) > 0 and regime_returns.std() > 0:
                transformer_sharpe_ratios.append((regime_returns.mean() / regime_returns.std()) * np.sqrt(252))
        
        # Define and print strengths of each model
        hmm_strengths = []
        transformer_strengths = []
        
        # Transition stability
        if hmm_transitions < transformer_transitions:
            hmm_strengths.append("More stable regimes with fewer transitions")
        else:
            transformer_strengths.append("More stable regimes with fewer transitions")
            
        # Sharpe ratios
        if np.mean(hmm_sharpe_ratios) > np.mean(transformer_sharpe_ratios):
            hmm_strengths.append("Better overall Sharpe ratios across regimes")
        else:
            transformer_strengths.append("Better overall Sharpe ratios across regimes")
            
        # Add inherent model strengths
        hmm_strengths.extend([
            "Simpler model with fewer parameters",
            "Works well with limited data",
            "Captures regime transitions well"
        ])
        
        transformer_strengths.extend([
            "Can capture more complex patterns",
            "Better at handling long-term dependencies",
            "Self-supervised learning captures market structure"
        ])
        
        # Print strengths
        print("\nHMM Strengths:")
        for strength in hmm_strengths:
            print(f"  - {strength}")
            
        print("\nTransformer Strengths:")
        for strength in transformer_strengths:
            print(f"  - {strength}")
            
        # Make recommendation
        print("\nRecommendation:")
        print("Based on the evaluation, I recommend:")
        print("  - Use a COMBINED APPROACH that leverages both models:")
        print("    1. Use HMM for stable regime identification (fewer transitions)")
        print("    2. Use Transformer for more nuanced pattern recognition within regimes")
        print("    3. When models agree, have higher confidence in the regime identification")
        print("    4. When models disagree, consider it a transitional period")
    
    def plot_comparison(self):
        """Plot regime comparison between models."""
        print("\nGenerating regime comparison plot...")
        
        # Make sure both models are available
        if self.hmm_model is None or self.transformer_model is None:
            print("Cannot plot comparison - both models must be available.")
            return
            
        # Use data where both models have predictions
        comparison_df = self.df.dropna(subset=['hmm_regime', 'transformer_regime'])
        
        # Create figure with multiple subplots
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15), sharex=True)
        
        # Plot price in the top subplot
        ax1.plot(comparison_df.index, comparison_df['gbclose'], color='black')
        ax1.set_title('Price and Regime Comparison', fontsize=16)
        ax1.set_ylabel('Price', fontsize=14)
        ax1.grid(True, alpha=0.3)
        
        # Define colors for regimes
        colors = {'Volatile': 'red', 'Trending': 'green', 'Mean-Reverting': 'blue', 'Unknown': 'gray'}
        
        # Plot HMM regimes in the middle subplot
        for regime_id, label in self.hmm_model.regime_labels.items():
            mask = comparison_df['hmm_regime'] == regime_id
            mask_indices = comparison_df.index[mask]
            
            # Create a color series for this regime
            color_series = pd.Series(1, index=mask_indices)
            
            # Plot as colored area
            ax2.fill_between(mask_indices, 0, color_series, alpha=0.7, color=colors[label], label=label)
            
        ax2.set_title('HMM Model Regimes', fontsize=14)
        ax2.set_ylabel('Regime', fontsize=12)
        ax2.legend(loc='upper right')
        ax2.grid(False)
        
        # Plot Transformer regimes in the bottom subplot
        for regime_id, label in self.transformer_model.regime_labels.items():
            mask = comparison_df['transformer_regime'] == regime_id
            mask_indices = comparison_df.index[mask]
            
            # Create a color series for this regime
            color_series = pd.Series(1, index=mask_indices)
            
            # Plot as colored area
            ax3.fill_between(mask_indices, 0, color_series, alpha=0.7, color=colors[label], label=label)
            
        ax3.set_title('Transformer Model Regimes', fontsize=14)
        ax3.set_xlabel('Date', fontsize=14)
        ax3.set_ylabel('Regime', fontsize=12)
        ax3.legend(loc='upper right')
        ax3.grid(False)
        
        # Format x-axis dates
        ax3.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        ax3.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig('regime_model_comparison.png', dpi=300)
        plt.close(fig)
        print("Plot saved as 'regime_model_comparison.png'")
    
    def get_current_regime(self, data=None, method='combined'):
        """
        Get the current market regime.
        
        Parameters:
        data (pd.DataFrame): Optional recent data to use
        method (str): Which model to use - 'hmm', 'transformer', or 'combined'
        
        Returns:
        dict: Information about the current regime
        """
        # Check if models are available
        if self.hmm_model is None or self.transformer_model is None:
            print("Cannot get regime - models are not available.")
            return None
        
        # Use most recent data from loaded dataset if no data provided
        if data is None:
            recent_data = self.df.iloc[-50:].copy()
        else:
            recent_data = data.copy()
        
        # Get predictions from both models
        result = {}
        
        # HMM regime
        if method in ['hmm', 'combined']:
            hmm_regimes = self.hmm_model.predict(recent_data)
            hmm_regime = hmm_regimes[-1]  # Most recent regime
            hmm_label = self.hmm_model.regime_labels.get(hmm_regime, "Unknown")
            
            result['hmm'] = {
                'regime_id': hmm_regime,
                'regime_label': hmm_label
            }
        
        # Transformer regime
        if method in ['transformer', 'combined']:
            transformer_regimes = self.transformer_model.predict(recent_data)
            transformer_regime = transformer_regimes[-1]  # Most recent regime
            transformer_label = self.transformer_model.regime_labels.get(transformer_regime, "Unknown")
            
            result['transformer'] = {
                'regime_id': transformer_regime,
                'regime_label': transformer_label
            }
        
        # Combined approach
        if method == 'combined' and 'hmm' in result and 'transformer' in result:
            if result['hmm']['regime_label'] == result['transformer']['regime_label']:
                # Models agree - high confidence
                result['combined'] = {
                    'regime_label': result['hmm']['regime_label'],
                    'confidence': 'high'
                }
            else:
                # Models disagree - consider it a transition
                result['combined'] = {
                    'regime_label': 'Transition',
                    'hmm_regime': result['hmm']['regime_label'],
                    'transformer_regime': result['transformer']['regime_label'],
                    'confidence': 'low'
                }
        
        return result
    
    def get_regime_history(self, method='combined'):
        """
        Get historical regime predictions.
        
        Parameters:
        method (str): Which model to use - 'hmm', 'transformer', or 'combined'
        
        Returns:
        pd.DataFrame: DataFrame with regime predictions
        """
        # Check if models are available
        if self.hmm_model is None or self.transformer_model is None:
            print("Cannot get regime history - models are not available.")
            return pd.DataFrame()
        
        # Make sure we have regime predictions
        if 'hmm_regime' not in self.df.columns or 'transformer_regime' not in self.df.columns:
            # Make predictions
            self.df['hmm_regime'] = self.hmm_model.predict(self.df)
            self.df['hmm_regime_label'] = [self.hmm_model.regime_labels.get(r, "Unknown") for r in self.df['hmm_regime']]
            
            self.df['transformer_regime'] = self.transformer_model.predict(self.df)
            self.df['transformer_regime_label'] = self.transformer_model.get_regime_labels(self.df['transformer_regime'])
        
        # Create a new dataframe with regimes
        history_df = self.df.copy()
        
        # Combined approach
        if method == 'combined':
            history_df['combined_regime'] = 'Transition'
            history_df['confidence'] = 'low'
            
            # Where models agree, use that regime with high confidence
            agreement_mask = history_df['hmm_regime_label'] == history_df['transformer_regime_label']
            history_df.loc[agreement_mask, 'combined_regime'] = history_df.loc[agreement_mask, 'hmm_regime_label']
            history_df.loc[agreement_mask, 'confidence'] = 'high'
            
            return history_df[['returns', 'hmm_regime_label', 'transformer_regime_label', 'combined_regime', 'confidence']]
            
        elif method == 'hmm':
            return history_df[['returns', 'hmm_regime', 'hmm_regime_label']]
            
        elif method == 'transformer':
            return history_df[['returns', 'transformer_regime', 'transformer_regime_label']]
        
        else:
            # Default to returning all regime information
            return history_df[[
                'returns', 
                'hmm_regime', 'hmm_regime_label',
                'transformer_regime', 'transformer_regime_label'
            ]]

================
File: rl_position_sizing.py
================
"""
Reinforcement Learning-based Dynamic Position Sizing module.

This module provides functionality for dynamically sizing trading positions
based on meta-labeling confidence, market regimes, and transaction costs.
The primary goal is to reduce unnecessary position changes that incur transaction costs.
"""

import numpy as np
import pandas as pd
from collections import deque
import pickle
import os

class RLPositionSizer:
    """
    Reinforcement Learning agent for dynamic position sizing
    that aims to minimize transaction costs while maximizing returns.
    """
    
    def __init__(self, n_states=10, n_actions=3, learning_rate=0.1, 
                 discount_factor=0.95, exploration_rate=1.0, 
                 exploration_decay=0.995, min_exploration_rate=0.01,
                 smoothing_alpha=0.2, transaction_cost=0.001, 
                 state_window=5):
        """
        Initialize the RL Position Sizer.
        
        Parameters:
        -----------
        n_states : int
            Number of discretized states for the Q-table
        n_actions : int
            Number of possible actions (e.g., increase, decrease, maintain position size)
        learning_rate : float
            Learning rate for Q-learning
        discount_factor : float
            Discount factor for future rewards
        exploration_rate : float
            Initial exploration rate
        exploration_decay : float
            Rate at which exploration decreases
        min_exploration_rate : float
            Minimum exploration rate
        smoothing_alpha : float
            EMA smoothing factor for position size changes
        transaction_cost : float
            Cost per unit of position size change (as a fraction)
        state_window : int
            Number of past returns and volatility observations to include in state
        """
        self.n_states = n_states
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.exploration_decay = exploration_decay
        self.min_exploration_rate = min_exploration_rate
        self.smoothing_alpha = smoothing_alpha
        self.transaction_cost = transaction_cost
        self.state_window = state_window
        
        # Initialize Q-table: states × actions
        self.q_table = np.zeros((n_states, n_states, n_states, n_actions))
        
        # History of returns and volatility for state representation
        self.returns_history = deque(maxlen=state_window)
        self.volatility_history = deque(maxlen=state_window)
        self.position_history = deque(maxlen=state_window)
        
        # Track current position size
        self.current_position_size = 0.0
        
        # For tracking performance
        self.total_reward = 0.0
        self.total_transaction_cost = 0.0
        self.episode_count = 0

    def discretize_state(self, confidence, regime, returns, volatility, position):
        """
        Convert continuous state values to discrete states for the Q-table.
        
        Parameters:
        -----------
        confidence : float
            Meta-labeling model confidence (0-1)
        regime : int
            Market regime identifier
        returns : list
            Recent returns history
        volatility : list
            Recent volatility history
        position : list
            Recent position history
            
        Returns:
        --------
        tuple: Discretized state indices
        """
        # Discretize confidence into n_states buckets
        conf_state = min(int(confidence * self.n_states), self.n_states - 1)
        
        # Use regime directly as one dimension of the state - ensure it's an integer
        try:
            regime_state = min(int(regime), self.n_states - 1)
        except (ValueError, TypeError):
            # Handle case where regime might be a float or other non-integer value
            regime_state = 0
            print(f"Warning: Invalid regime value {regime}, using default 0")
        
        # Calculate trend strength from returns and volatility
        if len(returns) > 0 and len(volatility) > 0:
            avg_return = np.mean(returns)
            avg_vol = np.mean(volatility)
            if avg_vol == 0:  # Avoid division by zero
                trend_strength = 0
            else:
                # Sharpe-like ratio as trend strength
                trend_strength = avg_return / avg_vol
                
            # Normalize and discretize trend strength
            trend_state = int((np.clip(trend_strength + 2, 0, 4) / 4) * self.n_states)
            trend_state = min(trend_state, self.n_states - 1)
        else:
            trend_state = 0
            
        return int(conf_state), int(regime_state), int(trend_state)

    def get_action(self, state, training=True):
        """
        Select an action using epsilon-greedy policy.
        
        Parameters:
        -----------
        state : tuple
            Current discretized state
        training : bool
            Whether to use exploration or exploitation
            
        Returns:
        --------
        int: Selected action index
        """
        if training and np.random.random() < self.exploration_rate:
            # Exploration: select random action
            return np.random.randint(self.n_actions)
        else:
            # Exploitation: select best action from Q-table
            try:
                # Unpack state tuple for proper indexing
                conf_state, regime_state, trend_state = state
                return np.argmax(self.q_table[conf_state, regime_state, trend_state])
            except Exception as e:
                print(f"Error getting action: {e}")
                print(f"State: {state}")
                # Return middle action (maintain position) if error occurs
                return 1

    def update_q_table(self, state, action, reward, next_state):
        """
        Update Q-table using Q-learning algorithm.
        
        Parameters:
        -----------
        state : tuple
            Current state
        action : int
            Chosen action
        reward : float
            Reward received
        next_state : tuple
            Next state after taking action
        """
        # Q-learning formula
        try:
            # Use state indices individually instead of as a tuple
            conf_state, regime_state, trend_state = state
            next_conf_state, next_regime_state, next_trend_state = next_state
            
            # Find best next action
            best_next_action = np.argmax(self.q_table[next_conf_state, next_regime_state, next_trend_state])
            
            # Calculate TD target and error
            td_target = reward + self.discount_factor * self.q_table[next_conf_state, next_regime_state, next_trend_state, best_next_action]
            td_error = td_target - self.q_table[conf_state, regime_state, trend_state, action]
            
            # Update Q-value
            self.q_table[conf_state, regime_state, trend_state, action] += self.learning_rate * td_error
        except Exception as e:
            print(f"Error updating Q-table: {e}")
            print(f"State: {state}, Action: {action}, Next State: {next_state}")
            # Continue without updating if error occurs
        
        # Decay exploration rate
        self.exploration_rate = max(
            self.min_exploration_rate, 
            self.exploration_rate * self.exploration_decay
        )

    def calculate_reward(self, profit, size_change):
        """
        Calculate reward based on profit and transaction costs.
        
        Parameters:
        -----------
        profit : float
            Trading profit
        size_change : float
            Absolute change in position size
            
        Returns:
        --------
        float: Net reward after transaction costs
        """
        # Calculate transaction cost
        cost = abs(size_change) * self.transaction_cost
        
        # Track total transaction cost
        self.total_transaction_cost += cost
        
        # Net reward
        net_reward = profit - cost
        self.total_reward += net_reward
        
        return net_reward

    def smooth_position(self, new_size):
        """
        Apply EMA smoothing to position size changes.
        
        Parameters:
        -----------
        new_size : float
            New position size
            
        Returns:
        --------
        float: Smoothed position size
        """
        return self.smoothing_alpha * new_size + (1 - self.smoothing_alpha) * self.current_position_size

    def size_to_action(self, confidence):
        """
        Convert confidence to action space.
        
        Parameters:
        -----------
        confidence : float
            Meta-labeling confidence (0-1)
            
        Returns:
        --------
        float: Position size multiplier (-1 to 1)
        """
        # Map action index to position size multiplier
        action_map = {
            0: -1.0,  # Full short position
            1: 0.0,   # No position
            2: 1.0    # Full long position
        }
        
        # Get current state
        state = self.get_current_state(confidence)
        
        # Get action based on state
        action = self.get_action(state)
        
        # Get size multiplier from action map
        size_multiplier = action_map[action]
        
        return size_multiplier

    def get_current_state(self, confidence, regime=0):
        """
        Get the current state for decision making.
        
        Parameters:
        -----------
        confidence : float
            Meta-labeling confidence (0-1)
        regime : int
            Market regime identifier
            
        Returns:
        --------
        tuple: Current state tuple
        """
        # Default values if history is empty
        if not self.returns_history:
            self.returns_history.extend([0.0] * self.state_window)
        if not self.volatility_history:
            self.volatility_history.extend([0.0] * self.state_window)
        if not self.position_history:
            self.position_history.extend([0.0] * self.state_window)
            
        # Discretize current state
        return self.discretize_state(
            confidence, 
            regime,
            list(self.returns_history),
            list(self.volatility_history),
            list(self.position_history)
        )

    def update_state_history(self, return_value, volatility, position):
        """
        Update the history of returns and volatility.
        
        Parameters:
        -----------
        return_value : float
            Latest return
        volatility : float
            Latest volatility measure
        position : float
            Latest position size
        """
        self.returns_history.append(return_value)
        self.volatility_history.append(volatility)
        self.position_history.append(position)

    def decide_position_size(self, confidence, regime, return_value, volatility, base_size=1.0):
        """
        Decide the position size based on current state and policy.
        
        Parameters:
        -----------
        confidence : float
            Meta-labeling confidence (0-1)
        regime : int
            Market regime identifier
        return_value : float
            Latest return
        volatility : float
            Latest volatility measure
        base_size : float
            Base position size (multiplier)
            
        Returns:
        --------
        float: New position size
        """
        # Update state history
        self.update_state_history(return_value, volatility, self.current_position_size)
        
        # Get current state
        state = self.get_current_state(confidence, regime)
        
        # Get action based on state
        action = self.get_action(state)
        
        # Map action to position size multiplier
        if action == 0:
            # Decrease position size
            new_position = max(-1.0, self.current_position_size - 0.25) * base_size
        elif action == 1:
            # Maintain position size
            new_position = self.current_position_size * base_size
        else:  # action == 2
            # Increase position size
            new_position = min(1.0, self.current_position_size + 0.25) * base_size
        
        # Calculate size change and reward
        size_change = new_position - self.current_position_size
        position_sign = 1 if self.current_position_size > 0 else (-1 if self.current_position_size < 0 else 0)
        profit = position_sign * return_value * abs(self.current_position_size)
        reward = self.calculate_reward(profit, size_change)
        
        # Apply smoothing to position size
        smoothed_position = self.smooth_position(new_position)
        
        # Update Q-table if in training mode
        next_state = self.get_current_state(confidence, regime)
        self.update_q_table(state, action, reward, next_state)
        
        # Update current position
        self.current_position_size = smoothed_position
        
        return smoothed_position

    def train(self, data, confidence_col, regime_col, returns_col, volatility_col='atr', 
              episodes=100, base_size=1.0):
        """
        Train the RL agent on historical data.
        
        Parameters:
        -----------
        data : DataFrame
            Historical data with required columns
        confidence_col : str
            Column name for meta-labeling confidence
        regime_col : str
            Column name for market regime
        returns_col : str
            Column name for returns
        volatility_col : str
            Column name for volatility
        episodes : int
            Number of training episodes
        base_size : float
            Base position size
            
        Returns:
        --------
        DataFrame: Performance statistics from training
        """
        performance_stats = []
        
        for episode in range(episodes):
            # Reset environment
            self.current_position_size = 0.0
            self.total_reward = 0.0
            self.total_transaction_cost = 0.0
            self.returns_history.clear()
            self.volatility_history.clear()
            self.position_history.clear()
            
            # Reset exploration rate at the beginning of each episode
            if episode == 0:
                self.exploration_rate = 1.0
            else:
                self.exploration_rate = max(
                    self.min_exploration_rate,
                    self.exploration_rate * self.exploration_decay
                )
            
            cum_return = 0.0
            positions = []
            
            # Iterate through each timestep
            for i in range(len(data)):
                row = data.iloc[i]
                confidence = row[confidence_col]
                regime = row[regime_col]
                ret = row[returns_col]
                vol = row[volatility_col]
                
                # Get position size
                position = self.decide_position_size(confidence, regime, ret, vol, base_size)
                positions.append(position)
                
                # Calculate period return (simplified)
                period_return = position * ret
                cum_return += period_return
            
            # Record performance
            performance_stats.append({
                'episode': episode,
                'final_reward': self.total_reward,
                'transaction_costs': self.total_transaction_cost,
                'cumulative_return': cum_return,
                'sharpe_ratio': cum_return / (np.std(positions) + 1e-6)
            })
            
            self.episode_count += 1
            
            # Print progress
            if (episode + 1) % 10 == 0:
                print(f"Episode {episode+1}/{episodes} - "
                      f"Reward: {self.total_reward:.4f}, "
                      f"Return: {cum_return:.4f}, "
                      f"Costs: {self.total_transaction_cost:.4f}")
        
        return pd.DataFrame(performance_stats)

    def apply(self, data, confidence_col, regime_col=None, returns_col=None, 
              volatility_col=None, base_size=1.0):
        """
        Apply the trained RL agent to new data.
        
        Parameters:
        -----------
        data : DataFrame
            New data with required columns
        confidence_col : str
            Column name for meta-labeling confidence
        regime_col : str
            Column name for market regime
        returns_col : str
            Column name for returns
        volatility_col : str
            Column name for volatility
        base_size : float
            Base position size
            
        Returns:
        --------
        DataFrame: Original data with position sizes added
        """
        # Make a copy to avoid modifying the original
        result = data.copy()
        result['rl_position_size'] = 0.0
        
        # Reset position size
        self.current_position_size = 0.0
        
        # Apply the model to each row (no exploration in application)
        for i in range(len(result)):
            row = result.iloc[i]
            confidence = row[confidence_col]
            
            # Use default values if columns not provided
            regime = row[regime_col] if regime_col and regime_col in row else 0
            ret = row[returns_col] if returns_col and returns_col in row else 0
            vol = row[volatility_col] if volatility_col and volatility_col in row else 0.01
            
            # Get position size without updating Q-table
            state = self.get_current_state(confidence, regime)
            action = self.get_action(state, training=False)
            
            # Map action to position size adjustments
            if action == 0:
                new_position = max(-1.0, self.current_position_size - 0.25) * base_size
            elif action == 1:
                new_position = self.current_position_size * base_size
            else:  # action == 2
                new_position = min(1.0, self.current_position_size + 0.25) * base_size
            
            # Apply smoothing
            smoothed_position = self.smooth_position(new_position)
            self.current_position_size = smoothed_position
            
            # Update state history
            self.update_state_history(ret, vol, smoothed_position)
            
            # Store position size
            result.loc[result.index[i], 'rl_position_size'] = smoothed_position
        
        return result

    def save(self, filepath):
        """
        Save the trained model to a file.
        
        Parameters:
        -----------
        filepath : str
            Path to save the model
        """
        model_data = {
            'q_table': self.q_table,
            'n_states': self.n_states,
            'n_actions': self.n_actions,
            'learning_rate': self.learning_rate,
            'discount_factor': self.discount_factor,
            'exploration_rate': self.exploration_rate,
            'min_exploration_rate': self.min_exploration_rate,
            'smoothing_alpha': self.smoothing_alpha,
            'transaction_cost': self.transaction_cost,
            'state_window': self.state_window,
            'episode_count': self.episode_count
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
            
        print(f"Model saved to {filepath}")

    @classmethod
    def load(cls, filepath):
        """
        Load a trained model from a file.
        
        Parameters:
        -----------
        filepath : str
            Path to the saved model
            
        Returns:
        --------
        RLPositionSizer: Loaded model
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        # Create a new instance
        sizer = cls(
            n_states=model_data['n_states'],
            n_actions=model_data['n_actions'],
            learning_rate=model_data['learning_rate'],
            discount_factor=model_data['discount_factor'],
            exploration_rate=model_data['exploration_rate'],
            min_exploration_rate=model_data['min_exploration_rate'],
            smoothing_alpha=model_data['smoothing_alpha'],
            transaction_cost=model_data['transaction_cost'],
            state_window=model_data['state_window']
        )
        
        # Load the Q-table and other saved attributes
        sizer.q_table = model_data['q_table']
        sizer.episode_count = model_data['episode_count']
        
        return sizer


def calculate_transaction_costs(positions, transaction_cost=0.001):
    """
    Calculate transaction costs from a series of positions.
    
    Parameters:
    -----------
    positions : array-like
        Series of position sizes
    transaction_cost : float
        Cost per unit of position size change
        
    Returns:
    --------
    float: Total transaction costs
    """
    if len(positions) <= 1:
        return 0.0
    
    # Calculate absolute differences between consecutive positions
    position_changes = np.abs(np.diff(positions))
    
    # Calculate transaction costs
    total_cost = np.sum(position_changes) * transaction_cost
    
    return total_cost


def evaluate_rl_strategy(data, base_position_col, rl_position_col, returns_col, 
                         transaction_cost=0.001):
    """
    Evaluate the RL strategy against the base strategy.
    
    Parameters:
    -----------
    data : DataFrame
        Data with positions and returns
    base_position_col : str
        Column name for base strategy positions
    rl_position_col : str
        Column name for RL strategy positions
    returns_col : str
        Column name for returns
    transaction_cost : float
        Cost per unit of position size change
        
    Returns:
    --------
    dict: Performance metrics
    """
    # Make a copy of the data
    result = data.copy()
    
    # Calculate returns for both strategies (before transaction costs)
    result['base_return'] = result[base_position_col] * result[returns_col]
    result['rl_return'] = result[rl_position_col] * result[returns_col]
    
    # Calculate cumulative returns
    result['base_cum_return'] = result['base_return'].cumsum()
    result['rl_cum_return'] = result['rl_return'].cumsum()
    
    # Calculate transaction costs
    base_costs = calculate_transaction_costs(result[base_position_col].values, transaction_cost)
    rl_costs = calculate_transaction_costs(result[rl_position_col].values, transaction_cost)
    
    # Adjust final returns for transaction costs
    base_net_return = result['base_cum_return'].iloc[-1] - base_costs
    rl_net_return = result['rl_cum_return'].iloc[-1] - rl_costs
    
    # Calculate performance metrics
    base_sharpe = result['base_return'].mean() / (result['base_return'].std() + 1e-6) * np.sqrt(252)
    rl_sharpe = result['rl_return'].mean() / (result['rl_return'].std() + 1e-6) * np.sqrt(252)
    
    # Calculate average position size and turnover
    base_avg_size = np.mean(np.abs(result[base_position_col]))
    rl_avg_size = np.mean(np.abs(result[rl_position_col]))
    
    base_turnover = np.sum(np.abs(np.diff(result[base_position_col]))) / len(result)
    rl_turnover = np.sum(np.abs(np.diff(result[rl_position_col]))) / len(result)
    
    metrics = {
        'base_return': base_net_return,
        'rl_return': rl_net_return,
        'improvement': rl_net_return - base_net_return,
        'base_costs': base_costs,
        'rl_costs': rl_costs,
        'cost_reduction': base_costs - rl_costs,
        'base_sharpe': base_sharpe,
        'rl_sharpe': rl_sharpe,
        'base_avg_size': base_avg_size,
        'rl_avg_size': rl_avg_size,
        'base_turnover': base_turnover,
        'rl_turnover': rl_turnover
    }
    
    return metrics

================
File: run_experiments.py
================
"""
Experiment runner for DL MetaLabeling Framework.

This script allows running experiments with different combinations of:
1. Market regime detection methods
2. Meta-labeling models
3. Position sizing models

Each experiment will be evaluated and performance metrics will be compared.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import logging
import argparse
from typing import Dict, List, Tuple

from dl_metalabeling.project_plan import DLMetaLabelingFramework
from dl_metalabeling.metalabeling import calculate_performance_metrics

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


def run_baseline_experiment(data_path: str, output_dir: str = 'results'):
    """
    Run baseline experiment with default settings.
    
    Args:
        data_path (str): Path to data file
        output_dir (str): Directory to save results
    """
    logger.info("Running baseline experiment")
    
    # Initialize framework with default settings
    framework = DLMetaLabelingFramework()
    
    # Override data path
    framework.config['data']['filepath'] = data_path
    
    # Run pipeline
    final_data = framework.run_pipeline()
    
    # Evaluate on test data
    evaluation = framework.evaluate_on_test_data()
    
    # Save models
    os.makedirs(os.path.join(output_dir, 'models', 'baseline'), exist_ok=True)
    framework.save_models(os.path.join(output_dir, 'models', 'baseline'))
    
    # Save results
    os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
    
    # Save performance metrics
    pd.DataFrame({
        'Metric': list(evaluation['base'].keys()),
        'Base Strategy': [evaluation['base'][m] for m in evaluation['base'].keys()],
        'Meta-Labeled Strategy': [evaluation['meta'][m] for m in evaluation['meta'].keys()],
        'RL Position Sizing': [evaluation['rl'][m] for m in evaluation['rl'].keys()]
    }).to_csv(os.path.join(output_dir, 'results', 'baseline_metrics.csv'), index=False)
    
    # Save returns series for plotting
    returns_data = pd.DataFrame({
        'DateTime': final_data.index,
        'Base Returns': final_data['strategy_returns'],
        'Meta Returns': final_data['meta_strategy_returns'],
        'RL Returns': final_data['rl_strategy_returns']
    })
    returns_data.to_csv(os.path.join(output_dir, 'results', 'baseline_returns.csv'), index=False)
    
    # Plot cumulative returns
    plt.figure(figsize=(12, 8))
    plt.plot((1 + final_data['strategy_returns']).cumprod() - 1, label='Base Strategy')
    plt.plot((1 + final_data['meta_strategy_returns']).cumprod() - 1, label='Meta-Labeled Strategy')
    plt.plot((1 + final_data['rl_strategy_returns']).cumprod() - 1, label='RL Position Sizing')
    plt.title('Cumulative Returns - Baseline Experiment')
    plt.xlabel('Time')
    plt.ylabel('Cumulative Returns')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, 'results', 'baseline_cumulative_returns.png'))
    
    logger.info("Baseline experiment completed")
    
    return framework, evaluation


def run_regime_comparison_experiment(data_path: str, output_dir: str = 'results'):
    """
    Compare different market regime detection methods.
    
    Args:
        data_path (str): Path to data file
        output_dir (str): Directory to save results
    """
    logger.info("Running market regime comparison experiment")
    
    # Define regime detection methods to compare
    methods = [
        ['hmm'],
        ['transformer'],
        ['hmm', 'transformer']
    ]
    
    results = {}
    
    for method in methods:
        method_name = '_'.join(method)
        logger.info(f"Testing regime detection method: {method_name}")
        
        # Initialize framework
        framework = DLMetaLabelingFramework()
        
        # Override data path and regime detection method
        framework.config['data']['filepath'] = data_path
        framework.config['regime_detection']['methods'] = method
        
        # Run pipeline
        try:
            final_data = framework.run_pipeline()
            
            # Evaluate on test data
            evaluation = framework.evaluate_on_test_data()
            
            # Save models
            os.makedirs(os.path.join(output_dir, 'models', f'regime_{method_name}'), exist_ok=True)
            framework.save_models(os.path.join(output_dir, 'models', f'regime_{method_name}'))
            
            # Store results
            results[method_name] = {
                'framework': framework,
                'evaluation': evaluation,
                'final_data': final_data
            }
            
            # Save returns series for plotting
            returns_data = pd.DataFrame({
                'DateTime': final_data.index,
                'Base Returns': final_data['strategy_returns'],
                'Meta Returns': final_data['meta_strategy_returns'],
                'RL Returns': final_data['rl_strategy_returns']
            })
            returns_data.to_csv(os.path.join(output_dir, 'results', f'regime_{method_name}_returns.csv'), index=False)
        
        except Exception as e:
            logger.error(f"Error running experiment with method {method_name}: {e}")
    
    # Compare results
    if results:
        # Prepare data for comparison
        comparison_data = []
        
        for method_name, result in results.items():
            evaluation = result['evaluation']
            comparison_data.append({
                'Method': method_name,
                'Base Sharpe': evaluation['base']['sharpe_ratio'],
                'Meta Sharpe': evaluation['meta']['sharpe_ratio'],
                'RL Sharpe': evaluation['rl']['sharpe_ratio'],
                'Base Return': evaluation['base']['annual_return'],
                'Meta Return': evaluation['meta']['annual_return'],
                'RL Return': evaluation['rl']['annual_return'],
                'Base Drawdown': evaluation['base']['max_drawdown'],
                'Meta Drawdown': evaluation['meta']['max_drawdown'],
                'RL Drawdown': evaluation['rl']['max_drawdown']
            })
        
        # Save comparison results
        comparison_df = pd.DataFrame(comparison_data)
        os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
        comparison_df.to_csv(os.path.join(output_dir, 'results', 'regime_comparison.csv'), index=False)
        
        # Plot comparison
        plt.figure(figsize=(12, 8))
        
        # Plot Sharpe ratios
        plt.subplot(3, 1, 1)
        plt.bar(comparison_df['Method'] + ' Base', comparison_df['Base Sharpe'], color='blue', alpha=0.7)
        plt.bar(comparison_df['Method'] + ' Meta', comparison_df['Meta Sharpe'], color='green', alpha=0.7)
        plt.bar(comparison_df['Method'] + ' RL', comparison_df['RL Sharpe'], color='red', alpha=0.7)
        plt.title('Sharpe Ratio Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Plot annual returns
        plt.subplot(3, 1, 2)
        plt.bar(comparison_df['Method'] + ' Base', comparison_df['Base Return'], color='blue', alpha=0.7)
        plt.bar(comparison_df['Method'] + ' Meta', comparison_df['Meta Return'], color='green', alpha=0.7)
        plt.bar(comparison_df['Method'] + ' RL', comparison_df['RL Return'], color='red', alpha=0.7)
        plt.title('Annual Return Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Plot maximum drawdowns
        plt.subplot(3, 1, 3)
        plt.bar(comparison_df['Method'] + ' Base', comparison_df['Base Drawdown'], color='blue', alpha=0.7)
        plt.bar(comparison_df['Method'] + ' Meta', comparison_df['Meta Drawdown'], color='green', alpha=0.7)
        plt.bar(comparison_df['Method'] + ' RL', comparison_df['RL Drawdown'], color='red', alpha=0.7)
        plt.title('Maximum Drawdown Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'results', 'regime_comparison.png'))
    
    logger.info("Market regime comparison experiment completed")
    
    return results


def run_meta_labeling_comparison_experiment(data_path: str, output_dir: str = 'results'):
    """
    Compare different meta-labeling models.
    
    Args:
        data_path (str): Path to data file
        output_dir (str): Directory to save results
    """
    logger.info("Running meta-labeling comparison experiment")
    
    # Define meta-labeling models to compare
    models = [
        ['random_forest'],
        ['neural_network'],
    ]
    
    # Define whether to use regime-specific meta-labeling
    regime_specific_options = [True, False]
    
    results = {}
    
    for model in models:
        for regime_specific in regime_specific_options:
            model_name = '_'.join(model)
            config_name = f"{model_name}_{'regime' if regime_specific else 'global'}"
            
            logger.info(f"Testing meta-labeling model: {config_name}")
            
            # Initialize framework
            framework = DLMetaLabelingFramework()
            
            # Override data path and meta-labeling configuration
            framework.config['data']['filepath'] = data_path
            framework.config['meta_labeling']['models'] = model
            framework.config['meta_labeling']['regime_specific'] = regime_specific
            
            # Run pipeline
            try:
                final_data = framework.run_pipeline()
                
                # Evaluate on test data
                evaluation = framework.evaluate_on_test_data()
                
                # Save models
                os.makedirs(os.path.join(output_dir, 'models', f'meta_{config_name}'), exist_ok=True)
                framework.save_models(os.path.join(output_dir, 'models', f'meta_{config_name}'))
                
                # Store results
                results[config_name] = {
                    'framework': framework,
                    'evaluation': evaluation,
                    'final_data': final_data
                }
                
                # Save returns series for plotting
                returns_data = pd.DataFrame({
                    'DateTime': final_data.index,
                    'Base Returns': final_data['strategy_returns'],
                    'Meta Returns': final_data['meta_strategy_returns'],
                    'RL Returns': final_data['rl_strategy_returns']
                })
                returns_data.to_csv(os.path.join(output_dir, 'results', f'meta_{config_name}_returns.csv'), index=False)
            
            except Exception as e:
                logger.error(f"Error running experiment with config {config_name}: {e}")
    
    # Compare results
    if results:
        # Prepare data for comparison
        comparison_data = []
        
        for config_name, result in results.items():
            evaluation = result['evaluation']
            comparison_data.append({
                'Config': config_name,
                'Meta Sharpe': evaluation['meta']['sharpe_ratio'],
                'RL Sharpe': evaluation['rl']['sharpe_ratio'],
                'Meta Return': evaluation['meta']['annual_return'],
                'RL Return': evaluation['rl']['annual_return'],
                'Meta Drawdown': evaluation['meta']['max_drawdown'],
                'RL Drawdown': evaluation['rl']['max_drawdown']
            })
        
        # Save comparison results
        comparison_df = pd.DataFrame(comparison_data)
        os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
        comparison_df.to_csv(os.path.join(output_dir, 'results', 'meta_comparison.csv'), index=False)
        
        # Plot comparison
        plt.figure(figsize=(12, 8))
        
        # Plot Sharpe ratios
        plt.subplot(3, 1, 1)
        plt.bar(comparison_df['Config'] + ' Meta', comparison_df['Meta Sharpe'], color='green', alpha=0.7)
        plt.bar(comparison_df['Config'] + ' RL', comparison_df['RL Sharpe'], color='red', alpha=0.7)
        plt.title('Sharpe Ratio Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Plot annual returns
        plt.subplot(3, 1, 2)
        plt.bar(comparison_df['Config'] + ' Meta', comparison_df['Meta Return'], color='green', alpha=0.7)
        plt.bar(comparison_df['Config'] + ' RL', comparison_df['RL Return'], color='red', alpha=0.7)
        plt.title('Annual Return Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Plot maximum drawdowns
        plt.subplot(3, 1, 3)
        plt.bar(comparison_df['Config'] + ' Meta', comparison_df['Meta Drawdown'], color='green', alpha=0.7)
        plt.bar(comparison_df['Config'] + ' RL', comparison_df['RL Drawdown'], color='red', alpha=0.7)
        plt.title('Maximum Drawdown Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'results', 'meta_comparison.png'))
    
    logger.info("Meta-labeling comparison experiment completed")
    
    return results


def run_position_sizing_comparison_experiment(data_path: str, output_dir: str = 'results'):
    """
    Compare different position sizing models.
    
    Args:
        data_path (str): Path to data file
        output_dir (str): Directory to save results
    """
    logger.info("Running position sizing comparison experiment")
    
    # Define position sizing models to compare
    models = ['base_rl', 'dqn', 'ppo', 'sac', 'dreamer']
    
    results = {}
    
    for model in models:
        logger.info(f"Testing position sizing model: {model}")
        
        # Initialize framework
        framework = DLMetaLabelingFramework()
        
        # Override data path and position sizing model
        framework.config['data']['filepath'] = data_path
        framework.config['position_sizing']['model'] = model
        
        # Run pipeline
        try:
            final_data = framework.run_pipeline()
            
            # Evaluate on test data
            evaluation = framework.evaluate_on_test_data()
            
            # Save models
            os.makedirs(os.path.join(output_dir, 'models', f'position_{model}'), exist_ok=True)
            framework.save_models(os.path.join(output_dir, 'models', f'position_{model}'))
            
            # Store results
            results[model] = {
                'framework': framework,
                'evaluation': evaluation,
                'final_data': final_data
            }
            
            # Save returns series for plotting
            returns_data = pd.DataFrame({
                'DateTime': final_data.index,
                'Base Returns': final_data['strategy_returns'],
                'Meta Returns': final_data['meta_strategy_returns'],
                'RL Returns': final_data['rl_strategy_returns']
            })
            returns_data.to_csv(os.path.join(output_dir, 'results', f'position_{model}_returns.csv'), index=False)
        
        except Exception as e:
            logger.error(f"Error running experiment with model {model}: {e}")
    
    # Compare results
    if results:
        # Prepare data for comparison
        comparison_data = []
        
        for model, result in results.items():
            evaluation = result['evaluation']
            comparison_data.append({
                'Model': model,
                'RL Sharpe': evaluation['rl']['sharpe_ratio'],
                'RL Return': evaluation['rl']['annual_return'],
                'RL Drawdown': evaluation['rl']['max_drawdown'],
                'RL Win Rate': evaluation['rl']['win_rate'],
                'RL Profit Factor': evaluation['rl']['profit_factor']
            })
        
        # Save comparison results
        comparison_df = pd.DataFrame(comparison_data)
        os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
        comparison_df.to_csv(os.path.join(output_dir, 'results', 'position_comparison.csv'), index=False)
        
        # Plot comparison
        plt.figure(figsize=(15, 10))
        
        # Plot Sharpe ratios
        plt.subplot(3, 2, 1)
        sns.barplot(x='Model', y='RL Sharpe', data=comparison_df)
        plt.title('Sharpe Ratio Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Plot annual returns
        plt.subplot(3, 2, 2)
        sns.barplot(x='Model', y='RL Return', data=comparison_df)
        plt.title('Annual Return Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Plot maximum drawdowns
        plt.subplot(3, 2, 3)
        sns.barplot(x='Model', y='RL Drawdown', data=comparison_df)
        plt.title('Maximum Drawdown Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Plot win rates
        plt.subplot(3, 2, 4)
        sns.barplot(x='Model', y='RL Win Rate', data=comparison_df)
        plt.title('Win Rate Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Plot profit factors
        plt.subplot(3, 2, 5)
        sns.barplot(x='Model', y='RL Profit Factor', data=comparison_df)
        plt.title('Profit Factor Comparison')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'results', 'position_comparison.png'))
        
        # Plot cumulative returns for each model
        plt.figure(figsize=(15, 10))
        
        for model, result in results.items():
            final_data = result['final_data']
            plt.plot((1 + final_data['rl_strategy_returns']).cumprod() - 1, label=model)
        
        plt.title('Cumulative Returns Comparison - Position Sizing Models')
        plt.xlabel('Time')
        plt.ylabel('Cumulative Returns')
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(output_dir, 'results', 'position_cumulative_returns.png'))
    
    logger.info("Position sizing comparison experiment completed")
    
    return results


def run_all_experiments(data_path: str, output_dir: str = 'results'):
    """
    Run all experiments.
    
    Args:
        data_path (str): Path to data file
        output_dir (str): Directory to save results
    """
    logger.info("Running all experiments")
    
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, 'results'), exist_ok=True)
    os.makedirs(os.path.join(output_dir, 'models'), exist_ok=True)
    
    # Run baseline experiment
    baseline_framework, baseline_evaluation = run_baseline_experiment(data_path, output_dir)
    
    # Run regime comparison experiment
    regime_results = run_regime_comparison_experiment(data_path, output_dir)
    
    # Run meta-labeling comparison experiment
    meta_results = run_meta_labeling_comparison_experiment(data_path, output_dir)
    
    # Run position sizing comparison experiment
    position_results = run_position_sizing_comparison_experiment(data_path, output_dir)
    
    # Combine all results for final comparison
    final_comparison = []
    
    # Add baseline results
    final_comparison.append({
        'Experiment': 'Baseline',
        'Base Sharpe': baseline_evaluation['base']['sharpe_ratio'],
        'Meta Sharpe': baseline_evaluation['meta']['sharpe_ratio'],
        'RL Sharpe': baseline_evaluation['rl']['sharpe_ratio'],
        'Base Return': baseline_evaluation['base']['annual_return'],
        'Meta Return': baseline_evaluation['meta']['annual_return'],
        'RL Return': baseline_evaluation['rl']['annual_return']
    })
    
    # Add best from each experiment category
    if regime_results:
        best_regime = max(regime_results.items(), key=lambda x: x[1]['evaluation']['rl']['sharpe_ratio'])
        final_comparison.append({
            'Experiment': f'Best Regime ({best_regime[0]})',
            'Base Sharpe': best_regime[1]['evaluation']['base']['sharpe_ratio'],
            'Meta Sharpe': best_regime[1]['evaluation']['meta']['sharpe_ratio'],
            'RL Sharpe': best_regime[1]['evaluation']['rl']['sharpe_ratio'],
            'Base Return': best_regime[1]['evaluation']['base']['annual_return'],
            'Meta Return': best_regime[1]['evaluation']['meta']['annual_return'],
            'RL Return': best_regime[1]['evaluation']['rl']['annual_return']
        })
    
    if meta_results:
        best_meta = max(meta_results.items(), key=lambda x: x[1]['evaluation']['meta']['sharpe_ratio'])
        final_comparison.append({
            'Experiment': f'Best Meta ({best_meta[0]})',
            'Base Sharpe': best_meta[1]['evaluation']['base']['sharpe_ratio'],
            'Meta Sharpe': best_meta[1]['evaluation']['meta']['sharpe_ratio'],
            'RL Sharpe': best_meta[1]['evaluation']['rl']['sharpe_ratio'],
            'Base Return': best_meta[1]['evaluation']['base']['annual_return'],
            'Meta Return': best_meta[1]['evaluation']['meta']['annual_return'],
            'RL Return': best_meta[1]['evaluation']['rl']['annual_return']
        })
    
    if position_results:
        best_position = max(position_results.items(), key=lambda x: x[1]['evaluation']['rl']['sharpe_ratio'])
        final_comparison.append({
            'Experiment': f'Best Position ({best_position[0]})',
            'Base Sharpe': best_position[1]['evaluation']['base']['sharpe_ratio'],
            'Meta Sharpe': best_position[1]['evaluation']['meta']['sharpe_ratio'],
            'RL Sharpe': best_position[1]['evaluation']['rl']['sharpe_ratio'],
            'Base Return': best_position[1]['evaluation']['base']['annual_return'],
            'Meta Return': best_position[1]['evaluation']['meta']['annual_return'],
            'RL Return': best_position[1]['evaluation']['rl']['annual_return']
        })
    
    # Save final comparison
    pd.DataFrame(final_comparison).to_csv(os.path.join(output_dir, 'results', 'final_comparison.csv'), index=False)
    
    # Plot final comparison
    plt.figure(figsize=(12, 8))
    
    # Plot Sharpe ratios
    plt.subplot(2, 1, 1)
    final_df = pd.DataFrame(final_comparison)
    plt.bar(final_df['Experiment'] + ' Base', final_df['Base Sharpe'], color='blue', alpha=0.7)
    plt.bar(final_df['Experiment'] + ' Meta', final_df['Meta Sharpe'], color='green', alpha=0.7)
    plt.bar(final_df['Experiment'] + ' RL', final_df['RL Sharpe'], color='red', alpha=0.7)
    plt.title('Sharpe Ratio Comparison - Best Configurations')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # Plot annual returns
    plt.subplot(2, 1, 2)
    plt.bar(final_df['Experiment'] + ' Base', final_df['Base Return'], color='blue', alpha=0.7)
    plt.bar(final_df['Experiment'] + ' Meta', final_df['Meta Return'], color='green', alpha=0.7)
    plt.bar(final_df['Experiment'] + ' RL', final_df['RL Return'], color='red', alpha=0.7)
    plt.title('Annual Return Comparison - Best Configurations')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'results', 'final_comparison.png'))
    
    logger.info("All experiments completed")
    
    return {
        'baseline': (baseline_framework, baseline_evaluation),
        'regime': regime_results,
        'meta': meta_results,
        'position': position_results
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run experiments with DL MetaLabeling Framework')
    parser.add_argument('--data_path', type=str, default='data/cmma.csv', help='Path to data file')
    parser.add_argument('--output_dir', type=str, default='results', help='Directory to save results')
    parser.add_argument('--experiment', type=str, default='all', 
                        choices=['all', 'baseline', 'regime', 'meta', 'position'],
                        help='Which experiment to run')
    
    args = parser.parse_args()
    
    if args.experiment == 'all':
        run_all_experiments(args.data_path, args.output_dir)
    elif args.experiment == 'baseline':
        run_baseline_experiment(args.data_path, args.output_dir)
    elif args.experiment == 'regime':
        run_regime_comparison_experiment(args.data_path, args.output_dir)
    elif args.experiment == 'meta':
        run_meta_labeling_comparison_experiment(args.data_path, args.output_dir)
    elif args.experiment == 'position':
        run_position_sizing_comparison_experiment(args.data_path, args.output_dir)

================
File: sac_position_sizing.py
================
"""
SAC-based Position Sizing model for dynamic trading position sizing.

This module implements a Soft Actor-Critic (SAC) algorithm for 
dynamically adjusting position sizes in trading strategies based on market regimes
and meta-labeling signals with entropy maximization for better exploration.
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
from collections import deque, namedtuple
import matplotlib.pyplot as plt
import time
import pickle
import random

# Set seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Define device for PyTorch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Experience Replay Buffer
class ReplayBuffer:
    """Experience replay buffer to store and sample transitions."""
    
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
        
    def push(self, state, action, reward, next_state, done):
        """Add a new experience to the buffer."""
        self.buffer.append((state, action, reward, next_state, done))
        
    def sample(self, batch_size):
        """Sample a batch of experiences."""
        if batch_size > len(self.buffer):
            batch_size = len(self.buffer)
            
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))
        
        # Convert to tensors
        states = torch.FloatTensor(states).to(device)
        actions = torch.FloatTensor(actions).to(device)
        rewards = torch.FloatTensor(rewards).reshape(-1, 1).to(device)
        next_states = torch.FloatTensor(next_states).to(device)
        dones = torch.FloatTensor(dones).reshape(-1, 1).to(device)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

# SAC Actor Network (Gaussian Policy)
class SACPolicy(nn.Module):
    """SAC policy network that outputs a Gaussian distribution."""
    
    def __init__(self, state_dim, action_dim, hidden_dim=128, 
                log_std_min=-20, log_std_max=2, action_scale=1.0):
        super(SACPolicy, self).__init__()
        
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.action_scale = action_scale
        
        # Shared network layers
        self.linear1 = nn.Linear(state_dim, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, hidden_dim)
        
        # Mean and log std output layers
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize neural network weights."""
        nn.init.xavier_uniform_(self.linear1.weight)
        nn.init.xavier_uniform_(self.linear2.weight)
        nn.init.xavier_uniform_(self.mean.weight)
        nn.init.xavier_uniform_(self.log_std.weight)
        
        # Initialize bias to small values
        nn.init.constant_(self.linear1.bias, 0.0)
        nn.init.constant_(self.linear2.bias, 0.0)
        nn.init.constant_(self.mean.bias, 0.0)
        nn.init.constant_(self.log_std.bias, 0.0)
        
    def forward(self, state):
        """Forward pass through the network."""
        x = F.relu(self.linear1(state))
        x = F.relu(self.linear2(x))
        
        mean = self.mean(x)
        log_std = self.log_std(x)
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        
        return mean, log_std
    
    def sample(self, state, deterministic=False):
        """Sample an action from the policy distribution."""
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        # If deterministic, return mean directly
        if deterministic:
            action = mean
            log_prob = None
        else:
            # Use reparameterization trick to sample from Gaussian
            normal = torch.distributions.Normal(mean, std)
            x_t = normal.rsample()  # reparameterized sample
            
            # Enforce action bounds with tanh and calculate log_prob
            y_t = torch.tanh(x_t)
            action = y_t * self.action_scale
            
            # Calculate log probability, accounting for tanh squashing
            log_prob = normal.log_prob(x_t) - torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
            log_prob = log_prob.sum(1, keepdim=True)
        
        return action, log_prob

# SAC Q-Network
class QNetwork(nn.Module):
    """Q-Network to estimate the Q-value for state-action pairs."""
    
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(QNetwork, self).__init__()
        
        # Q1 network
        self.q1 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Q2 network (for reducing overestimation bias)
        self.q2 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        """Initialize neural network weights."""
        # Q1 weights
        for i in range(0, len(self.q1), 2):  # Only initialize Linear layers
            if isinstance(self.q1[i], nn.Linear):
                nn.init.xavier_uniform_(self.q1[i].weight)
                nn.init.constant_(self.q1[i].bias, 0.0)
        
        # Q2 weights
        for i in range(0, len(self.q2), 2):  # Only initialize Linear layers
            if isinstance(self.q2[i], nn.Linear):
                nn.init.xavier_uniform_(self.q2[i].weight)
                nn.init.constant_(self.q2[i].bias, 0.0)
        
    def forward(self, state, action):
        """Forward pass to calculate Q-values."""
        # Concatenate state and action
        x = torch.cat([state, action], dim=1)
        
        # Calculate Q1 and Q2 values
        q1 = self.q1(x)
        q2 = self.q2(x)
        
        return q1, q2
    
    def q1_forward(self, state, action):
        """Get only Q1 value (used for policy optimization)."""
        x = torch.cat([state, action], dim=1)
        return self.q1(x)

# SAC Position Sizer
class SACPositionSizer:
    """Position sizing using Soft Actor-Critic algorithm with entropy regularization."""
    
    def __init__(self, state_dim, action_dim=1, hidden_dim=128, replay_buffer_size=100000,
                 batch_size=256, gamma=0.99, tau=0.005, alpha=0.2, lr=3e-4, 
                 action_scale=1.0, smoothing_alpha=0.2, transaction_cost=0.000002,
                 target_entropy=None, auto_entropy_tuning=True):
        """
        Initialize SAC Position Sizer.
        
        Args:
            state_dim: Dimension of state space
            action_dim: Dimension of action space (1 for position sizing)
            hidden_dim: Hidden layer size
            replay_buffer_size: Capacity of experience replay buffer
            batch_size: Batch size for training
            gamma: Discount factor
            tau: Soft update coefficient
            alpha: Entropy regularization coefficient (or initial value if auto-tuned)
            lr: Learning rate
            action_scale: Maximum action value 
            smoothing_alpha: EMA smoothing factor for position sizing
            transaction_cost: Trading transaction cost
            target_entropy: Target entropy value for auto-tuning (if None, set to -action_dim)
            auto_entropy_tuning: Whether to automatically adjust entropy coefficient
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.batch_size = batch_size
        self.gamma = gamma
        self.tau = tau
        self.action_scale = action_scale
        self.smoothing_alpha = smoothing_alpha
        self.transaction_cost = transaction_cost
        self.auto_entropy_tuning = auto_entropy_tuning
        
        # Initialize networks
        self.policy = SACPolicy(state_dim, action_dim, hidden_dim, action_scale=action_scale).to(device)
        self.q_net = QNetwork(state_dim, action_dim, hidden_dim).to(device)
        self.target_q_net = QNetwork(state_dim, action_dim, hidden_dim).to(device)
        
        # Copy parameters from Q-network to target Q-network
        for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):
            target_param.data.copy_(param.data)
            
        # Initialize optimizers
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.q_optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        
        # Initialize replay buffer
        self.replay_buffer = ReplayBuffer(capacity=replay_buffer_size)
        
        # Entropy related parameters
        if self.auto_entropy_tuning:
            self.target_entropy = -np.prod(action_dim) if target_entropy is None else target_entropy
            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
            self.alpha = self.log_alpha.exp().item()
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)
        else:
            self.alpha = alpha
            
        # For tracking performance
        self.training_performance = {
            'episode': [],
            'final_reward': [],
            'cumulative_return': [],
            'transaction_costs': [],
            'sharpe_ratio': []
        }
        
        # Initialize step counters
        self.total_steps = 0
        self.updates = 0
        
    def update_parameters(self):
        """Update network parameters using SAC algorithm."""
        # Skip update if buffer doesn't have enough samples
        if len(self.replay_buffer) < self.batch_size:
            return
            
        # Sample a batch from the replay buffer
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
        
        # Update Q-networks
        with torch.no_grad():
            # Sample next actions and log probs from current policy
            next_actions, next_log_probs = self.policy.sample(next_states)
            
            # Get target Q-values
            next_q1, next_q2 = self.target_q_net(next_states, next_actions)
            next_q = torch.min(next_q1, next_q2)
            
            # Calculate target values with entropy regularization
            target_q = rewards + self.gamma * (1 - dones) * (next_q - self.alpha * next_log_probs)
        
        # Current Q-values
        current_q1, current_q2 = self.q_net(states, actions)
        
        # Calculate Q-network loss (MSE)
        q1_loss = F.mse_loss(current_q1, target_q)
        q2_loss = F.mse_loss(current_q2, target_q)
        q_loss = q1_loss + q2_loss
        
        # Update Q-networks
        self.q_optimizer.zero_grad()
        q_loss.backward()
        self.q_optimizer.step()
        
        # Update policy network
        new_actions, log_probs = self.policy.sample(states)
        q_values = self.q_net.q1_forward(states, new_actions)
        
        # Policy loss = E[α * log π(a|s) - Q(s,a)]
        policy_loss = (self.alpha * log_probs - q_values).mean()
        
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()
        
        # Update alpha (entropy coefficient) if auto-tuning
        if self.auto_entropy_tuning:
            # Calculate alpha loss
            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
            
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
            
            # Update alpha value
            self.alpha = self.log_alpha.exp().item()
            
        # Soft update target Q-network
        for target_param, param in zip(self.target_q_net.parameters(), self.q_net.parameters()):
            target_param.data.copy_(
                target_param.data * (1.0 - self.tau) + param.data * self.tau
            )
            
        # Increment update counter
        self.updates += 1
    
    def prepare_state(self, data, idx, meta_col, regime_col, features_cols, current_position):
        """Prepare state representation for the model."""
        # Get meta signal
        meta_signal = data.iloc[idx][meta_col]
        
        # Get regime and one-hot encode it
        regime = data.iloc[idx][regime_col]
        regime_one_hot = np.zeros(3)  # Assuming 3 regimes: Bull, Bear, Sideways
        regime_one_hot[int(regime)] = 1
        
        # Get features
        features = data.iloc[idx][features_cols].values
        
        # Combine all into state vector
        state = np.hstack([
            np.array([meta_signal]),
            np.array([current_position]),
            regime_one_hot,
            features
        ])
        
        return state
    
    def calculate_reward(self, data, idx, action, prev_action, returns_col):
        """Calculate reward for the action taken."""
        # Get return for this time step
        returns = data.iloc[idx][returns_col]
        
        # Calculate profit/loss based on the action (position size)
        profit = returns * action
        
        # Calculate transaction cost
        size_change = abs(action - prev_action)
        transaction_cost = size_change * self.transaction_cost
        
        # Final reward is profit minus transaction cost
        reward = profit - transaction_cost
        
        return reward, profit, transaction_cost
    
    def train(self, data, meta_col, regime_col, features_cols, returns_col, episodes=50, 
             base_size=1.0, eval_interval=5, updates_per_step=1):
        """
        Train the SAC model on the given data.
        
        Args:
            data: DataFrame with features and returns
            meta_col: Column name for meta-labeling signal
            regime_col: Column name for market regime
            features_cols: List of feature column names
            returns_col: Column name for returns
            episodes: Number of episodes to train
            base_size: Base position size (maximum)
            eval_interval: Interval to evaluate and save model
            updates_per_step: Number of parameter updates per step
        
        Returns:
            Dict with training performance metrics
        """
        best_reward = -np.inf
        
        for episode in range(1, episodes + 1):
            # Reset tracking variables
            total_reward = 0
            cumulative_return = 0
            total_transaction_cost = 0
            returns_history = []
            
            # Start with zero position
            current_position = 0
            
            # Loop through the entire dataset
            for idx in range(len(data)):
                # Prepare state
                state = self.prepare_state(data, idx, meta_col, regime_col, features_cols, current_position)
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                
                # Sample action from policy
                with torch.no_grad():
                    action_tensor, _ = self.policy.sample(state_tensor, deterministic=False)
                action = action_tensor.cpu().numpy()[0][0]
                
                # Apply EMA smoothing to avoid large position changes
                smooth_action = current_position + self.smoothing_alpha * (action - current_position)
                
                # Calculate reward
                reward, profit, transaction_cost = self.calculate_reward(
                    data, idx, smooth_action, current_position, returns_col
                )
                
                # Prepare next state
                next_idx = min(idx + 1, len(data) - 1)
                next_state = self.prepare_state(data, next_idx, meta_col, regime_col, 
                                               features_cols, smooth_action)
                
                # Determine if terminal state
                done = (next_idx == len(data) - 1)
                
                # Store transition in replay buffer
                self.replay_buffer.push(state, np.array([smooth_action]), reward, next_state, done)
                
                # Update networks
                for _ in range(updates_per_step):
                    self.update_parameters()
                
                # Update tracking variables
                total_reward += reward
                cumulative_return += profit
                total_transaction_cost += transaction_cost
                returns_history.append(profit)
                
                # Update current position for next step
                current_position = smooth_action
                
                # Increment step counter
                self.total_steps += 1
            
            # Calculate Sharpe ratio
            if len(returns_history) > 1:
                sharpe = np.mean(returns_history) / (np.std(returns_history) + 1e-8) * np.sqrt(252)
            else:
                sharpe = 0
            
            # Record performance
            self.training_performance['episode'].append(episode)
            self.training_performance['final_reward'].append(total_reward)
            self.training_performance['cumulative_return'].append(cumulative_return)
            self.training_performance['transaction_costs'].append(total_transaction_cost)
            self.training_performance['sharpe_ratio'].append(sharpe)
            
            # Print progress
            if episode % eval_interval == 0 or episode == 1:
                print(f"Episode {episode}/{episodes} - "
                      f"Reward: {total_reward:.4f}, "
                      f"Return: {cumulative_return:.4f}, "
                      f"Costs: {total_transaction_cost:.4f}, "
                      f"Sharpe: {sharpe:.4f}, "
                      f"Alpha: {self.alpha:.4f}")
                
                # Save best model
                if total_reward > best_reward:
                    best_reward = total_reward
                    self.save('models/sac_position_sizer_best')
        
        return self.training_performance
    
    def apply(self, data, meta_col, regime_col, features_cols, returns_col, base_size=1.0):
        """
        Apply the trained model to new data.
        
        Args:
            data: DataFrame with features and returns
            meta_col: Column name for meta-labeling signal
            regime_col: Column name for market regime
            features_cols: List of feature column names
            returns_col: Column name for returns
            base_size: Base position size (maximum)
        
        Returns:
            DataFrame with original data plus model's position sizes
        """
        # Make a copy to avoid modifying the original
        result_df = data.copy()
        
        # Add new column for position sizes
        result_df['sac_position_size'] = np.zeros(len(result_df))
        
        # Start with zero position
        current_position = 0
        
        # Loop through the data
        for idx in range(len(data)):
            # Prepare state
            state = self.prepare_state(data, idx, meta_col, regime_col, features_cols, current_position)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            
            # Get action (position size) using deterministic policy for evaluation
            with torch.no_grad():
                action_tensor, _ = self.policy.sample(state_tensor, deterministic=True)
            action = action_tensor.cpu().numpy()[0][0]
            
            # Apply EMA smoothing
            smooth_action = current_position + self.smoothing_alpha * (action - current_position)
            
            # Store in result DataFrame
            result_df.iloc[idx, result_df.columns.get_loc('sac_position_size')] = smooth_action
            
            # Update current position for next step
            current_position = smooth_action
        
        return result_df
    
    def save(self, path):
        """Save the model to disk."""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        model_data = {
            'policy_state_dict': self.policy.state_dict(),
            'q_net_state_dict': self.q_net.state_dict(),
            'target_q_net_state_dict': self.target_q_net.state_dict(),
            'hyperparams': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'hidden_dim': self.hidden_dim,
                'batch_size': self.batch_size,
                'gamma': self.gamma,
                'tau': self.tau,
                'action_scale': self.action_scale,
                'smoothing_alpha': self.smoothing_alpha,
                'transaction_cost': self.transaction_cost,
                'auto_entropy_tuning': self.auto_entropy_tuning
            },
            'training_performance': self.training_performance
        }
        
        if self.auto_entropy_tuning:
            model_data['log_alpha'] = self.log_alpha.item()
            model_data['target_entropy'] = self.target_entropy
        else:
            model_data['alpha'] = self.alpha
        
        torch.save(model_data, path)
        print(f"Model saved to {path}")
    
    @classmethod
    def load(cls, path):
        """Load the model from disk."""
        model_data = torch.load(path)
        
        # Determine if using auto entropy tuning
        auto_entropy_tuning = model_data['hyperparams']['auto_entropy_tuning']
        
        # Create hyperparameters dict
        hyperparams = model_data['hyperparams'].copy()
        
        if auto_entropy_tuning:
            hyperparams['target_entropy'] = model_data['target_entropy']
        else:
            hyperparams['alpha'] = model_data['alpha']
        
        # Create instance with saved hyperparameters
        instance = cls(**hyperparams)
        
        # Load network weights
        instance.policy.load_state_dict(model_data['policy_state_dict'])
        instance.q_net.load_state_dict(model_data['q_net_state_dict'])
        instance.target_q_net.load_state_dict(model_data['target_q_net_state_dict'])
        
        # Load alpha for auto entropy tuning
        if auto_entropy_tuning:
            with torch.no_grad():
                instance.log_alpha.copy_(torch.tensor([model_data['log_alpha']], device=device))
            instance.alpha = instance.log_alpha.exp().item()
        
        # Load training performance if available
        if 'training_performance' in model_data:
            instance.training_performance = model_data['training_performance']
        
        return instance

def evaluate_sac_strategy(data, base_position_col, sac_position_col, returns_col, transaction_cost=0.000002):
    """
    Evaluate the SAC-based position sizing strategy against the base strategy.
    
    Args:
        data: DataFrame with both base and SAC position sizes
        base_position_col: Column name for base position signals
        sac_position_col: Column name for SAC position sizes
        returns_col: Column name for returns
        transaction_cost: Transaction cost per unit change
    
    Returns:
        Tuple of (metrics_dict, evaluated_data_df)
    """
    # Make a copy to avoid modifying the original
    result = data.copy()
    
    # Calculate base strategy returns and transaction costs
    base_position_shifted = result[base_position_col].shift(1).fillna(0)  # Shift to avoid lookahead bias
    base_returns = base_position_shifted * result[returns_col]
    base_tc = np.abs(result[base_position_col].diff()).fillna(0) * transaction_cost
    
    # Calculate SAC strategy returns and transaction costs
    sac_position_shifted = result[sac_position_col].shift(1).fillna(0)  # Shift to avoid lookahead bias
    sac_returns = sac_position_shifted * result[returns_col]
    sac_tc = np.abs(result[sac_position_col].diff()).fillna(0) * transaction_cost
    
    # Store in result DataFrame
    result['base_returns'] = base_returns
    result['base_tc'] = base_tc
    result['base_net_returns'] = base_returns - base_tc
    
    result['sac_returns'] = sac_returns
    result['sac_tc'] = sac_tc
    result['sac_net_returns'] = sac_returns - sac_tc
    
    # Calculate metrics
    metrics = {}
    
    # Base strategy metrics
    metrics['base_total_return'] = result['base_returns'].sum()
    metrics['base_transaction_costs'] = result['base_tc'].sum()
    metrics['base_net_return'] = metrics['base_total_return'] - metrics['base_transaction_costs']
    metrics['base_sharpe'] = (result['base_net_returns'].mean() / result['base_net_returns'].std() * np.sqrt(252)
                             if result['base_net_returns'].std() > 0 else 0)
    
    # Calculate drawdown for base strategy
    cum_returns = (1 + result['base_net_returns']).cumprod()
    running_max = cum_returns.cummax()
    drawdown = (cum_returns / running_max - 1)
    metrics['base_max_drawdown'] = drawdown.min()
    
    # Win rate for base strategy
    metrics['base_win_rate'] = (result['base_net_returns'] > 0).mean()
    
    # Average position size and trade frequency for base strategy
    metrics['base_avg_position_size'] = result[base_position_col].abs().mean()
    metrics['base_trade_frequency'] = (result[base_position_col].diff() != 0).mean()
    
    # SAC strategy metrics
    metrics['sac_total_return'] = result['sac_returns'].sum()
    metrics['sac_transaction_costs'] = result['sac_tc'].sum()
    metrics['sac_net_return'] = metrics['sac_total_return'] - metrics['sac_transaction_costs']
    metrics['sac_sharpe'] = (result['sac_net_returns'].mean() / result['sac_net_returns'].std() * np.sqrt(252)
                             if result['sac_net_returns'].std() > 0 else 0)
    
    # Calculate drawdown for SAC strategy
    cum_returns = (1 + result['sac_net_returns']).cumprod()
    running_max = cum_returns.cummax()
    drawdown = (cum_returns / running_max - 1)
    metrics['sac_max_drawdown'] = drawdown.min()
    
    # Win rate for SAC strategy
    metrics['sac_win_rate'] = (result['sac_net_returns'] > 0).mean()
    
    # Average position size and trade frequency for SAC strategy
    metrics['sac_avg_position_size'] = result[sac_position_col].abs().mean()
    metrics['sac_trade_frequency'] = (result[sac_position_col].diff() != 0).mean()
    
    # Plot cumulative returns
    plt.figure(figsize=(12, 6))
    
    # Calculate cumulative returns for both strategies
    base_cum_returns = (1 + result['base_net_returns']).cumprod() - 1
    sac_cum_returns = (1 + result['sac_net_returns']).cumprod() - 1
    
    plt.plot(base_cum_returns.index, base_cum_returns, label='Base Strategy')
    plt.plot(sac_cum_returns.index, sac_cum_returns, label='SAC Strategy')
    
    plt.title('Cumulative Returns Comparison')
    plt.xlabel('Date')
    plt.ylabel('Cumulative Return')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('sac_vs_base_returns.png')
    
    # Plot position sizes comparison
    plt.figure(figsize=(12, 6))
    
    plt.plot(result.index, result[base_position_col], label='Base Position', alpha=0.7)
    plt.plot(result.index, result[sac_position_col], label='SAC Position', alpha=0.7)
    
    plt.title('Position Sizes Comparison')
    plt.xlabel('Date')
    plt.ylabel('Position Size')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('sac_vs_base_positions.png')
    
    # Print metrics
    print("\nPerformance Metrics:")
    print(f"Base Strategy - Net Return: {metrics['base_net_return']:.4f}, "
          f"Sharpe: {metrics['base_sharpe']:.4f}, "
          f"Max DD: {metrics['base_max_drawdown']:.4f}")
    print(f"SAC Strategy - Net Return: {metrics['sac_net_return']:.4f}, "
          f"Sharpe: {metrics['sac_sharpe']:.4f}, "
          f"Max DD: {metrics['sac_max_drawdown']:.4f}")
    
    return metrics, result

================
File: utils/__init__.py
================
"""
Utilities module for the DL MetaLabeling package.

This module provides various utility functions and tools for data processing,
visualization, model evaluation, and other common tasks.
"""

from .data_processing import (
    load_data,
    preprocess_data,
    add_returns,
    add_technical_indicators,
    add_volatility_features,
    add_target_signal,
    prepare_training_data,
    generate_cv_time_series_splits,
    calculate_performance_metrics
)

from .visualization import (
    set_plotting_style,
    plot_returns,
    plot_regime_distribution,
    plot_signal_distribution,
    plot_performance_metrics,
    plot_confusion_matrix,
    plot_feature_importance,
    plot_learning_curves,
    plot_regime_performance
)

# Define exported components
__all__ = [
    # Data processing functions
    'load_data',
    'preprocess_data',
    'add_returns',
    'add_technical_indicators',
    'add_volatility_features',
    'add_target_signal',
    'prepare_training_data',
    'generate_cv_time_series_splits',
    'calculate_performance_metrics',
    
    # Visualization functions
    'set_plotting_style',
    'plot_returns',
    'plot_regime_distribution',
    'plot_signal_distribution',
    'plot_performance_metrics',
    'plot_confusion_matrix',
    'plot_feature_importance',
    'plot_learning_curves',
    'plot_regime_performance'
]

================
File: utils/data_processing.py
================
"""
Data processing utilities for the DL MetaLabeling package.

This module provides functions for data loading, preprocessing, feature engineering,
and other data processing tasks commonly used in the framework.
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Union, Optional, Any, Tuple
import ta
import os

logger = logging.getLogger(__name__)

def load_data(file_path: str, date_column: str = None) -> pd.DataFrame:
    """
    Load data from CSV or other supported formats.
    
    Args:
        file_path: Path to the data file
        date_column: Column to parse as dates
        
    Returns:
        DataFrame with loaded data
    """
    logger.info(f"Loading data from {file_path}")
    
    try:
        # Determine file extension
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == '.csv':
            # Load CSV file
            if date_column:
                df = pd.read_csv(file_path, parse_dates=[date_column])
                # Set date column as index if provided
                df.set_index(date_column, inplace=True)
            else:
                df = pd.read_csv(file_path)
        elif ext in ['.pkl', '.pickle']:
            # Load pickle file
            df = pd.read_pickle(file_path)
        elif ext == '.parquet':
            # Load parquet file
            df = pd.read_parquet(file_path)
        else:
            logger.error(f"Unsupported file format: {ext}")
            return pd.DataFrame()
        
        logger.info(f"Loaded data with shape {df.shape}")
        return df
    
    except Exception as e:
        logger.error(f"Error loading data: {str(e)}")
        return pd.DataFrame()

def preprocess_data(data: pd.DataFrame, 
                   fillna_method: str = 'ffill', 
                   dropna: bool = True) -> pd.DataFrame:
    """
    Preprocess data by handling missing values and data types.
    
    Args:
        data: Input DataFrame
        fillna_method: Method for filling NaN values ('ffill', 'bfill', 'mean', None)
        dropna: Whether to drop remaining NaN values
        
    Returns:
        Preprocessed DataFrame
    """
    logger.info("Preprocessing data")
    
    # Make a copy to avoid modifying the input
    df = data.copy()
    
    # Handle missing values
    if fillna_method:
        if fillna_method == 'ffill':
            logger.info("Filling missing values with forward fill")
            df = df.fillna(method='ffill')
        elif fillna_method == 'bfill':
            logger.info("Filling missing values with backward fill")
            df = df.fillna(method='bfill')
        elif fillna_method == 'mean':
            logger.info("Filling missing values with column means")
            df = df.fillna(df.mean())
        else:
            logger.warning(f"Unknown fillna method: {fillna_method}")
    
    # Drop remaining NaN values if requested
    if dropna:
        logger.info("Dropping rows with missing values")
        original_len = len(df)
        df = df.dropna()
        dropped = original_len - len(df)
        if dropped > 0:
            logger.info(f"Dropped {dropped} rows with missing values ({dropped/original_len:.2%})")
    
    return df

def add_returns(data: pd.DataFrame, 
               price_col: str = 'close',
               periods: List[int] = [1],
               log_returns: bool = False) -> pd.DataFrame:
    """
    Add return columns to the DataFrame.
    
    Args:
        data: Input DataFrame
        price_col: Column name for price data
        periods: List of periods for calculating returns
        log_returns: Whether to calculate log returns
        
    Returns:
        DataFrame with added return columns
    """
    logger.info(f"Adding returns with periods {periods}")
    
    # Make a copy to avoid modifying the input
    df = data.copy()
    
    # Check if price column exists
    if price_col not in df.columns:
        logger.error(f"Price column '{price_col}' not found in data")
        return df
    
    # Calculate returns for each period
    for period in periods:
        col_name = f'returns_{period}' if period > 1 else 'returns'
        
        # Calculate returns
        if log_returns:
            df[col_name] = np.log(df[price_col] / df[price_col].shift(period))
        else:
            df[col_name] = df[price_col].pct_change(period)
        
        logger.info(f"Added {col_name} column")
    
    return df

def add_technical_indicators(data: pd.DataFrame,
                            indicators: List[str] = None,
                            window: int = 14) -> pd.DataFrame:
    """
    Add technical indicators to the DataFrame.
    
    Args:
        data: Input DataFrame with OHLCV data
        indicators: List of indicators to add (None for all common indicators)
        window: Window size for indicators
        
    Returns:
        DataFrame with added technical indicators
    """
    logger.info(f"Adding technical indicators with window {window}")
    
    # Make a copy to avoid modifying the input
    df = data.copy()
    
    # Default indicators if none specified
    all_indicators = [
        'sma', 'ema', 'rsi', 'macd', 'bbands', 'atr', 'adx'
    ]
    
    indicators = indicators or all_indicators
    
    # Check for required columns
    required_cols = {
        'sma': ['close'],
        'ema': ['close'],
        'rsi': ['close'],
        'macd': ['close'],
        'bbands': ['close'],
        'atr': ['high', 'low', 'close'],
        'adx': ['high', 'low', 'close'],
    }
    
    for indicator in indicators:
        cols = required_cols.get(indicator, [])
        if not all(col in df.columns for col in cols):
            logger.warning(f"Missing required columns for {indicator}: {cols}")
            continue
        
        try:
            # Add the indicator
            if indicator == 'sma':
                df[f'sma_{window}'] = ta.trend.sma_indicator(df['close'], window=window)
            elif indicator == 'ema':
                df[f'ema_{window}'] = ta.trend.ema_indicator(df['close'], window=window)
            elif indicator == 'rsi':
                df[f'rsi_{window}'] = ta.momentum.rsi(df['close'], window=window)
            elif indicator == 'macd':
                macd = ta.trend.MACD(df['close'], window_fast=12, window_slow=26, window_sign=9)
                df['macd'] = macd.macd()
                df['macd_signal'] = macd.macd_signal()
                df['macd_diff'] = macd.macd_diff()
            elif indicator == 'bbands':
                bbands = ta.volatility.BollingerBands(df['close'], window=window)
                df['bb_high'] = bbands.bollinger_hband()
                df['bb_low'] = bbands.bollinger_lband()
                df['bb_mid'] = bbands.bollinger_mavg()
                df['bb_width'] = bbands.bollinger_wband()
            elif indicator == 'atr':
                df[f'atr_{window}'] = ta.volatility.average_true_range(
                    df['high'], df['low'], df['close'], window=window
                )
            elif indicator == 'adx':
                df[f'adx_{window}'] = ta.trend.adx(
                    df['high'], df['low'], df['close'], window=window
                )
            
            logger.info(f"Added {indicator} indicator")
        
        except Exception as e:
            logger.error(f"Error adding {indicator} indicator: {str(e)}")
    
    return df

def add_volatility_features(data: pd.DataFrame,
                           returns_col: str = 'returns',
                           windows: List[int] = [21, 63, 126]) -> pd.DataFrame:
    """
    Add volatility features to the DataFrame.
    
    Args:
        data: Input DataFrame with returns
        returns_col: Column name for returns
        windows: List of windows for volatility calculation
        
    Returns:
        DataFrame with added volatility features
    """
    logger.info(f"Adding volatility features with windows {windows}")
    
    # Make a copy to avoid modifying the input
    df = data.copy()
    
    # Check if returns column exists
    if returns_col not in df.columns:
        logger.error(f"Returns column '{returns_col}' not found in data")
        return df
    
    # Calculate volatility for each window
    for window in windows:
        col_name = f'volatility_{window}'
        df[col_name] = df[returns_col].rolling(window=window).std() * np.sqrt(252)
        logger.info(f"Added {col_name} column")
    
    return df

def add_target_signal(data: pd.DataFrame,
                     method: str = 'momentum',
                     price_col: str = 'close',
                     returns_col: str = 'returns',
                     signal_col: str = 'position',
                     **kwargs) -> pd.DataFrame:
    """
    Add target signal to the DataFrame.
    
    Args:
        data: Input DataFrame
        method: Method for signal generation ('momentum', 'sma_crossover', etc.)
        price_col: Column name for price data
        returns_col: Column name for returns data
        signal_col: Column name for output signal
        kwargs: Additional parameters for signal generation
        
    Returns:
        DataFrame with added signal column
    """
    logger.info(f"Adding {method} target signal")
    
    # Make a copy to avoid modifying the input
    df = data.copy()
    
    # Generate signal based on method
    if method == 'momentum':
        # Parameters
        lookback = kwargs.get('lookback', 20)
        threshold = kwargs.get('threshold', 0)
        
        # Calculate momentum signal
        df[signal_col] = 0
        df[f'momentum_{lookback}'] = df[price_col].pct_change(lookback)
        df.loc[df[f'momentum_{lookback}'] > threshold, signal_col] = 1
        df.loc[df[f'momentum_{lookback}'] < -threshold, signal_col] = -1
        
        logger.info(f"Added momentum signal with lookback={lookback}, threshold={threshold}")
        
    elif method == 'sma_crossover':
        # Parameters
        fast_period = kwargs.get('fast_period', 20)
        slow_period = kwargs.get('slow_period', 50)
        
        # Calculate SMAs
        df[f'sma_{fast_period}'] = df[price_col].rolling(window=fast_period).mean()
        df[f'sma_{slow_period}'] = df[price_col].rolling(window=slow_period).mean()
        
        # Generate signal
        df[signal_col] = 0
        df.loc[df[f'sma_{fast_period}'] > df[f'sma_{slow_period}'], signal_col] = 1
        df.loc[df[f'sma_{fast_period}'] < df[f'sma_{slow_period}'], signal_col] = -1
        
        logger.info(f"Added SMA crossover signal with fast={fast_period}, slow={slow_period}")
        
    elif method == 'bollinger_bands':
        # Parameters
        window = kwargs.get('window', 20)
        std_dev = kwargs.get('std_dev', 2)
        
        # Calculate Bollinger Bands
        df['bb_mid'] = df[price_col].rolling(window=window).mean()
        df['bb_std'] = df[price_col].rolling(window=window).std()
        df['bb_upper'] = df['bb_mid'] + std_dev * df['bb_std']
        df['bb_lower'] = df['bb_mid'] - std_dev * df['bb_std']
        
        # Generate signal
        df[signal_col] = 0
        df.loc[df[price_col] > df['bb_upper'], signal_col] = -1  # Sell signal (overbought)
        df.loc[df[price_col] < df['bb_lower'], signal_col] = 1   # Buy signal (oversold)
        
        logger.info(f"Added Bollinger Bands signal with window={window}, std_dev={std_dev}")
        
    else:
        logger.error(f"Unknown signal method: {method}")
    
    # Count signals
    long_count = (df[signal_col] == 1).sum()
    short_count = (df[signal_col] == -1).sum()
    neutral_count = (df[signal_col] == 0).sum()
    total_count = len(df)
    
    logger.info(f"Signal distribution: long={long_count}/{total_count} ({long_count/total_count:.2%}), " +
                f"short={short_count}/{total_count} ({short_count/total_count:.2%}), " +
                f"neutral={neutral_count}/{total_count} ({neutral_count/total_count:.2%})")
    
    return df

def prepare_training_data(data: pd.DataFrame,
                         features: List[str] = None,
                         target: str = None,
                         test_size: float = 0.2,
                         shuffle: bool = False,
                         random_state: int = 42) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Prepare training and testing data for machine learning models.
    
    Args:
        data: Input DataFrame
        features: List of feature columns
        target: Target column name
        test_size: Fraction of data to use for testing
        shuffle: Whether to shuffle the data
        random_state: Random seed for reproducibility
        
    Returns:
        Tuple of (X_train, X_test, y_train, y_test)
    """
    from sklearn.model_selection import train_test_split
    
    logger.info(f"Preparing training data with test_size={test_size}")
    
    # Use all numeric columns as features if not specified
    if features is None:
        features = data.select_dtypes(include=[np.number]).columns.tolist()
        if target in features:
            features.remove(target)
        logger.info(f"Using {len(features)} numeric columns as features")
    
    # Check if all feature columns exist
    missing_features = [f for f in features if f not in data.columns]
    if missing_features:
        logger.warning(f"Missing feature columns: {missing_features}")
        features = [f for f in features if f in data.columns]
    
    # Check if target column exists
    if target not in data.columns:
        logger.error(f"Target column '{target}' not found in data")
        return None, None, None, None
    
    # Extract features and target
    X = data[features].values
    y = data[target].values
    
    # Split data
    if shuffle:
        logger.info("Splitting data with shuffling")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, shuffle=True
        )
    else:
        # Time series split (no shuffling)
        logger.info("Splitting data chronologically (without shuffling)")
        split_idx = int(len(X) * (1 - test_size))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
    
    logger.info(f"Training set: {X_train.shape}, Testing set: {X_test.shape}")
    
    return X_train, X_test, y_train, y_test

def generate_cv_time_series_splits(data: pd.DataFrame,
                                 n_splits: int = 5,
                                 test_size: float = 0.2,
                                 gap: int = 0) -> List[Tuple[np.ndarray, np.ndarray]]:
    """
    Generate cross-validation splits for time series data.
    
    Args:
        data: Input DataFrame
        n_splits: Number of splits
        test_size: Fraction of data to use for testing in each split
        gap: Gap between train and test sets
        
    Returns:
        List of (train_idx, test_idx) tuples
    """
    from sklearn.model_selection import TimeSeriesSplit
    
    logger.info(f"Generating {n_splits} time series CV splits with test_size={test_size}")
    
    # Initialize TimeSeriesSplit
    tscv = TimeSeriesSplit(n_splits=n_splits, gap=gap, test_size=int(len(data) * test_size))
    
    # Generate splits
    splits = []
    for i, (train_idx, test_idx) in enumerate(tscv.split(data)):
        logger.info(f"Split {i+1}: train={len(train_idx)}, test={len(test_idx)}")
        splits.append((train_idx, test_idx))
    
    return splits

def calculate_performance_metrics(returns: pd.Series, 
                                risk_free_rate: float = 0.0,
                                annualization_factor: int = 252) -> Dict[str, float]:
    """
    Calculate common performance metrics for a returns series.
    
    Args:
        returns: Series of returns to analyze
        risk_free_rate: Annualized risk-free rate
        annualization_factor: Factor for annualizing returns (252 for daily data)
        
    Returns:
        Dictionary of performance metrics
    """
    logger.info("Calculating performance metrics")
    
    # Ensure returns are a pandas Series
    if not isinstance(returns, pd.Series):
        returns = pd.Series(returns)
    
    # Clean returns
    clean_returns = returns.replace([np.inf, -np.inf], np.nan).dropna()
    
    # Basic metrics
    total_return = (1 + clean_returns).prod() - 1
    annualized_return = (1 + total_return) ** (annualization_factor / len(clean_returns)) - 1
    daily_vol = clean_returns.std()
    annualized_vol = daily_vol * np.sqrt(annualization_factor)
    
    # Sharpe ratio
    daily_risk_free = (1 + risk_free_rate) ** (1 / annualization_factor) - 1
    excess_returns = clean_returns - daily_risk_free
    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(annualization_factor)
    
    # Drawdown analysis
    cumulative = (1 + clean_returns).cumprod()
    peak = cumulative.expanding().max()
    drawdown = (cumulative / peak) - 1
    max_drawdown = drawdown.min()
    
    # Sortino ratio
    negative_returns = clean_returns[clean_returns < 0]
    downside_deviation = negative_returns.std() * np.sqrt(annualization_factor)
    sortino_ratio = 0 if downside_deviation == 0 else annualized_return / downside_deviation
    
    # Calmar ratio
    calmar_ratio = 0 if max_drawdown == 0 else annualized_return / abs(max_drawdown)
    
    # Return metrics dictionary
    metrics = {
        'total_return': total_return,
        'annualized_return': annualized_return,
        'annualized_volatility': annualized_vol,
        'sharpe_ratio': sharpe_ratio,
        'sortino_ratio': sortino_ratio,
        'max_drawdown': max_drawdown,
        'calmar_ratio': calmar_ratio,
        'positive_days': (clean_returns > 0).sum() / len(clean_returns),
        'win_rate': (clean_returns > 0).sum() / len(clean_returns),
    }
    
    logger.info(f"Performance metrics calculated: {metrics}")
    return metrics

================
File: utils/visualization.py
================
"""
Visualization utilities for the DL MetaLabeling package.

This module provides functions for creating visualizations of data, performance metrics,
model results, and other graphics useful for analysis and presentation.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Union, Optional, Any, Tuple
import logging
import os
from matplotlib.ticker import FuncFormatter

logger = logging.getLogger(__name__)

def set_plotting_style():
    """
    Set the default style for all plots.
    """
    # Use ggplot style as a base
    plt.style.use('ggplot')
    
    # Customize the style
    plt.rcParams['figure.figsize'] = (12, 6)
    plt.rcParams['font.size'] = 12
    plt.rcParams['axes.titlesize'] = 14
    plt.rcParams['axes.labelsize'] = 12
    plt.rcParams['xtick.labelsize'] = 10
    plt.rcParams['ytick.labelsize'] = 10
    plt.rcParams['legend.fontsize'] = 10
    plt.rcParams['legend.frameon'] = True
    plt.rcParams['legend.framealpha'] = 0.8
    plt.rcParams['legend.edgecolor'] = 'gray'
    plt.rcParams['figure.autolayout'] = True
    
    # Set colors
    sns.set_palette("deep")
    
    logger.info("Visualization style set")

def plot_returns(returns_data: pd.DataFrame, 
                title: str = 'Cumulative Returns', 
                figsize: Tuple[int, int] = (12, 6),
                save_path: str = None,
                benchmark_col: str = None) -> plt.Figure:
    """
    Plot cumulative returns.
    
    Args:
        returns_data: DataFrame with returns columns
        title: Plot title
        figsize: Figure size (width, height)
        save_path: Path to save the figure
        benchmark_col: Column to use as benchmark
        
    Returns:
        Matplotlib figure
    """
    logger.info(f"Plotting cumulative returns with {len(returns_data)} points")
    
    # Set style
    set_plotting_style()
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Calculate cumulative returns
    cum_returns = (1 + returns_data).cumprod()
    
    # Plot returns
    for col in cum_returns.columns:
        if col == benchmark_col:
            # Plot benchmark with different style
            ax.plot(cum_returns.index, cum_returns[col], 'k--', label=f'{col} (Benchmark)', 
                    linewidth=1.5, alpha=0.7)
        else:
            ax.plot(cum_returns.index, cum_returns[col], label=col, linewidth=1.5)
    
    # Add horizontal line at y=1
    ax.axhline(y=1.0, color='gray', linestyle='-', linewidth=0.5)
    
    # Format y-axis as percentage
    ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.2f}x'))
    
    # Set labels and title
    ax.set_title(title)
    ax.set_xlabel('Date')
    ax.set_ylabel('Cumulative Returns (multiple)')
    
    # Add legend
    ax.legend()
    
    # Add grid
    ax.grid(True, alpha=0.3)
    
    # Rotate x-axis labels if index is datetime
    if isinstance(returns_data.index, pd.DatetimeIndex):
        plt.xticks(rotation=45)
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved figure to {save_path}")
        except Exception as e:
            logger.error(f"Error saving figure: {str(e)}")
    
    return fig

def plot_regime_distribution(data: pd.DataFrame,
                           regime_col: str,
                           date_col: str = None,
                           title: str = 'Regime Distribution',
                           figsize: Tuple[int, int] = (12, 6),
                           save_path: str = None) -> plt.Figure:
    """
    Plot the distribution of market regimes.
    
    Args:
        data: DataFrame with regime column
        regime_col: Column name for regimes
        date_col: Column name for dates
        title: Plot title
        figsize: Figure size (width, height)
        save_path: Path to save the figure
        
    Returns:
        Matplotlib figure
    """
    logger.info(f"Plotting regime distribution with {len(data)} points")
    
    # Set style
    set_plotting_style()
    
    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, height_ratios=[1, 3])
    
    # Plot regime pie chart
    regime_counts = data[regime_col].value_counts()
    ax1.pie(regime_counts, labels=regime_counts.index, autopct='%1.1f%%', startangle=90, 
            wedgeprops={'alpha': 0.7})
    ax1.set_title('Regime Distribution')
    
    # Plot regimes over time if date column is provided
    if date_col and date_col in data.columns:
        # Use date as index
        if date_col != data.index.name:
            plot_data = data.set_index(date_col)
        else:
            plot_data = data
        
        # Create unique colors for each regime
        regimes = plot_data[regime_col].unique()
        colors = sns.color_palette("deep", len(regimes))
        regime_colors = dict(zip(regimes, colors))
        
        # Plot regimes over time with different colors
        for regime in regimes:
            regime_data = plot_data[plot_data[regime_col] == regime]
            ax2.scatter(regime_data.index, [1] * len(regime_data), 
                       marker='|', s=100, label=f'Regime {regime}', color=regime_colors[regime])
            
            # Fill time periods
            for i in range(len(regime_data) - 1):
                start = regime_data.index[i]
                end = regime_data.index[i+1]
                ax2.axvspan(start, end, alpha=0.3, color=regime_colors[regime])
        
        # Remove y-axis ticks and labels
        ax2.set_yticks([])
        ax2.set_ylabel('')
        
        # Set x-axis label
        ax2.set_xlabel('Date')
        
        # Set title
        ax2.set_title('Regimes Over Time')
        
        # Rotate x-axis labels
        plt.xticks(rotation=45)
    else:
        # If no date column, show warning on plot
        ax2.text(0.5, 0.5, "No date column provided for time series plot", 
                ha='center', va='center', fontsize=12)
        ax2.set_xticks([])
        ax2.set_yticks([])
    
    # Add overall title
    fig.suptitle(title, fontsize=16)
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved figure to {save_path}")
        except Exception as e:
            logger.error(f"Error saving figure: {str(e)}")
    
    return fig

def plot_signal_distribution(data: pd.DataFrame,
                           signal_col: str = 'position',
                           meta_signal_col: str = 'meta_position',
                           returns_col: str = 'returns',
                           title: str = 'Signal Distribution',
                           figsize: Tuple[int, int] = (12, 8),
                           save_path: str = None) -> plt.Figure:
    """
    Plot the distribution of trading signals and their performance.
    
    Args:
        data: DataFrame with signal and returns columns
        signal_col: Column name for original signals
        meta_signal_col: Column name for meta-labeled signals
        returns_col: Column name for returns
        title: Plot title
        figsize: Figure size (width, height)
        save_path: Path to save the figure
        
    Returns:
        Matplotlib figure
    """
    logger.info(f"Plotting signal distribution with {len(data)} points")
    
    # Set style
    set_plotting_style()
    
    # Create figure with three subplots
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=figsize)
    
    # Check if required columns exist
    if signal_col not in data.columns:
        logger.warning(f"Signal column '{signal_col}' not found in data")
        return None
    
    # Plot original signal distribution
    signal_counts = data[signal_col].value_counts()
    signal_counts.index = signal_counts.index.map(lambda x: f"{x} ({['Hold', 'Long', 'Short'][int(x) + 1]})")
    ax1.bar(signal_counts.index, signal_counts.values, alpha=0.7)
    ax1.set_title(f'Original Signal Distribution ({signal_col})')
    ax1.set_ylabel('Count')
    
    # Plot meta-signal distribution if available
    if meta_signal_col in data.columns:
        meta_signal_counts = data[meta_signal_col].value_counts()
        meta_signal_counts.index = meta_signal_counts.index.map(lambda x: f"{x} ({['Hold', 'Long', 'Short'][int(x) + 1]})")
        
        ax2.bar(meta_signal_counts.index, meta_signal_counts.values, alpha=0.7, color='orange')
        ax2.set_title(f'Meta-Signal Distribution ({meta_signal_col})')
        ax2.set_ylabel('Count')
        
        # Calculate what percentage of original signals were kept
        if signal_col in data.columns:
            original_signals = (data[signal_col] != 0).sum()
            meta_signals = (data[meta_signal_col] != 0).sum()
            kept_ratio = meta_signals / original_signals if original_signals > 0 else 0
            ax2.text(0.02, 0.9, f"Kept {meta_signals}/{original_signals} signals ({kept_ratio:.1%})", 
                    transform=ax2.transAxes, fontsize=10, 
                    bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.5'))
    else:
        ax2.text(0.5, 0.5, f"Meta-signal column '{meta_signal_col}' not found in data", 
                ha='center', va='center', fontsize=12)
        ax2.set_xticks([])
        ax2.set_yticks([])
    
    # Plot signal returns if returns column is available
    if returns_col in data.columns:
        # Calculate signal returns
        if signal_col in data.columns:
            signal_returns = data[signal_col].shift(1) * data[returns_col]
            ax3.hist(signal_returns.dropna(), bins=50, alpha=0.5, label=f'{signal_col} Returns')
        
        # Calculate meta-signal returns if available
        if meta_signal_col in data.columns:
            meta_returns = data[meta_signal_col].shift(1) * data[returns_col]
            ax3.hist(meta_returns.dropna(), bins=50, alpha=0.5, label=f'{meta_signal_col} Returns')
        
        ax3.set_title('Distribution of Signal Returns')
        ax3.set_xlabel('Return')
        ax3.set_ylabel('Frequency')
        ax3.legend()
        
        # Add vertical line at zero
        ax3.axvline(x=0, color='black', linestyle='--', alpha=0.7)
    else:
        ax3.text(0.5, 0.5, f"Returns column '{returns_col}' not found in data", 
                ha='center', va='center', fontsize=12)
        ax3.set_xticks([])
        ax3.set_yticks([])
    
    # Add overall title
    fig.suptitle(title, fontsize=16)
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved figure to {save_path}")
        except Exception as e:
            logger.error(f"Error saving figure: {str(e)}")
    
    return fig

def plot_performance_metrics(metrics: Dict[str, float],
                           title: str = 'Model Performance Metrics',
                           figsize: Tuple[int, int] = (10, 6),
                           save_path: str = None) -> plt.Figure:
    """
    Plot performance metrics for a model.
    
    Args:
        metrics: Dictionary of metric names and values
        title: Plot title
        figsize: Figure size (width, height)
        save_path: Path to save the figure
        
    Returns:
        Matplotlib figure
    """
    logger.info(f"Plotting performance metrics: {list(metrics.keys())}")
    
    # Set style
    set_plotting_style()
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Sort metrics by value
    sorted_metrics = {k: v for k, v in sorted(metrics.items(), key=lambda item: item[1], reverse=True)}
    
    # Plot bar chart
    bars = ax.bar(sorted_metrics.keys(), sorted_metrics.values(), alpha=0.7)
    
    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.4f}', ha='center', va='bottom', fontsize=9)
    
    # Set labels and title
    ax.set_title(title)
    ax.set_xlabel('Metric')
    ax.set_ylabel('Value')
    
    # Set y-axis limits
    ax.set_ylim(0, max(sorted_metrics.values()) * 1.1)
    
    # Rotate x-axis labels
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved figure to {save_path}")
        except Exception as e:
            logger.error(f"Error saving figure: {str(e)}")
    
    return fig

def plot_confusion_matrix(y_true: np.ndarray, 
                        y_pred: np.ndarray,
                        labels: List[str] = None,
                        title: str = 'Confusion Matrix',
                        figsize: Tuple[int, int] = (8, 6),
                        save_path: str = None) -> plt.Figure:
    """
    Plot confusion matrix for classification results.
    
    Args:
        y_true: True labels
        y_pred: Predicted labels
        labels: Label names
        title: Plot title
        figsize: Figure size (width, height)
        save_path: Path to save the figure
        
    Returns:
        Matplotlib figure
    """
    from sklearn.metrics import confusion_matrix
    import itertools
    
    logger.info("Plotting confusion matrix")
    
    # Set style
    set_plotting_style()
    
    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # Normalize the confusion matrix
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Use a color map
    cmap = plt.cm.Blues
    
    # Plot the confusion matrix
    im = ax.imshow(cm_normalized, interpolation='nearest', cmap=cmap)
    
    # Add colorbar
    plt.colorbar(im)
    
    # Add labels and title
    classes = labels or [str(i) for i in range(cm.shape[0])]
    tick_marks = np.arange(len(classes))
    ax.set_xticks(tick_marks)
    ax.set_yticks(tick_marks)
    ax.set_xticklabels(classes)
    ax.set_yticklabels(classes)
    
    # Add text with values
    fmt = '.2f'  # Format for the values
    thresh = cm_normalized.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        # Add normalized value
        ax.text(j, i, format(cm_normalized[i, j], fmt),
                horizontalalignment="center",
                color="white" if cm_normalized[i, j] > thresh else "black")
        
        # Add absolute counts
        ax.text(j, i + 0.3, f"({cm[i, j]})",
                horizontalalignment="center", fontsize=8,
                color="white" if cm_normalized[i, j] > thresh else "black")
    
    # Set labels and title
    ax.set_title(title)
    ax.set_ylabel('True label')
    ax.set_xlabel('Predicted label')
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved figure to {save_path}")
        except Exception as e:
            logger.error(f"Error saving figure: {str(e)}")
    
    return fig

def plot_feature_importance(features: List[str],
                          importance: np.ndarray,
                          title: str = 'Feature Importance',
                          figsize: Tuple[int, int] = (10, 8),
                          save_path: str = None,
                          top_n: int = None) -> plt.Figure:
    """
    Plot feature importance.
    
    Args:
        features: List of feature names
        importance: Array of importance values
        title: Plot title
        figsize: Figure size (width, height)
        save_path: Path to save the figure
        top_n: Number of top features to show (None for all)
        
    Returns:
        Matplotlib figure
    """
    logger.info(f"Plotting feature importance for {len(features)} features")
    
    # Set style
    set_plotting_style()
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Sort features by importance
    indices = np.argsort(importance)
    
    # Show only top N if specified
    if top_n is not None and top_n < len(features):
        indices = indices[-top_n:]
    
    # Plot horizontal bar chart
    plt.barh(range(len(indices)), importance[indices], align='center', alpha=0.7)
    plt.yticks(range(len(indices)), [features[i] for i in indices])
    
    # Add value labels to bars
    for i, v in enumerate(importance[indices]):
        ax.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9)
    
    # Set labels and title
    ax.set_title(title)
    ax.set_xlabel('Importance')
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved figure to {save_path}")
        except Exception as e:
            logger.error(f"Error saving figure: {str(e)}")
    
    return fig

def plot_learning_curves(train_history: Dict[str, List],
                       title: str = 'Learning Curves',
                       figsize: Tuple[int, int] = (10, 6),
                       save_path: str = None) -> plt.Figure:
    """
    Plot learning curves from training history.
    
    Args:
        train_history: Dictionary with lists of values for each metric
        title: Plot title
        figsize: Figure size (width, height)
        save_path: Path to save the figure
        
    Returns:
        Matplotlib figure
    """
    logger.info(f"Plotting learning curves with metrics: {list(train_history.keys())}")
    
    # Set style
    set_plotting_style()
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Plot each metric
    for metric, values in train_history.items():
        # Skip metrics with 'val_' prefix since they will be plotted alongside the main metrics
        if metric.startswith('val_'):
            continue
        
        # Plot training metric
        ax.plot(values, label=f'Training {metric}')
        
        # Plot validation metric if available
        val_metric = f'val_{metric}'
        if val_metric in train_history:
            ax.plot(train_history[val_metric], linestyle='--', label=f'Validation {metric}')
    
    # Set labels and title
    ax.set_title(title)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Value')
    
    # Add legend
    ax.legend()
    
    # Add grid
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved figure to {save_path}")
        except Exception as e:
            logger.error(f"Error saving figure: {str(e)}")
    
    return fig

def plot_regime_performance(regime_performance: Dict[int, Dict[str, float]],
                          title: str = 'Performance by Regime',
                          figsize: Tuple[int, int] = (12, 8),
                          save_path: str = None) -> plt.Figure:
    """
    Plot performance metrics by regime.
    
    Args:
        regime_performance: Dictionary of regimes with their performance metrics
        title: Plot title
        figsize: Figure size (width, height)
        save_path: Path to save the figure
        
    Returns:
        Matplotlib figure
    """
    logger.info(f"Plotting performance for {len(regime_performance)} regimes")
    
    # Set style
    set_plotting_style()
    
    # Get all metrics from the first regime
    if not regime_performance:
        logger.warning("No regime performance data provided")
        return None
    
    first_regime = next(iter(regime_performance.values()))
    metrics = list(first_regime.keys())
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Set x positions for bars
    x = np.arange(len(metrics))
    width = 0.8 / len(regime_performance)  # Width of bars
    
    # Plot bars for each regime
    for i, (regime, perf) in enumerate(regime_performance.items()):
        values = [perf.get(metric, 0) for metric in metrics]
        bars = ax.bar(x + i * width - 0.4 + width/2, values, width, label=f'Regime {regime}')
        
        # Add value labels
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{height:.2f}', ha='center', va='bottom', fontsize=7, rotation=90)
    
    # Set labels and title
    ax.set_title(title)
    ax.set_ylabel('Value')
    ax.set_xticks(x)
    ax.set_xticklabels(metrics, rotation=45)
    
    # Add legend
    ax.legend()
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved figure to {save_path}")
        except Exception as e:
            logger.error(f"Error saving figure: {str(e)}")
    
    return fig



================================================================
End of Codebase
================================================================
